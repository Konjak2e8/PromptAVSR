2025-02-02 16:41:51 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11151
2025-02-02 16:41:51 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11151
[W202 16:41:51.381764472 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 16:41:51 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 3
2025-02-02 16:41:52 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11151
[W202 16:41:52.481955014 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 16:41:52 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 2
2025-02-02 16:41:52 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11151
[W202 16:41:52.516325095 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W202 16:41:52.516547009 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 16:41:52 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 1
2025-02-02 16:41:52 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 0
[2025-02-02 16:41:52,957][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/workspace/av_hubert/avhubert', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11151', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 2, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 45000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'av_hubert_seq2seq', 'w2v_path': '/workspace/AV_HuBERT_pretrained/base_vox_iter5.pt', 'apply_mask': False, 'mask_selection': 'static', 'mask_length': 10, 'mask_other': 0, 'mask_prob': 0.75, 'mask_channel_selection': 'static', 'mask_channel_length': 64, 'mask_channel_other': 0, 'mask_channel_prob': 0.5, 'layerdrop': 0.1, 'dropout': 0.0, 'activation_dropout': 0.1, 'attention_dropout': 0.0, 'feature_grad_mult': 1.0, 'decoder_layers': 6, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.0, 'decoder_activation_dropout': 0.1, 'freeze_finetune_updates': 22500, 'share_decoder_input_output_embed': True, 'decoder_normalize_before': True, 'prompting': True}, 'task': {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/workspace/lrs2/433h_data', 'label_dir': '/workspace/lrs2/433h_data', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'fine_tuning': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video', 'audio'], 'image_aug': True, 'pad_audio': True, 'random_crop': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': True, 'ignore_prefix_size': 0, 'sentence_avg': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 15000, 'hold_steps': 0, 'decay_steps': 30000, 'phase_ratio': None, 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 45000, 'lr': [0.001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-02-02 16:41:52,963][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/avhubert/finetune_prompt_withres
[2025-02-02 16:41:52,963][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['wrd'], 'label_dir': '/workspace/lrs2/433h_data', 'label_rate': -1, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 500, 'min_sample_size': None, 'max_trim_sample_size': '${task.max_sample_size}', 'single_target': True, 'random_crop': False, 'pad_audio': True, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['video', 'audio'], 'is_s2s': True, 'tokenizer_bpe_name': 'sentencepiece', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': True}
[2025-02-02 16:41:53,896][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/avhubert/finetune_prompt_withres
[2025-02-02 16:41:53,897][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['km'], 'label_dir': '/checkpoint/bshi/data/lrs3//video/hubert/stitch-iters/envox-iter4-l12c2000/', 'label_rate': 25, 'sample_rate': 25, 'normalize': True, 'enable_padding': False, 'max_sample_size': 2000, 'min_sample_size': 5, 'max_trim_sample_size': 400, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['audio', 'video'], 'is_s2s': False, 'tokenizer_bpe_name': None, 'tokenizer_bpe_model': None, 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': False}
2025-02-02 16:41:53 | INFO | avhubert.hubert | HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
2025-02-02 16:41:53 | INFO | avhubert.hubert | HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
[2025-02-02 16:41:53,906][avhubert.hubert][INFO] - HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
2025-02-02 16:41:53 | INFO | avhubert.hubert | HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
encoder.w2v_model.modal_prompt_learner.video_prompt_complete
encoder.w2v_model.modal_prompt_learner.video_prompt_missing
encoder.w2v_model.modal_prompt_learner.audio_prompt_complete
encoder.w2v_model.modal_prompt_learner.audio_prompt_missing
encoder.w2v_model.modal_prompt_learner.common_prompt_complete
encoder.w2v_model.modal_prompt_learner.common_prompt_video
encoder.w2v_model.modal_prompt_learner.common_prompt_audio
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.bias
decoder.embed_tokens.weight
decoder.layers.0.self_attn.k_proj.weight
decoder.layers.0.self_attn.k_proj.bias
decoder.layers.0.self_attn.v_proj.weight
decoder.layers.0.self_attn.v_proj.bias
decoder.layers.0.self_attn.q_proj.weight
decoder.layers.0.self_attn.q_proj.bias
decoder.layers.0.self_attn.out_proj.weight
decoder.layers.0.self_attn.out_proj.bias
decoder.layers.0.self_attn_layer_norm.weight
decoder.layers.0.self_attn_layer_norm.bias
decoder.layers.0.encoder_attn.k_proj.weight
decoder.layers.0.encoder_attn.k_proj.bias
decoder.layers.0.encoder_attn.v_proj.weight
decoder.layers.0.encoder_attn.v_proj.bias
decoder.layers.0.encoder_attn.q_proj.weight
decoder.layers.0.encoder_attn.q_proj.bias
decoder.layers.0.encoder_attn.out_proj.weight
decoder.layers.0.encoder_attn.out_proj.bias
decoder.layers.0.encoder_attn_layer_norm.weight
decoder.layers.0.encoder_attn_layer_norm.bias
decoder.layers.0.fc1.weight
decoder.layers.0.fc1.bias
decoder.layers.0.fc2.weight
decoder.layers.0.fc2.bias
decoder.layers.0.final_layer_norm.weight
decoder.layers.0.final_layer_norm.bias
decoder.layers.1.self_attn.k_proj.weight
decoder.layers.1.self_attn.k_proj.bias
decoder.layers.1.self_attn.v_proj.weight
decoder.layers.1.self_attn.v_proj.bias
decoder.layers.1.self_attn.q_proj.weight
decoder.layers.1.self_attn.q_proj.bias
decoder.layers.1.self_attn.out_proj.weight
decoder.layers.1.self_attn.out_proj.bias
decoder.layers.1.self_attn_layer_norm.weight
decoder.layers.1.self_attn_layer_norm.bias
decoder.layers.1.encoder_attn.k_proj.weight
decoder.layers.1.encoder_attn.k_proj.bias
decoder.layers.1.encoder_attn.v_proj.weight
decoder.layers.1.encoder_attn.v_proj.bias
decoder.layers.1.encoder_attn.q_proj.weight
decoder.layers.1.encoder_attn.q_proj.bias
decoder.layers.1.encoder_attn.out_proj.weight
decoder.layers.1.encoder_attn.out_proj.bias
decoder.layers.1.encoder_attn_layer_norm.weight
decoder.layers.1.encoder_attn_layer_norm.bias
decoder.layers.1.fc1.weight
decoder.layers.1.fc1.bias
decoder.layers.1.fc2.weight
decoder.layers.1.fc2.bias
decoder.layers.1.final_layer_norm.weight
decoder.layers.1.final_layer_norm.bias
decoder.layers.2.self_attn.k_proj.weight
decoder.layers.2.self_attn.k_proj.bias
decoder.layers.2.self_attn.v_proj.weight
decoder.layers.2.self_attn.v_proj.bias
decoder.layers.2.self_attn.q_proj.weight
decoder.layers.2.self_attn.q_proj.bias
decoder.layers.2.self_attn.out_proj.weight
decoder.layers.2.self_attn.out_proj.bias
decoder.layers.2.self_attn_layer_norm.weight
decoder.layers.2.self_attn_layer_norm.bias
decoder.layers.2.encoder_attn.k_proj.weight
decoder.layers.2.encoder_attn.k_proj.bias
decoder.layers.2.encoder_attn.v_proj.weight
decoder.layers.2.encoder_attn.v_proj.bias
decoder.layers.2.encoder_attn.q_proj.weight
decoder.layers.2.encoder_attn.q_proj.bias
decoder.layers.2.encoder_attn.out_proj.weight
decoder.layers.2.encoder_attn.out_proj.bias
decoder.layers.2.encoder_attn_layer_norm.weight
decoder.layers.2.encoder_attn_layer_norm.bias
decoder.layers.2.fc1.weight
decoder.layers.2.fc1.bias
decoder.layers.2.fc2.weight
decoder.layers.2.fc2.bias
decoder.layers.2.final_layer_norm.weight
decoder.layers.2.final_layer_norm.bias
decoder.layers.3.self_attn.k_proj.weight
decoder.layers.3.self_attn.k_proj.bias
decoder.layers.3.self_attn.v_proj.weight
decoder.layers.3.self_attn.v_proj.bias
decoder.layers.3.self_attn.q_proj.weight
decoder.layers.3.self_attn.q_proj.bias
decoder.layers.3.self_attn.out_proj.weight
decoder.layers.3.self_attn.out_proj.bias
encoder.w2v_model.modal_prompt_learner.video_prompt_complete
encoder.w2v_model.modal_prompt_learner.video_prompt_missing
encoder.w2v_model.modal_prompt_learner.audio_prompt_complete
encoder.w2v_model.modal_prompt_learner.audio_prompt_missing
encoder.w2v_model.modal_prompt_learner.common_prompt_complete
encoder.w2v_model.modal_prompt_learner.common_prompt_video
encoder.w2v_model.modal_prompt_learner.common_prompt_audio
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.bias
decoder.embed_tokens.weight
decoder.layers.0.self_attn.k_proj.weight
decoder.layers.0.self_attn.k_proj.bias
decoder.layers.0.self_attn.v_proj.weight
decoder.layers.0.self_attn.v_proj.bias
decoder.layers.0.self_attn.q_proj.weight
decoder.layers.0.self_attn.q_proj.bias
decoder.layers.0.self_attn.out_proj.weight
decoder.layers.0.self_attn.out_proj.bias
decoder.layers.0.self_attn_layer_norm.weight
decoder.layers.0.self_attn_layer_norm.bias
decoder.layers.0.encoder_attn.k_proj.weight
decoder.layers.0.encoder_attn.k_proj.bias
decoder.layers.0.encoder_attn.v_proj.weight
decoder.layers.0.encoder_attn.v_proj.bias
decoder.layers.0.encoder_attn.q_proj.weight
decoder.layers.0.encoder_attn.q_proj.bias
decoder.layers.0.encoder_attn.out_proj.weight
decoder.layers.0.encoder_attn.out_proj.bias
decoder.layers.0.encoder_attn_layer_norm.weight
decoder.layers.0.encoder_attn_layer_norm.bias
decoder.layers.0.fc1.weight
decoder.layers.0.fc1.bias
decoder.layers.0.fc2.weight
decoder.layers.0.fc2.bias
decoder.layers.0.final_layer_norm.weight
decoder.layers.0.final_layer_norm.bias
decoder.layers.1.self_attn.k_proj.weight
decoder.layers.1.self_attn.k_proj.bias
decoder.layers.1.self_attn.v_proj.weight
decoder.layers.1.self_attn.v_proj.bias
decoder.layers.1.self_attn.q_proj.weight
decoder.layers.1.self_attn.q_proj.bias
decoder.layers.1.self_attn.out_proj.weight
decoder.layers.1.self_attn.out_proj.bias
decoder.layers.1.self_attn_layer_norm.weight
decoder.layers.1.self_attn_layer_norm.bias
decoder.layers.1.encoder_attn.k_proj.weight
decoder.layers.1.encoder_attn.k_proj.bias
decoder.layers.1.encoder_attn.v_proj.weight
decoder.layers.1.encoder_attn.v_proj.bias
decoder.layers.1.encoder_attn.q_proj.weight
decoder.layers.1.encoder_attn.q_proj.bias
decoder.layers.1.encoder_attn.out_proj.weight
decoder.layers.1.encoder_attn.out_proj.bias
decoder.layers.1.encoder_attn_layer_norm.weight
decoder.layers.1.encoder_attn_layer_norm.bias
decoder.layers.1.fc1.weight
decoder.layers.1.fc1.bias
decoder.layers.1.fc2.weight
decoder.layers.1.fc2.bias
decoder.layers.1.final_layer_norm.weight
decoder.layers.1.final_layer_norm.bias
decoder.layers.2.self_attn.k_proj.weight
decoder.layers.2.self_attn.k_proj.bias
decoder.layers.2.self_attn.v_proj.weight
decoder.layers.2.self_attn.v_proj.bias
decoder.layers.2.self_attn.q_proj.weight
decoder.layers.2.self_attn.q_proj.bias
decoder.layers.2.self_attn.out_proj.weight
decoder.layers.2.self_attn.out_proj.bias
decoder.layers.2.self_attn_layer_norm.weight
decoder.layers.2.self_attn_layer_norm.bias
decoder.layers.2.encoder_attn.k_proj.weight
decoder.layers.2.encoder_attn.k_proj.bias
decoder.layers.2.encoder_attn.v_proj.weight
decoder.layers.2.encoder_attn.v_proj.bias
decoder.layers.2.encoder_attn.q_proj.weight
decoder.layers.2.encoder_attn.q_proj.bias
decoder.layers.2.encoder_attn.out_proj.weight
decoder.layers.2.encoder_attn.out_proj.bias
decoder.layers.2.encoder_attn_layer_norm.weight
decoder.layers.2.encoder_attn_layer_norm.bias
decoder.layers.2.fc1.weight
decoder.layers.2.fc1.bias
decoder.layers.2.fc2.weight
decoder.layers.2.fc2.bias
decoder.layers.2.final_layer_norm.weight
decoder.layers.2.final_layer_norm.bias
decoder.layers.3.self_attn.k_proj.weight
decoder.layers.3.self_attn.k_proj.bias
decoder.layers.3.self_attn.v_proj.weight
decoder.layers.3.self_attn.v_proj.bias
decoder.layers.3.self_attn.q_proj.weight
decoder.layers.3.self_attn.q_proj.bias
decoder.layers.3.self_attn.out_proj.weight
decoder.layers.3.self_attn.out_proj.bias
encoder.w2v_model.modal_prompt_learner.video_prompt_complete
encoder.w2v_model.modal_prompt_learner.video_prompt_missing
encoder.w2v_model.modal_prompt_learner.audio_prompt_complete
encoder.w2v_model.modal_prompt_learner.audio_prompt_missing
encoder.w2v_model.modal_prompt_learner.common_prompt_complete
encoder.w2v_model.modal_prompt_learner.common_prompt_video
encoder.w2v_model.modal_prompt_learner.common_prompt_audio
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.bias
decoder.embed_tokens.weight
decoder.layers.0.self_attn.k_proj.weight
decoder.layers.0.self_attn.k_proj.bias
decoder.layers.0.self_attn.v_proj.weight
decoder.layers.0.self_attn.v_proj.bias
decoder.layers.0.self_attn.q_proj.weight
decoder.layers.0.self_attn.q_proj.bias
decoder.layers.0.self_attn.out_proj.weight
decoder.layers.0.self_attn.out_proj.bias
decoder.layers.0.self_attn_layer_norm.weight
decoder.layers.0.self_attn_layer_norm.bias
decoder.layers.0.encoder_attn.k_proj.weight
decoder.layers.0.encoder_attn.k_proj.bias
decoder.layers.0.encoder_attn.v_proj.weight
decoder.layers.0.encoder_attn.v_proj.bias
decoder.layers.0.encoder_attn.q_proj.weight
decoder.layers.0.encoder_attn.q_proj.bias
decoder.layers.0.encoder_attn.out_proj.weight
decoder.layers.0.encoder_attn.out_proj.bias
decoder.layers.0.encoder_attn_layer_norm.weight
decoder.layers.0.encoder_attn_layer_norm.bias
decoder.layers.0.fc1.weight
decoder.layers.0.fc1.bias
decoder.layers.0.fc2.weight
decoder.layers.0.fc2.bias
decoder.layers.0.final_layer_norm.weight
decoder.layers.0.final_layer_norm.bias
decoder.layers.1.self_attn.k_proj.weight
decoder.layers.1.self_attn.k_proj.bias
decoder.layers.1.self_attn.v_proj.weight
decoder.layers.1.self_attn.v_proj.bias
decoder.layers.1.self_attn.q_proj.weight
decoder.layers.1.self_attn.q_proj.bias
decoder.layers.1.self_attn.out_proj.weight
decoder.layers.1.self_attn.out_proj.bias
decoder.layers.1.self_attn_layer_norm.weight
decoder.layers.1.self_attn_layer_norm.bias
decoder.layers.1.encoder_attn.k_proj.weight
decoder.layers.1.encoder_attn.k_proj.bias
decoder.layers.1.encoder_attn.v_proj.weight
decoder.layers.1.encoder_attn.v_proj.bias
decoder.layers.1.encoder_attn.q_proj.weight
decoder.layers.1.encoder_attn.q_proj.bias
decoder.layers.1.encoder_attn.out_proj.weight
decoder.layers.1.encoder_attn.out_proj.bias
decoder.layers.1.encoder_attn_layer_norm.weight
decoder.layers.1.encoder_attn_layer_norm.bias
decoder.layers.1.fc1.weight
decoder.layers.1.fc1.bias
decoder.layers.1.fc2.weight
decoder.layers.1.fc2.bias
decoder.layers.1.final_layer_norm.weight
decoder.layers.1.final_layer_norm.bias
decoder.layers.2.self_attn.k_proj.weight
decoder.layers.2.self_attn.k_proj.bias
decoder.layers.2.self_attn.v_proj.weight
decoder.layers.2.self_attn.v_proj.bias
decoder.layers.2.self_attn.q_proj.weight
decoder.layers.2.self_attn.q_proj.bias
decoder.layers.2.self_attn.out_proj.weight
decoder.layers.2.self_attn.out_proj.bias
decoder.layers.2.self_attn_layer_norm.weight
decoder.layers.2.self_attn_layer_norm.bias
decoder.layers.2.encoder_attn.k_proj.weight
decoder.layers.2.encoder_attn.k_proj.bias
decoder.layers.2.encoder_attn.v_proj.weight
decoder.layers.2.encoder_attn.v_proj.bias
decoder.layers.2.encoder_attn.q_proj.weight
decoder.layers.2.encoder_attn.q_proj.bias
decoder.layers.2.encoder_attn.out_proj.weight
decoder.layers.2.encoder_attn.out_proj.bias
decoder.layers.2.encoder_attn_layer_norm.weight
decoder.layers.2.encoder_attn_layer_norm.bias
decoder.layers.2.fc1.weight
decoder.layers.2.fc1.bias
decoder.layers.2.fc2.weight
decoder.layers.2.fc2.bias
decoder.layers.2.final_layer_norm.weight
decoder.layers.2.final_layer_norm.bias
decoder.layers.3.self_attn.k_proj.weight
decoder.layers.3.self_attn.k_proj.bias
decoder.layers.3.self_attn.v_proj.weight
decoder.layers.3.self_attn.v_proj.bias
decoder.layers.3.self_attn.q_proj.weight
decoder.layers.3.self_attn.q_proj.bias
decoder.layers.3.self_attn.out_proj.weight
decoder.layers.3.self_attn.out_proj.bias
encoder.w2v_model.modal_prompt_learner.video_prompt_complete
encoder.w2v_model.modal_prompt_learner.video_prompt_missing
encoder.w2v_model.modal_prompt_learner.audio_prompt_complete
encoder.w2v_model.modal_prompt_learner.audio_prompt_missing
encoder.w2v_model.modal_prompt_learner.common_prompt_complete
encoder.w2v_model.modal_prompt_learner.common_prompt_video
encoder.w2v_model.modal_prompt_learner.common_prompt_audio
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_audio.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_audio.3.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.0.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.1.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.2.2.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.0.bias
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.weight
encoder.w2v_model.modal_prompt_learner.compound_prompt_projections_video.3.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.0.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.1.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.2.bias
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.weight
encoder.w2v_model.modal_prompt_learner.layernorm_video.3.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_video.2.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.0.bias
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.weight
encoder.w2v_model.modal_prompt_learner.common_prompt_projection_audio.2.bias
decoder.embed_tokens.weight
decoder.layers.0.self_attn.k_proj.weight
decoder.layers.0.self_attn.k_proj.bias
decoder.layers.0.self_attn.v_proj.weight
decoder.layers.0.self_attn.v_proj.bias
decoder.layers.0.self_attn.q_proj.weight
decoder.layers.0.self_attn.q_proj.bias
decoder.layers.0.self_attn.out_proj.weight
decoder.layers.0.self_attn.out_proj.bias
decoder.layers.0.self_attn_layer_norm.weight
decoder.layers.0.self_attn_layer_norm.bias
decoder.layers.0.encoder_attn.k_proj.weight
decoder.layers.0.encoder_attn.k_proj.bias
decoder.layers.0.encoder_attn.v_proj.weight
decoder.layers.0.encoder_attn.v_proj.bias
decoder.layers.0.encoder_attn.q_proj.weight
decoder.layers.0.encoder_attn.q_proj.bias
decoder.layers.0.encoder_attn.out_proj.weight
decoder.layers.0.encoder_attn.out_proj.bias
decoder.layers.0.encoder_attn_layer_norm.weight
decoder.layers.0.encoder_attn_layer_norm.bias
decoder.layers.0.fc1.weight
decoder.layers.0.fc1.bias
decoder.layers.0.fc2.weight
decoder.layers.0.fc2.bias
decoder.layers.0.final_layer_norm.weight
decoder.layers.0.final_layer_norm.bias
decoder.layers.1.self_attn.k_proj.weight
decoder.layers.1.self_attn.k_proj.bias
decoder.layers.1.self_attn.v_proj.weight
decoder.layers.1.self_attn.v_proj.bias
decoder.layers.1.self_attn.q_proj.weight
decoder.layers.1.self_attn.q_proj.bias
decoder.layers.1.self_attn.out_proj.weight
decoder.layers.1.self_attn.out_proj.bias
decoder.layers.1.self_attn_layer_norm.weight
decoder.layers.1.self_attn_layer_norm.bias
decoder.layers.1.encoder_attn.k_proj.weight
decoder.layers.1.encoder_attn.k_proj.bias
decoder.layers.1.encoder_attn.v_proj.weight
decoder.layers.1.encoder_attn.v_proj.bias
decoder.layers.1.encoder_attn.q_proj.weight
decoder.layers.1.encoder_attn.q_proj.bias
decoder.layers.1.encoder_attn.out_proj.weight
decoder.layers.1.encoder_attn.out_proj.bias
decoder.layers.1.encoder_attn_layer_norm.weight
decoder.layers.1.encoder_attn_layer_norm.bias
decoder.layers.1.fc1.weight
decoder.layers.1.fc1.bias
decoder.layers.1.fc2.weight
decoder.layers.1.fc2.bias
decoder.layers.1.final_layer_norm.weight
decoder.layers.1.final_layer_norm.bias
decoder.layers.2.self_attn.k_proj.weight
decoder.layers.2.self_attn.k_proj.bias
decoder.layers.2.self_attn.v_proj.weight
decoder.layers.2.self_attn.v_proj.bias
decoder.layers.2.self_attn.q_proj.weight
decoder.layers.2.self_attn.q_proj.bias
decoder.layers.2.self_attn.out_proj.weight
decoder.layers.2.self_attn.out_proj.bias
decoder.layers.2.self_attn_layer_norm.weight
decoder.layers.2.self_attn_layer_norm.bias
decoder.layers.2.encoder_attn.k_proj.weight
decoder.layers.2.encoder_attn.k_proj.bias
decoder.layers.2.encoder_attn.v_proj.weight
decoder.layers.2.encoder_attn.v_proj.bias
decoder.layers.2.encoder_attn.q_proj.weight
decoder.layers.2.encoder_attn.q_proj.bias
decoder.layers.2.encoder_attn.out_proj.weight
decoder.layers.2.encoder_attn.out_proj.bias
decoder.layers.2.encoder_attn_layer_norm.weight
decoder.layers.2.encoder_attn_layer_norm.bias
decoder.layers.2.fc1.weight
decoder.layers.2.fc1.bias
decoder.layers.2.fc2.weight
decoder.layers.2.fc2.bias
decoder.layers.2.final_layer_norm.weight
decoder.layers.2.final_layer_norm.bias
decoder.layers.3.self_attn.k_proj.weight
decoder.layers.3.self_attn.k_proj.bias
decoder.layers.3.self_attn.v_proj.weight
decoder.layers.3.self_attn.v_proj.bias
decoder.layers.3.self_attn.q_proj.weight
decoder.layers.3.self_attn.q_proj.bias
decoder.layers.3.self_attn.out_proj.weight
decoder.layers.3.self_attn.out_proj.bias
decoder.layers.3.self_attn_layer_norm.weight
decoder.layers.3.self_attn_layer_norm.bias
decoder.layers.3.encoder_attn.k_proj.weight
decoder.layers.3.encoder_attn.k_proj.bias
decoder.layers.3.encoder_attn.v_proj.weight
decoder.layers.3.encoder_attn.v_proj.bias
decoder.layers.3.encoder_attn.q_proj.weight
decoder.layers.3.encoder_attn.q_proj.bias
decoder.layers.3.encoder_attn.out_proj.weight
decoder.layers.3.encoder_attn.out_proj.bias
decoder.layers.3.encoder_attn_layer_norm.weight
decoder.layers.3.encoder_attn_layer_norm.bias
decoder.layers.3.fc1.weight
decoder.layers.3.fc1.bias
decoder.layers.3.fc2.weight
decoder.layers.3.fc2.bias
decoder.layers.3.final_layer_norm.weight
decoder.layers.3.final_layer_norm.bias
decoder.layers.4.self_attn.k_proj.weight
decoder.layers.4.self_attn.k_proj.bias
decoder.layers.4.self_attn.v_proj.weight
decoder.layers.4.self_attn.v_proj.bias
decoder.layers.4.self_attn.q_proj.weight
decoder.layers.4.self_attn.q_proj.bias
decoder.layers.4.self_attn.out_proj.weight
decoder.layers.4.self_attn.out_proj.bias
decoder.layers.4.self_attn_layer_norm.weight
decoder.layers.4.self_attn_layer_norm.bias
decoder.layers.4.encoder_attn.k_proj.weight
decoder.layers.4.encoder_attn.k_proj.bias
decoder.layers.4.encoder_attn.v_proj.weight
decoder.layers.4.encoder_attn.v_proj.bias
decoder.layers.4.encoder_attn.q_proj.weight
decoder.layers.4.encoder_attn.q_proj.bias
decoder.layers.4.encoder_attn.out_proj.weight
decoder.layers.4.encoder_attn.out_proj.bias
decoder.layers.4.encoder_attn_layer_norm.weight
decoder.layers.4.encoder_attn_layer_norm.bias
decoder.layers.4.fc1.weight
decoder.layers.4.fc1.bias
decoder.layers.4.fc2.weight
decoder.layers.4.fc2.bias
decoder.layers.4.final_layer_norm.weight
decoder.layers.4.final_layer_norm.bias
decoder.layers.5.self_attn.k_proj.weight
decoder.layers.5.self_attn.k_proj.bias
decoder.layers.5.self_attn.v_proj.weight
decoder.layers.5.self_attn.v_proj.bias
decoder.layers.5.self_attn.q_proj.weight
decoder.layers.5.self_attn.q_proj.bias
decoder.layers.5.self_attn.out_proj.weight
decoder.layers.5.self_attn.out_proj.bias
decoder.layers.5.self_attn_layer_norm.weight
decoder.layers.5.self_attn_layer_norm.bias
decoder.layers.5.encoder_attn.k_proj.weight
decoder.layers.5.encoder_attn.k_proj.bias
decoder.layers.5.encoder_attn.v_proj.weight
decoder.layers.5.encoder_attn.v_proj.bias
decoder.layers.5.encoder_attn.q_proj.weight
decoder.layers.5.encoder_attn.q_proj.bias
decoder.layers.5.encoder_attn.out_proj.weight
decoder.layers.5.encoder_attn.out_proj.bias
decoder.layers.5.encoder_attn_layer_norm.weight
decoder.layers.5.encoder_attn_layer_norm.bias
decoder.layers.5.fc1.weight
decoder.layers.5.fc1.bias
decoder.layers.5.fc2.weight
decoder.layers.5.fc2.bias
decoder.layers.5.final_layer_norm.weight
decoder.layers.5.final_layer_norm.bias
decoder.layer_norm.weight
decoder.layer_norm.bias
all_params: 162.59M learnable_params: 59.45M
[2025-02-02 16:41:58,208][fairseq_cli.train][INFO] - AVHubertSeq2Seq(
  (encoder): HubertEncoderWrapper(
    (w2v_model): AVHubertModel(
      (modal_prompt_learner): MultiModalPromptLearner(
        (compound_prompt_projections_audio): ModuleList(
          (0-3): 4 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_audio): ModuleList(
          (0-3): 4 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (compound_prompt_projections_video): ModuleList(
          (0-3): 4 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_video): ModuleList(
          (0-3): 4 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (common_prompt_projection_video): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
        (common_prompt_projection_audio): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
      )
      (feature_extractor_audio): SubModel(
        (proj): Linear(in_features=104, out_features=768, bias=True)
      )
      (feature_extractor_video): SubModel(
        (resnet): ResEncoder(
          (frontend3D): Sequential(
            (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
            (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): PReLU(num_parameters=64)
            (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
          )
          (trunk): ResNet(
            (layer1): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer2): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer3): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer4): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (avgpool): AdaptiveAvgPool2d(output_size=1)
          )
        )
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
      (post_extract_proj): Linear(in_features=1536, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (resblocks_audio): Sequential()
      (resblocks_video): Sequential()
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(1000, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
[2025-02-02 16:41:58,212][fairseq_cli.train][INFO] - task: AVHubertPretrainingTask
[2025-02-02 16:41:58,212][fairseq_cli.train][INFO] - model: AVHubertSeq2Seq
[2025-02-02 16:41:58,212][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2025-02-02 16:41:58,215][fairseq_cli.train][INFO] - num. shared model params: 162,586,696 (num. trained: 59,451,744)
[2025-02-02 16:41:58,217][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-02-02 16:41:58,219][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2025-02-02 16:41:58,229][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 1082, skipped 0 short and 0 long and 0 unaligned, longest-loaded=153, shortest-loaded=14
[2025-02-02 16:41:58,230][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/valid.wrd is sequence label. skipped
[2025-02-02 16:41:58,230][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    <avhubert.utils.CenterCrop object at 0x7f612b28d0a0>
    Normalize(mean=0.421, std=0.165)
)
[2025-02-02 16:41:58,230][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-02-02 16:41:58,230][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2025-02-02 16:41:58,367][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv1.bias
[2025-02-02 16:41:58,367][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.0.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.0.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv1.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv2.bias
[2025-02-02 16:41:58,368][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.0.bias
[2025-02-02 16:41:58,369][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv1.bias
[2025-02-02 16:41:58,369][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv2.bias
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 16:41:58,544][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2025-02-02 16:41:58,544][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2025-02-02 16:41:58,544][fairseq_cli.train][INFO] - max tokens per device = 1000 and max sentences per device = None
[2025-02-02 16:41:58,545][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2025-02-02 16:42:02,889][fairseq.trainer][INFO] - Loaded checkpoint checkpoints/checkpoint_last.pt (epoch 35 @ 0 updates)
[2025-02-02 16:42:03,032][fairseq.trainer][INFO] - loading train data for epoch 35
[2025-02-02 16:42:03,033][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2025-02-02 16:42:03,820][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 163822, skipped 0 short and 292 long and 0 unaligned, longest-loaded=500, shortest-loaded=0
[2025-02-02 16:42:03,922][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/train.wrd is sequence label. skipped
[2025-02-02 16:42:03,923][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    RandomCrop(size=(88, 88))
    <avhubert.utils.HorizontalFlip object at 0x7f60e656e880>
    Normalize(mean=0.421, std=0.165)
)
[2025-02-02 16:42:03,923][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-02-02 16:42:03,923][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2025-02-02 16:42:05,263][fairseq.trainer][INFO] - begin training epoch 35
[2025-02-02 16:42:05,264][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
decoder.layers.3.self_attn_layer_norm.weight
decoder.layers.3.self_attn_layer_norm.bias
decoder.layers.3.encoder_attn.k_proj.weight
decoder.layers.3.encoder_attn.k_proj.bias
decoder.layers.3.encoder_attn.v_proj.weight
decoder.layers.3.encoder_attn.v_proj.bias
decoder.layers.3.encoder_attn.q_proj.weight
decoder.layers.3.encoder_attn.q_proj.bias
decoder.layers.3.encoder_attn.out_proj.weight
decoder.layers.3.encoder_attn.out_proj.bias
decoder.layers.3.encoder_attn_layer_norm.weight
decoder.layers.3.encoder_attn_layer_norm.bias
decoder.layers.3.fc1.weight
decoder.layers.3.fc1.bias
decoder.layers.3.fc2.weight
decoder.layers.3.fc2.bias
decoder.layers.3.final_layer_norm.weight
decoder.layers.3.final_layer_norm.bias
decoder.layers.4.self_attn.k_proj.weight
decoder.layers.4.self_attn.k_proj.bias
decoder.layers.4.self_attn.v_proj.weight
decoder.layers.4.self_attn.v_proj.bias
decoder.layers.4.self_attn.q_proj.weight
decoder.layers.4.self_attn.q_proj.bias
decoder.layers.4.self_attn.out_proj.weight
decoder.layers.4.self_attn.out_proj.bias
decoder.layers.4.self_attn_layer_norm.weight
decoder.layers.4.self_attn_layer_norm.bias
decoder.layers.4.encoder_attn.k_proj.weight
decoder.layers.4.encoder_attn.k_proj.bias
decoder.layers.4.encoder_attn.v_proj.weight
decoder.layers.4.encoder_attn.v_proj.bias
decoder.layers.4.encoder_attn.q_proj.weight
decoder.layers.4.encoder_attn.q_proj.bias
decoder.layers.4.encoder_attn.out_proj.weight
decoder.layers.4.encoder_attn.out_proj.bias
decoder.layers.4.encoder_attn_layer_norm.weight
decoder.layers.4.encoder_attn_layer_norm.bias
decoder.layers.4.fc1.weight
decoder.layers.4.fc1.bias
decoder.layers.4.fc2.weight
decoder.layers.4.fc2.bias
decoder.layers.4.final_layer_norm.weight
decoder.layers.4.final_layer_norm.bias
decoder.layers.5.self_attn.k_proj.weight
decoder.layers.5.self_attn.k_proj.bias
decoder.layers.5.self_attn.v_proj.weight
decoder.layers.5.self_attn.v_proj.bias
decoder.layers.5.self_attn.q_proj.weight
decoder.layers.5.self_attn.q_proj.bias
decoder.layers.5.self_attn.out_proj.weight
decoder.layers.5.self_attn.out_proj.bias
decoder.layers.5.self_attn_layer_norm.weight
decoder.layers.5.self_attn_layer_norm.bias
decoder.layers.5.encoder_attn.k_proj.weight
decoder.layers.5.encoder_attn.k_proj.bias
decoder.layers.5.encoder_attn.v_proj.weight
decoder.layers.5.encoder_attn.v_proj.bias
decoder.layers.5.encoder_attn.q_proj.weight
decoder.layers.5.encoder_attn.q_proj.bias
decoder.layers.5.encoder_attn.out_proj.weight
decoder.layers.5.encoder_attn.out_proj.bias
decoder.layers.5.encoder_attn_layer_norm.weight
decoder.layers.5.encoder_attn_layer_norm.bias
decoder.layers.5.fc1.weight
decoder.layers.5.fc1.bias
decoder.layers.5.fc2.weight
decoder.layers.5.fc2.bias
decoder.layers.5.final_layer_norm.weight
decoder.layers.5.final_layer_norm.bias
decoder.layer_norm.weight
decoder.layer_norm.bias
all_params: 162.59M learnable_params: 59.45M
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
decoder.layers.3.self_attn_layer_norm.weight
decoder.layers.3.self_attn_layer_norm.bias
decoder.layers.3.encoder_attn.k_proj.weight
decoder.layers.3.encoder_attn.k_proj.bias
decoder.layers.3.encoder_attn.v_proj.weight
decoder.layers.3.encoder_attn.v_proj.bias
decoder.layers.3.encoder_attn.q_proj.weight
decoder.layers.3.encoder_attn.q_proj.bias
decoder.layers.3.encoder_attn.out_proj.weight
decoder.layers.3.encoder_attn.out_proj.bias
decoder.layers.3.encoder_attn_layer_norm.weight
decoder.layers.3.encoder_attn_layer_norm.bias
decoder.layers.3.fc1.weight
decoder.layers.3.fc1.bias
decoder.layers.3.fc2.weight
decoder.layers.3.fc2.bias
decoder.layers.3.final_layer_norm.weight
decoder.layers.3.final_layer_norm.bias
decoder.layers.4.self_attn.k_proj.weight
decoder.layers.4.self_attn.k_proj.bias
decoder.layers.4.self_attn.v_proj.weight
decoder.layers.4.self_attn.v_proj.bias
decoder.layers.4.self_attn.q_proj.weight
decoder.layers.4.self_attn.q_proj.bias
decoder.layers.4.self_attn.out_proj.weight
decoder.layers.4.self_attn.out_proj.bias
decoder.layers.4.self_attn_layer_norm.weight
decoder.layers.4.self_attn_layer_norm.bias
decoder.layers.4.encoder_attn.k_proj.weight
decoder.layers.4.encoder_attn.k_proj.bias
decoder.layers.4.encoder_attn.v_proj.weight
decoder.layers.4.encoder_attn.v_proj.bias
decoder.layers.4.encoder_attn.q_proj.weight
decoder.layers.4.encoder_attn.q_proj.bias
decoder.layers.4.encoder_attn.out_proj.weight
decoder.layers.4.encoder_attn.out_proj.bias
decoder.layers.4.encoder_attn_layer_norm.weight
decoder.layers.4.encoder_attn_layer_norm.bias
decoder.layers.4.fc1.weight
decoder.layers.4.fc1.bias
decoder.layers.4.fc2.weight
decoder.layers.4.fc2.bias
decoder.layers.4.final_layer_norm.weight
decoder.layers.4.final_layer_norm.bias
decoder.layers.5.self_attn.k_proj.weight
decoder.layers.5.self_attn.k_proj.bias
decoder.layers.5.self_attn.v_proj.weight
decoder.layers.5.self_attn.v_proj.bias
decoder.layers.5.self_attn.q_proj.weight
decoder.layers.5.self_attn.q_proj.bias
decoder.layers.5.self_attn.out_proj.weight
decoder.layers.5.self_attn.out_proj.bias
decoder.layers.5.self_attn_layer_norm.weight
decoder.layers.5.self_attn_layer_norm.bias
decoder.layers.5.encoder_attn.k_proj.weight
decoder.layers.5.encoder_attn.k_proj.bias
decoder.layers.5.encoder_attn.v_proj.weight
decoder.layers.5.encoder_attn.v_proj.bias
decoder.layers.5.encoder_attn.q_proj.weight
decoder.layers.5.encoder_attn.q_proj.bias
decoder.layers.5.encoder_attn.out_proj.weight
decoder.layers.5.encoder_attn.out_proj.bias
decoder.layers.5.encoder_attn_layer_norm.weight
decoder.layers.5.encoder_attn_layer_norm.bias
decoder.layers.5.fc1.weight
decoder.layers.5.fc1.bias
decoder.layers.5.fc2.weight
decoder.layers.5.fc2.bias
decoder.layers.5.final_layer_norm.weight
decoder.layers.5.final_layer_norm.bias
decoder.layer_norm.weight
decoder.layer_norm.bias
all_params: 162.59M learnable_params: 59.45M
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
decoder.layers.3.self_attn_layer_norm.weight
decoder.layers.3.self_attn_layer_norm.bias
decoder.layers.3.encoder_attn.k_proj.weight
decoder.layers.3.encoder_attn.k_proj.bias
decoder.layers.3.encoder_attn.v_proj.weight
decoder.layers.3.encoder_attn.v_proj.bias
decoder.layers.3.encoder_attn.q_proj.weight
decoder.layers.3.encoder_attn.q_proj.bias
decoder.layers.3.encoder_attn.out_proj.weight
decoder.layers.3.encoder_attn.out_proj.bias
decoder.layers.3.encoder_attn_layer_norm.weight
decoder.layers.3.encoder_attn_layer_norm.bias
decoder.layers.3.fc1.weight
decoder.layers.3.fc1.bias
decoder.layers.3.fc2.weight
decoder.layers.3.fc2.bias
decoder.layers.3.final_layer_norm.weight
decoder.layers.3.final_layer_norm.bias
decoder.layers.4.self_attn.k_proj.weight
decoder.layers.4.self_attn.k_proj.bias
decoder.layers.4.self_attn.v_proj.weight
decoder.layers.4.self_attn.v_proj.bias
decoder.layers.4.self_attn.q_proj.weight
decoder.layers.4.self_attn.q_proj.bias
decoder.layers.4.self_attn.out_proj.weight
decoder.layers.4.self_attn.out_proj.bias
decoder.layers.4.self_attn_layer_norm.weight
decoder.layers.4.self_attn_layer_norm.bias
decoder.layers.4.encoder_attn.k_proj.weight
decoder.layers.4.encoder_attn.k_proj.bias
decoder.layers.4.encoder_attn.v_proj.weight
decoder.layers.4.encoder_attn.v_proj.bias
decoder.layers.4.encoder_attn.q_proj.weight
decoder.layers.4.encoder_attn.q_proj.bias
decoder.layers.4.encoder_attn.out_proj.weight
decoder.layers.4.encoder_attn.out_proj.bias
decoder.layers.4.encoder_attn_layer_norm.weight
decoder.layers.4.encoder_attn_layer_norm.bias
decoder.layers.4.fc1.weight
decoder.layers.4.fc1.bias
decoder.layers.4.fc2.weight
decoder.layers.4.fc2.bias
decoder.layers.4.final_layer_norm.weight
decoder.layers.4.final_layer_norm.bias
decoder.layers.5.self_attn.k_proj.weight
decoder.layers.5.self_attn.k_proj.bias
decoder.layers.5.self_attn.v_proj.weight
decoder.layers.5.self_attn.v_proj.bias
decoder.layers.5.self_attn.q_proj.weight
decoder.layers.5.self_attn.q_proj.bias
decoder.layers.5.self_attn.out_proj.weight
decoder.layers.5.self_attn.out_proj.bias
decoder.layers.5.self_attn_layer_norm.weight
decoder.layers.5.self_attn_layer_norm.bias
decoder.layers.5.encoder_attn.k_proj.weight
decoder.layers.5.encoder_attn.k_proj.bias
decoder.layers.5.encoder_attn.v_proj.weight
decoder.layers.5.encoder_attn.v_proj.bias
decoder.layers.5.encoder_attn.q_proj.weight
decoder.layers.5.encoder_attn.q_proj.bias
decoder.layers.5.encoder_attn.out_proj.weight
decoder.layers.5.encoder_attn.out_proj.bias
decoder.layers.5.encoder_attn_layer_norm.weight
decoder.layers.5.encoder_attn_layer_norm.bias
decoder.layers.5.fc1.weight
decoder.layers.5.fc1.bias
decoder.layers.5.fc2.weight
decoder.layers.5.fc2.bias
decoder.layers.5.final_layer_norm.weight
decoder.layers.5.final_layer_norm.bias
decoder.layer_norm.weight
decoder.layer_norm.bias
all_params: 162.59M learnable_params: 59.45M
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
[2025-02-02 16:46:15,941][train_inner][INFO] - {"epoch": 35, "update": 34.034, "loss": "108.062", "nll_loss": "3.496", "total": "667.01", "n_correct": "413.255", "ppl": "11.29", "accuracy": "61.956", "wps": "667.2", "ups": "1", "wpb": "667", "bsz": "27.2", "num_updates": "200", "lr": "2.32e-05", "gnorm": "37.162", "loss_scale": "128", "train_wall": "219", "gb_free": "10.1", "wall": "257"}
[2025-02-02 16:49:30,603][train_inner][INFO] - {"epoch": 35, "update": 34.068, "loss": "95.797", "nll_loss": "3.066", "total": "666.2", "n_correct": "430.28", "ppl": "8.37", "accuracy": "64.587", "wps": "684.6", "ups": "1.03", "wpb": "666.2", "bsz": "27.8", "num_updates": "400", "lr": "3.64e-05", "gnorm": "26.742", "loss_scale": "128", "train_wall": "187", "gb_free": "10", "wall": "452"}
[2025-02-02 16:52:42,296][train_inner][INFO] - {"epoch": 35, "update": 34.102, "loss": "94.087", "nll_loss": "2.933", "total": "668.24", "n_correct": "433.61", "ppl": "7.64", "accuracy": "64.888", "wps": "697.3", "ups": "1.04", "wpb": "668.2", "bsz": "27.6", "num_updates": "600", "lr": "4.96e-05", "gnorm": "22.884", "loss_scale": "128", "train_wall": "184", "gb_free": "10", "wall": "644"}
[2025-02-02 16:55:47,824][train_inner][INFO] - {"epoch": 35, "update": 34.136, "loss": "96.114", "nll_loss": "2.898", "total": "660.64", "n_correct": "425.955", "ppl": "7.45", "accuracy": "64.476", "wps": "712.4", "ups": "1.08", "wpb": "660.6", "bsz": "26.6", "num_updates": "800", "lr": "6.28e-05", "gnorm": "22.827", "loss_scale": "128", "train_wall": "178", "gb_free": "10", "wall": "829"}
[2025-02-02 16:58:56,274][train_inner][INFO] - {"epoch": 35, "update": 34.171, "loss": "90.836", "nll_loss": "2.778", "total": "664.395", "n_correct": "434.72", "ppl": "6.86", "accuracy": "65.431", "wps": "705.2", "ups": "1.06", "wpb": "664.4", "bsz": "27.6", "num_updates": "1000", "lr": "7.6e-05", "gnorm": "20.609", "loss_scale": "128", "train_wall": "182", "gb_free": "10", "wall": "1018"}
[2025-02-02 17:02:03,934][train_inner][INFO] - {"epoch": 35, "update": 34.205, "loss": "92.64", "nll_loss": "2.787", "total": "666.46", "n_correct": "432.725", "ppl": "6.9", "accuracy": "64.929", "wps": "710.3", "ups": "1.07", "wpb": "666.5", "bsz": "27.2", "num_updates": "1200", "lr": "8.92e-05", "gnorm": "21.667", "loss_scale": "128", "train_wall": "181", "gb_free": "9.9", "wall": "1205"}
[2025-02-02 17:05:14,019][train_inner][INFO] - {"epoch": 35, "update": 34.239, "loss": "87.173", "nll_loss": "2.601", "total": "668.475", "n_correct": "445.8", "ppl": "6.07", "accuracy": "66.689", "wps": "703.4", "ups": "1.05", "wpb": "668.5", "bsz": "27.8", "num_updates": "1400", "lr": "0.0001024", "gnorm": "22.082", "loss_scale": "128", "train_wall": "182", "gb_free": "10", "wall": "1395"}
[2025-02-02 17:08:27,322][train_inner][INFO] - {"epoch": 35, "update": 34.273, "loss": "82.692", "nll_loss": "2.445", "total": "667.59", "n_correct": "456.215", "ppl": "5.45", "accuracy": "68.338", "wps": "690.7", "ups": "1.03", "wpb": "667.6", "bsz": "28.2", "num_updates": "1600", "lr": "0.0001156", "gnorm": "19.52", "loss_scale": "128", "train_wall": "186", "gb_free": "10", "wall": "1589"}
[2025-02-02 17:11:40,569][train_inner][INFO] - {"epoch": 35, "update": 34.307, "loss": "78.666", "nll_loss": "2.373", "total": "659.85", "n_correct": "455.515", "ppl": "5.18", "accuracy": "69.033", "wps": "683", "ups": "1.04", "wpb": "659.9", "bsz": "28.8", "num_updates": "1800", "lr": "0.0001288", "gnorm": "19.873", "loss_scale": "128", "train_wall": "186", "gb_free": "9.9", "wall": "1782"}
[2025-02-02 17:14:55,837][train_inner][INFO] - {"epoch": 35, "update": 34.341, "loss": "77.269", "nll_loss": "2.263", "total": "668.44", "n_correct": "469.555", "ppl": "4.8", "accuracy": "70.246", "wps": "684.7", "ups": "1.02", "wpb": "668.4", "bsz": "28.9", "num_updates": "2000", "lr": "0.000142", "gnorm": "17.98", "loss_scale": "128", "train_wall": "187", "gb_free": "10.1", "wall": "1977"}
[2025-02-02 17:18:09,021][train_inner][INFO] - {"epoch": 35, "update": 34.375, "loss": "77.838", "nll_loss": "2.27", "total": "667.925", "n_correct": "467.46", "ppl": "4.82", "accuracy": "69.987", "wps": "691.6", "ups": "1.04", "wpb": "667.9", "bsz": "28.7", "num_updates": "2200", "lr": "0.0001552", "gnorm": "18.638", "loss_scale": "128", "train_wall": "185", "gb_free": "9.9", "wall": "2170"}
[2025-02-02 17:21:28,054][train_inner][INFO] - {"epoch": 35, "update": 34.409, "loss": "73.458", "nll_loss": "2.203", "total": "669.69", "n_correct": "473.96", "ppl": "4.6", "accuracy": "70.773", "wps": "673.1", "ups": "1.01", "wpb": "669.7", "bsz": "30", "num_updates": "2400", "lr": "0.0001684", "gnorm": "17.979", "loss_scale": "128", "train_wall": "191", "gb_free": "10", "wall": "2369"}
[2025-02-02 17:24:37,539][train_inner][INFO] - {"epoch": 35, "update": 34.444, "loss": "81.745", "nll_loss": "2.253", "total": "666.005", "n_correct": "465.1", "ppl": "4.77", "accuracy": "69.834", "wps": "703", "ups": "1.06", "wpb": "666", "bsz": "27.2", "num_updates": "2600", "lr": "0.0001816", "gnorm": "19.24", "loss_scale": "128", "train_wall": "182", "gb_free": "10", "wall": "2559"}
[2025-02-02 17:27:48,341][train_inner][INFO] - {"epoch": 35, "update": 34.478, "loss": "77.889", "nll_loss": "2.208", "total": "668.785", "n_correct": "471.54", "ppl": "4.62", "accuracy": "70.507", "wps": "701.1", "ups": "1.05", "wpb": "668.8", "bsz": "28.3", "num_updates": "2800", "lr": "0.0001948", "gnorm": "18.307", "loss_scale": "128", "train_wall": "184", "gb_free": "10", "wall": "2750"}
[2025-02-02 17:30:58,033][train_inner][INFO] - {"epoch": 35, "update": 34.512, "loss": "79.475", "nll_loss": "2.195", "total": "664.785", "n_correct": "467.16", "ppl": "4.58", "accuracy": "70.272", "wps": "701.1", "ups": "1.05", "wpb": "664.8", "bsz": "27.5", "num_updates": "3000", "lr": "0.000208", "gnorm": "18.098", "loss_scale": "128", "train_wall": "183", "gb_free": "10.1", "wall": "2939"}
[2025-02-02 17:34:07,843][train_inner][INFO] - {"epoch": 35, "update": 34.546, "loss": "79.807", "nll_loss": "2.174", "total": "670.51", "n_correct": "472.95", "ppl": "4.51", "accuracy": "70.536", "wps": "706.5", "ups": "1.05", "wpb": "670.5", "bsz": "27.4", "num_updates": "3200", "lr": "0.0002212", "gnorm": "18.151", "loss_scale": "128", "train_wall": "183", "gb_free": "9.9", "wall": "3129"}
[2025-02-02 17:37:17,967][train_inner][INFO] - {"epoch": 35, "update": 34.58, "loss": "78.822", "nll_loss": "2.154", "total": "663.105", "n_correct": "470.205", "ppl": "4.45", "accuracy": "70.91", "wps": "697.6", "ups": "1.05", "wpb": "663.1", "bsz": "27.3", "num_updates": "3400", "lr": "0.0002344", "gnorm": "17.026", "loss_scale": "128", "train_wall": "183", "gb_free": "10.1", "wall": "3319"}
[2025-02-02 17:40:26,008][train_inner][INFO] - {"epoch": 35, "update": 34.614, "loss": "77.697", "nll_loss": "2.162", "total": "660.85", "n_correct": "466.66", "ppl": "4.48", "accuracy": "70.615", "wps": "702.9", "ups": "1.06", "wpb": "660.9", "bsz": "27.7", "num_updates": "3600", "lr": "0.0002476", "gnorm": "18.16", "loss_scale": "128", "train_wall": "181", "gb_free": "10", "wall": "3507"}
[2025-02-02 17:43:34,852][train_inner][INFO] - {"epoch": 35, "update": 34.648, "loss": "77.161", "nll_loss": "2.067", "total": "666.98", "n_correct": "478.845", "ppl": "4.19", "accuracy": "71.793", "wps": "706.4", "ups": "1.06", "wpb": "667", "bsz": "27.5", "num_updates": "3800", "lr": "0.0002608", "gnorm": "16.954", "loss_scale": "128", "train_wall": "182", "gb_free": "10.1", "wall": "3696"}
[2025-02-02 17:46:48,964][train_inner][INFO] - {"epoch": 35, "update": 34.682, "loss": "71.48", "nll_loss": "2.064", "total": "664.8", "n_correct": "477.82", "ppl": "4.18", "accuracy": "71.874", "wps": "685.2", "ups": "1.03", "wpb": "664.8", "bsz": "29.5", "num_updates": "4000", "lr": "0.000274", "gnorm": "16.033", "loss_scale": "128", "train_wall": "187", "gb_free": "10", "wall": "3890"}
[2025-02-02 17:49:59,507][train_inner][INFO] - {"epoch": 35, "update": 34.716, "loss": "75.128", "nll_loss": "2.125", "total": "662.295", "n_correct": "470.415", "ppl": "4.36", "accuracy": "71.028", "wps": "695.2", "ups": "1.05", "wpb": "662.3", "bsz": "28.5", "num_updates": "4200", "lr": "0.0002872", "gnorm": "16.283", "loss_scale": "256", "train_wall": "183", "gb_free": "10", "wall": "4081"}
[2025-02-02 17:53:08,450][train_inner][INFO] - {"epoch": 35, "update": 34.751, "loss": "75.657", "nll_loss": "2.038", "total": "662.595", "n_correct": "477.31", "ppl": "4.11", "accuracy": "72.036", "wps": "701.4", "ups": "1.06", "wpb": "662.6", "bsz": "27.6", "num_updates": "4400", "lr": "0.0003004", "gnorm": "17.706", "loss_scale": "256", "train_wall": "182", "gb_free": "10", "wall": "4270"}
[2025-02-02 17:56:18,811][train_inner][INFO] - {"epoch": 35, "update": 34.785, "loss": "75.721", "nll_loss": "2.082", "total": "662.805", "n_correct": "473.515", "ppl": "4.24", "accuracy": "71.441", "wps": "696.4", "ups": "1.05", "wpb": "662.8", "bsz": "27.9", "num_updates": "4600", "lr": "0.0003136", "gnorm": "16.752", "loss_scale": "256", "train_wall": "183", "gb_free": "9.9", "wall": "4460"}
[2025-02-02 17:59:32,464][train_inner][INFO] - {"epoch": 35, "update": 34.819, "loss": "75.279", "nll_loss": "2.054", "total": "669.46", "n_correct": "480.8", "ppl": "4.15", "accuracy": "71.819", "wps": "691.5", "ups": "1.03", "wpb": "669.5", "bsz": "28.2", "num_updates": "4800", "lr": "0.0003268", "gnorm": "16.168", "loss_scale": "256", "train_wall": "187", "gb_free": "9.9", "wall": "4654"}
[2025-02-02 18:02:38,753][train_inner][INFO] - {"epoch": 35, "update": 34.853, "loss": "78.254", "nll_loss": "2.07", "total": "670.785", "n_correct": "480.015", "ppl": "4.2", "accuracy": "71.56", "wps": "720.2", "ups": "1.07", "wpb": "670.8", "bsz": "27.3", "num_updates": "5000", "lr": "0.00034", "gnorm": "17.397", "loss_scale": "256", "train_wall": "180", "gb_free": "9.9", "wall": "4840"}
[2025-02-02 18:05:47,476][train_inner][INFO] - {"epoch": 35, "update": 34.887, "loss": "76.543", "nll_loss": "2.064", "total": "662.175", "n_correct": "474.26", "ppl": "4.18", "accuracy": "71.622", "wps": "701.8", "ups": "1.06", "wpb": "662.2", "bsz": "27.5", "num_updates": "5200", "lr": "0.0003532", "gnorm": "16.103", "loss_scale": "256", "train_wall": "181", "gb_free": "9.9", "wall": "5029"}
[2025-02-02 18:08:56,436][train_inner][INFO] - {"epoch": 35, "update": 34.921, "loss": "76.082", "nll_loss": "2.023", "total": "661.95", "n_correct": "477.115", "ppl": "4.06", "accuracy": "72.077", "wps": "700.7", "ups": "1.06", "wpb": "662", "bsz": "27.3", "num_updates": "5400", "lr": "0.0003664", "gnorm": "16.682", "loss_scale": "256", "train_wall": "182", "gb_free": "10", "wall": "5218"}
[2025-02-02 18:12:05,892][train_inner][INFO] - {"epoch": 35, "update": 34.955, "loss": "74.718", "nll_loss": "2.021", "total": "667.505", "n_correct": "481.265", "ppl": "4.06", "accuracy": "72.099", "wps": "704.8", "ups": "1.06", "wpb": "667.5", "bsz": "28.1", "num_updates": "5600", "lr": "0.0003796", "gnorm": "16.11", "loss_scale": "256", "train_wall": "182", "gb_free": "9.9", "wall": "5407"}
[2025-02-02 18:15:20,348][train_inner][INFO] - {"epoch": 35, "update": 34.989, "loss": "70.811", "nll_loss": "1.966", "total": "666.935", "n_correct": "486.07", "ppl": "3.91", "accuracy": "72.881", "wps": "686", "ups": "1.03", "wpb": "666.9", "bsz": "29.2", "num_updates": "5800", "lr": "0.0003928", "gnorm": "15.315", "loss_scale": "256", "train_wall": "188", "gb_free": "9.9", "wall": "5602"}
[2025-02-02 18:16:04,825][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-02-02 18:16:04,829][train][INFO] - {"epoch": 35, "train_loss": "81.05", "train_nll_loss": "2.336", "train_total": "665.831", "train_n_correct": "462.673", "train_ppl": "5.05", "train_accuracy": "69.488", "train_wps": "698.4", "train_ups": "1.05", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "5862", "train_lr": "0.000396892", "train_gnorm": "19.21", "train_loss_scale": "256", "train_train_wall": "5402", "train_gb_free": "10", "train_wall": "5646"}
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2025-02-02 18:16:05,970][fairseq.trainer][INFO] - begin training epoch 36
[2025-02-02 18:16:05,971][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2025-02-02 18:18:57,882][train_inner][INFO] - {"epoch": 36, "update": 35.024, "loss": "73.245", "nll_loss": "1.962", "total": "670.2", "n_correct": "487.335", "ppl": "3.9", "accuracy": "72.715", "wps": "616.2", "ups": "0.92", "wpb": "670.2", "bsz": "28.3", "num_updates": "6000", "lr": "0.000406", "gnorm": "15.486", "loss_scale": "256", "train_wall": "187", "gb_free": "10", "wall": "5819"}
[2025-02-02 18:22:05,909][train_inner][INFO] - {"epoch": 36, "update": 35.058, "loss": "76.069", "nll_loss": "1.935", "total": "664.47", "n_correct": "485.52", "ppl": "3.82", "accuracy": "73.069", "wps": "706.8", "ups": "1.06", "wpb": "664.5", "bsz": "26.8", "num_updates": "6200", "lr": "0.0004192", "gnorm": "16.153", "loss_scale": "256", "train_wall": "180", "gb_free": "10", "wall": "6007"}
[2025-02-02 18:25:21,750][train_inner][INFO] - {"epoch": 36, "update": 35.092, "loss": "70.253", "nll_loss": "1.887", "total": "673.895", "n_correct": "495.915", "ppl": "3.7", "accuracy": "73.589", "wps": "688.3", "ups": "1.02", "wpb": "673.9", "bsz": "29.1", "num_updates": "6400", "lr": "0.0004324", "gnorm": "15.234", "loss_scale": "256", "train_wall": "188", "gb_free": "10.1", "wall": "6203"}
[2025-02-02 18:28:32,255][train_inner][INFO] - {"epoch": 36, "update": 35.126, "loss": "72.92", "nll_loss": "1.948", "total": "659.645", "n_correct": "480.62", "ppl": "3.86", "accuracy": "72.86", "wps": "692.6", "ups": "1.05", "wpb": "659.6", "bsz": "27.9", "num_updates": "6600", "lr": "0.0004456", "gnorm": "15.673", "loss_scale": "256", "train_wall": "183", "gb_free": "9.9", "wall": "6394"}
[2025-02-02 18:31:49,877][train_inner][INFO] - {"epoch": 36, "update": 35.16, "loss": "69.285", "nll_loss": "1.909", "total": "668.31", "n_correct": "490.665", "ppl": "3.76", "accuracy": "73.419", "wps": "676.4", "ups": "1.01", "wpb": "668.3", "bsz": "29.4", "num_updates": "6800", "lr": "0.0004588", "gnorm": "15.122", "loss_scale": "256", "train_wall": "190", "gb_free": "10.1", "wall": "6591"}
[2025-02-02 18:34:55,174][train_inner][INFO] - {"epoch": 36, "update": 35.194, "loss": "73.592", "nll_loss": "1.931", "total": "661.41", "n_correct": "483.86", "ppl": "3.81", "accuracy": "73.156", "wps": "714", "ups": "1.08", "wpb": "661.4", "bsz": "27.6", "num_updates": "7000", "lr": "0.000472", "gnorm": "15.57", "loss_scale": "256", "train_wall": "179", "gb_free": "9.9", "wall": "6777"}
[2025-02-02 18:38:10,198][train_inner][INFO] - {"epoch": 36, "update": 35.228, "loss": "69.123", "nll_loss": "1.883", "total": "663.99", "n_correct": "489.465", "ppl": "3.69", "accuracy": "73.716", "wps": "681", "ups": "1.03", "wpb": "664", "bsz": "29.1", "num_updates": "7200", "lr": "0.0004852", "gnorm": "14.508", "loss_scale": "256", "train_wall": "187", "gb_free": "10", "wall": "6972"}
[2025-02-02 18:41:19,626][train_inner][INFO] - {"epoch": 36, "update": 35.262, "loss": "72.048", "nll_loss": "1.902", "total": "667.585", "n_correct": "490.765", "ppl": "3.74", "accuracy": "73.513", "wps": "705", "ups": "1.06", "wpb": "667.6", "bsz": "28.2", "num_updates": "7400", "lr": "0.0004984", "gnorm": "14.89", "loss_scale": "256", "train_wall": "181", "gb_free": "10", "wall": "7161"}
[2025-02-02 18:44:35,551][train_inner][INFO] - {"epoch": 36, "update": 35.296, "loss": "71.533", "nll_loss": "1.956", "total": "667.6", "n_correct": "484.78", "ppl": "3.88", "accuracy": "72.615", "wps": "681.5", "ups": "1.02", "wpb": "667.6", "bsz": "28.9", "num_updates": "7600", "lr": "0.0005116", "gnorm": "14.894", "loss_scale": "256", "train_wall": "188", "gb_free": "10.1", "wall": "7357"}
[2025-02-02 18:47:45,923][train_inner][INFO] - {"epoch": 36, "update": 35.331, "loss": "75.107", "nll_loss": "1.963", "total": "659.53", "n_correct": "478.92", "ppl": "3.9", "accuracy": "72.615", "wps": "692.9", "ups": "1.05", "wpb": "659.5", "bsz": "27.2", "num_updates": "7800", "lr": "0.0005248", "gnorm": "15.576", "loss_scale": "256", "train_wall": "183", "gb_free": "10.1", "wall": "7547"}
[2025-02-02 18:50:50,841][train_inner][INFO] - {"epoch": 36, "update": 35.365, "loss": "77.632", "nll_loss": "2.013", "total": "668.605", "n_correct": "479.855", "ppl": "4.04", "accuracy": "71.77", "wps": "723.2", "ups": "1.08", "wpb": "668.6", "bsz": "27.1", "num_updates": "8000", "lr": "0.000538", "gnorm": "15.821", "loss_scale": "256", "train_wall": "178", "gb_free": "10.1", "wall": "7732"}
[2025-02-02 18:54:03,947][train_inner][INFO] - {"epoch": 36, "update": 35.399, "loss": "70.302", "nll_loss": "1.9", "total": "663.165", "n_correct": "486.98", "ppl": "3.73", "accuracy": "73.433", "wps": "686.9", "ups": "1.04", "wpb": "663.2", "bsz": "28.7", "num_updates": "8200", "lr": "0.0005512", "gnorm": "14.646", "loss_scale": "512", "train_wall": "186", "gb_free": "10", "wall": "7925"}
[2025-02-02 18:57:19,140][train_inner][INFO] - {"epoch": 36, "update": 35.433, "loss": "68.928", "nll_loss": "1.898", "total": "663.55", "n_correct": "487.585", "ppl": "3.73", "accuracy": "73.481", "wps": "679.9", "ups": "1.02", "wpb": "663.6", "bsz": "29.3", "num_updates": "8400", "lr": "0.0005644", "gnorm": "13.939", "loss_scale": "512", "train_wall": "188", "gb_free": "10", "wall": "8121"}
[2025-02-02 19:00:24,781][train_inner][INFO] - {"epoch": 36, "update": 35.467, "loss": "75.079", "nll_loss": "1.945", "total": "659.685", "n_correct": "480.715", "ppl": "3.85", "accuracy": "72.87", "wps": "710.8", "ups": "1.08", "wpb": "659.7", "bsz": "27.1", "num_updates": "8600", "lr": "0.0005776", "gnorm": "15.127", "loss_scale": "512", "train_wall": "178", "gb_free": "10", "wall": "8306"}
[2025-02-02 19:03:33,331][train_inner][INFO] - {"epoch": 36, "update": 35.501, "loss": "76.049", "nll_loss": "1.94", "total": "665.305", "n_correct": "484.495", "ppl": "3.84", "accuracy": "72.823", "wps": "705.7", "ups": "1.06", "wpb": "665.3", "bsz": "27", "num_updates": "8800", "lr": "0.0005908", "gnorm": "15.515", "loss_scale": "512", "train_wall": "181", "gb_free": "10.1", "wall": "8495"}
[2025-02-02 19:06:39,785][train_inner][INFO] - {"epoch": 36, "update": 35.535, "loss": "72.909", "nll_loss": "1.898", "total": "664.495", "n_correct": "487.46", "ppl": "3.73", "accuracy": "73.358", "wps": "712.8", "ups": "1.07", "wpb": "664.5", "bsz": "27.8", "num_updates": "9000", "lr": "0.000604", "gnorm": "14.982", "loss_scale": "512", "train_wall": "179", "gb_free": "10", "wall": "8681"}
[2025-02-02 19:09:49,992][train_inner][INFO] - {"epoch": 36, "update": 35.569, "loss": "75.497", "nll_loss": "1.961", "total": "668.44", "n_correct": "484.69", "ppl": "3.89", "accuracy": "72.511", "wps": "702.9", "ups": "1.05", "wpb": "668.4", "bsz": "27.5", "num_updates": "9200", "lr": "0.0006172", "gnorm": "15.172", "loss_scale": "512", "train_wall": "184", "gb_free": "10", "wall": "8871"}
[2025-02-02 19:12:59,718][train_inner][INFO] - {"epoch": 36, "update": 35.604, "loss": "70.108", "nll_loss": "1.848", "total": "672.345", "n_correct": "498.475", "ppl": "3.6", "accuracy": "74.14", "wps": "708.8", "ups": "1.05", "wpb": "672.3", "bsz": "28.8", "num_updates": "9400", "lr": "0.0006304", "gnorm": "13.495", "loss_scale": "512", "train_wall": "182", "gb_free": "10", "wall": "9061"}
[2025-02-02 19:16:06,911][train_inner][INFO] - {"epoch": 36, "update": 35.638, "loss": "75.572", "nll_loss": "1.926", "total": "663.925", "n_correct": "484.84", "ppl": "3.8", "accuracy": "73.026", "wps": "709.4", "ups": "1.07", "wpb": "663.9", "bsz": "27", "num_updates": "9600", "lr": "0.0006436", "gnorm": "14.605", "loss_scale": "512", "train_wall": "180", "gb_free": "9.9", "wall": "9248"}
[2025-02-02 19:19:18,370][train_inner][INFO] - {"epoch": 36, "update": 35.672, "loss": "72.392", "nll_loss": "1.905", "total": "668.295", "n_correct": "489.37", "ppl": "3.75", "accuracy": "73.227", "wps": "698.2", "ups": "1.04", "wpb": "668.3", "bsz": "28.2", "num_updates": "9800", "lr": "0.0006568", "gnorm": "14.762", "loss_scale": "512", "train_wall": "185", "gb_free": "10", "wall": "9440"}
[2025-02-02 19:22:21,986][train_inner][INFO] - {"epoch": 36, "update": 35.706, "loss": "74.61", "nll_loss": "1.905", "total": "663.855", "n_correct": "486.845", "ppl": "3.75", "accuracy": "73.336", "wps": "723.1", "ups": "1.09", "wpb": "663.9", "bsz": "27.2", "num_updates": "10000", "lr": "0.00067", "gnorm": "14.388", "loss_scale": "512", "train_wall": "177", "gb_free": "10", "wall": "9623"}
[2025-02-02 19:25:32,545][train_inner][INFO] - {"epoch": 36, "update": 35.74, "loss": "74.089", "nll_loss": "1.956", "total": "670.25", "n_correct": "485.695", "ppl": "3.88", "accuracy": "72.465", "wps": "703.5", "ups": "1.05", "wpb": "670.2", "bsz": "28", "num_updates": "10200", "lr": "0.0006832", "gnorm": "14.511", "loss_scale": "512", "train_wall": "184", "gb_free": "10", "wall": "9814"}
[2025-02-02 19:28:42,301][train_inner][INFO] - {"epoch": 36, "update": 35.774, "loss": "72.157", "nll_loss": "1.904", "total": "667.165", "n_correct": "488.95", "ppl": "3.74", "accuracy": "73.288", "wps": "703.3", "ups": "1.05", "wpb": "667.2", "bsz": "28.2", "num_updates": "10400", "lr": "0.0006964", "gnorm": "14.271", "loss_scale": "512", "train_wall": "183", "gb_free": "10", "wall": "10004"}
[2025-02-02 19:31:51,058][train_inner][INFO] - {"epoch": 36, "update": 35.808, "loss": "73.607", "nll_loss": "1.94", "total": "665.71", "n_correct": "485.065", "ppl": "3.84", "accuracy": "72.864", "wps": "705.4", "ups": "1.06", "wpb": "665.7", "bsz": "27.9", "num_updates": "10600", "lr": "0.0007096", "gnorm": "14.647", "loss_scale": "512", "train_wall": "181", "gb_free": "9.9", "wall": "10193"}
[2025-02-02 19:34:58,304][train_inner][INFO] - {"epoch": 36, "update": 35.842, "loss": "75.625", "nll_loss": "1.911", "total": "669.74", "n_correct": "490.12", "ppl": "3.76", "accuracy": "73.181", "wps": "715.5", "ups": "1.07", "wpb": "669.7", "bsz": "27.1", "num_updates": "10800", "lr": "0.0007228", "gnorm": "14.42", "loss_scale": "512", "train_wall": "180", "gb_free": "10", "wall": "10380"}
[2025-02-02 19:38:13,108][train_inner][INFO] - {"epoch": 36, "update": 35.876, "loss": "69.741", "nll_loss": "1.887", "total": "669.98", "n_correct": "492.865", "ppl": "3.7", "accuracy": "73.564", "wps": "687.9", "ups": "1.03", "wpb": "670", "bsz": "29.2", "num_updates": "11000", "lr": "0.000736", "gnorm": "12.915", "loss_scale": "512", "train_wall": "187", "gb_free": "9.9", "wall": "10575"}
[2025-02-02 19:41:17,365][train_inner][INFO] - {"epoch": 36, "update": 35.911, "loss": "74.894", "nll_loss": "1.891", "total": "665.875", "n_correct": "488.665", "ppl": "3.71", "accuracy": "73.387", "wps": "723", "ups": "1.09", "wpb": "665.9", "bsz": "27.1", "num_updates": "11200", "lr": "0.0007492", "gnorm": "13.93", "loss_scale": "512", "train_wall": "178", "gb_free": "10", "wall": "10759"}
[2025-02-02 19:44:24,460][train_inner][INFO] - {"epoch": 36, "update": 35.945, "loss": "74.333", "nll_loss": "1.909", "total": "658.37", "n_correct": "482.03", "ppl": "3.75", "accuracy": "73.216", "wps": "704.3", "ups": "1.07", "wpb": "658.4", "bsz": "27.1", "num_updates": "11400", "lr": "0.0007624", "gnorm": "14.015", "loss_scale": "512", "train_wall": "180", "gb_free": "10", "wall": "10946"}
[2025-02-02 19:47:33,742][train_inner][INFO] - {"epoch": 36, "update": 35.979, "loss": "71.883", "nll_loss": "1.89", "total": "668.045", "n_correct": "490.495", "ppl": "3.71", "accuracy": "73.422", "wps": "705.9", "ups": "1.06", "wpb": "668", "bsz": "28.3", "num_updates": "11600", "lr": "0.0007756", "gnorm": "13.245", "loss_scale": "512", "train_wall": "183", "gb_free": "10.1", "wall": "11135"}
[2025-02-02 19:49:14,937][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-02 19:50:06,317][valid][INFO] - {"epoch": 36, "valid_loss": "29.312", "valid_nll_loss": "1.07", "valid_total": "678.4", "valid_n_correct": "574.8", "valid_ppl": "2.1", "valid_accuracy": "84.729", "valid_wps": "534.9", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "11724"}
[2025-02-02 19:50:06,319][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 11724 updates
[2025-02-02 19:50:06,320][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 19:50:09,534][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 19:50:12,911][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 36 @ 11724 updates, score 84.729) (writing took 6.591517334803939 seconds)
[2025-02-02 19:50:12,912][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-02-02 19:50:12,917][train][INFO] - {"epoch": 36, "train_loss": "72.966", "train_nll_loss": "1.918", "train_total": "665.831", "train_n_correct": "487.081", "train_ppl": "3.78", "train_accuracy": "73.154", "train_wps": "691", "train_ups": "1.04", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "11724", "train_lr": "0.000783784", "train_gnorm": "14.702", "train_loss_scale": "512", "train_train_wall": "5355", "train_gb_free": "10", "train_wall": "11294"}
[2025-02-02 19:50:13,464][fairseq.trainer][INFO] - begin training epoch 37
[2025-02-02 19:50:13,464][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-02 19:52:04,258][train_inner][INFO] - {"epoch": 37, "update": 36.013, "loss": "72.608", "nll_loss": "1.817", "total": "660.6", "n_correct": "490.865", "ppl": "3.52", "accuracy": "74.306", "wps": "488.4", "ups": "0.74", "wpb": "660.6", "bsz": "27.1", "num_updates": "11800", "lr": "0.0007888", "gnorm": "13.451", "loss_scale": "512", "train_wall": "168", "gb_free": "9.9", "wall": "11406"}
[2025-02-02 19:55:22,126][train_inner][INFO] - {"epoch": 37, "update": 36.047, "loss": "69.318", "nll_loss": "1.794", "total": "669.275", "n_correct": "498.6", "ppl": "3.47", "accuracy": "74.499", "wps": "676.5", "ups": "1.01", "wpb": "669.3", "bsz": "28.6", "num_updates": "12000", "lr": "0.000802", "gnorm": "12.703", "loss_scale": "512", "train_wall": "191", "gb_free": "9.9", "wall": "11604"}
[2025-02-02 19:58:29,503][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
[2025-02-02 19:58:33,807][train_inner][INFO] - {"epoch": 37, "update": 36.081, "loss": "69.191", "nll_loss": "1.806", "total": "670.265", "n_correct": "497.925", "ppl": "3.5", "accuracy": "74.288", "wps": "699.4", "ups": "1.04", "wpb": "670.3", "bsz": "28.8", "num_updates": "12200", "lr": "0.0008152", "gnorm": "13.238", "loss_scale": "256", "train_wall": "184", "gb_free": "10", "wall": "11795"}
[2025-02-02 20:01:43,323][train_inner][INFO] - {"epoch": 37, "update": 36.115, "loss": "72.018", "nll_loss": "1.808", "total": "662.785", "n_correct": "492.705", "ppl": "3.5", "accuracy": "74.339", "wps": "699.5", "ups": "1.06", "wpb": "662.8", "bsz": "27.4", "num_updates": "12400", "lr": "0.0008284", "gnorm": "13.635", "loss_scale": "256", "train_wall": "182", "gb_free": "10", "wall": "11985"}
[2025-02-02 20:04:49,103][train_inner][INFO] - {"epoch": 37, "update": 36.15, "loss": "72.771", "nll_loss": "1.83", "total": "670.78", "n_correct": "496.255", "ppl": "3.56", "accuracy": "73.982", "wps": "722.4", "ups": "1.08", "wpb": "670.8", "bsz": "27.6", "num_updates": "12600", "lr": "0.0008416", "gnorm": "13.383", "loss_scale": "256", "train_wall": "179", "gb_free": "10", "wall": "12171"}
[2025-02-02 20:08:00,541][train_inner][INFO] - {"epoch": 37, "update": 36.184, "loss": "69.505", "nll_loss": "1.787", "total": "668.855", "n_correct": "499.095", "ppl": "3.45", "accuracy": "74.619", "wps": "698.8", "ups": "1.04", "wpb": "668.9", "bsz": "28.4", "num_updates": "12800", "lr": "0.0008548", "gnorm": "13.014", "loss_scale": "256", "train_wall": "184", "gb_free": "10", "wall": "12362"}
[2025-02-02 20:11:07,908][train_inner][INFO] - {"epoch": 37, "update": 36.218, "loss": "72.473", "nll_loss": "1.82", "total": "663.675", "n_correct": "492.725", "ppl": "3.53", "accuracy": "74.242", "wps": "708.5", "ups": "1.07", "wpb": "663.7", "bsz": "27.4", "num_updates": "13000", "lr": "0.000868", "gnorm": "12.958", "loss_scale": "256", "train_wall": "181", "gb_free": "9.9", "wall": "12549"}
[2025-02-02 20:14:18,739][train_inner][INFO] - {"epoch": 37, "update": 36.252, "loss": "70.793", "nll_loss": "1.807", "total": "664.49", "n_correct": "495.145", "ppl": "3.5", "accuracy": "74.515", "wps": "696.5", "ups": "1.05", "wpb": "664.5", "bsz": "27.9", "num_updates": "13200", "lr": "0.0008812", "gnorm": "12.923", "loss_scale": "256", "train_wall": "183", "gb_free": "10", "wall": "12740"}
[2025-02-02 20:17:32,520][train_inner][INFO] - {"epoch": 37, "update": 36.286, "loss": "68.811", "nll_loss": "1.786", "total": "669.545", "n_correct": "500.53", "ppl": "3.45", "accuracy": "74.757", "wps": "691.1", "ups": "1.03", "wpb": "669.5", "bsz": "28.8", "num_updates": "13400", "lr": "0.0008944", "gnorm": "12.834", "loss_scale": "256", "train_wall": "186", "gb_free": "9.9", "wall": "12934"}
[2025-02-02 20:20:45,475][train_inner][INFO] - {"epoch": 37, "update": 36.32, "loss": "68.512", "nll_loss": "1.801", "total": "672.835", "n_correct": "501.89", "ppl": "3.49", "accuracy": "74.593", "wps": "697.5", "ups": "1.04", "wpb": "672.8", "bsz": "29.2", "num_updates": "13600", "lr": "0.0009076", "gnorm": "12.607", "loss_scale": "256", "train_wall": "186", "gb_free": "10", "wall": "13127"}
[2025-02-02 20:23:57,325][train_inner][INFO] - {"epoch": 37, "update": 36.354, "loss": "72.273", "nll_loss": "1.849", "total": "667.585", "n_correct": "492.65", "ppl": "3.6", "accuracy": "73.796", "wps": "696", "ups": "1.04", "wpb": "667.6", "bsz": "27.8", "num_updates": "13800", "lr": "0.0009208", "gnorm": "12.872", "loss_scale": "256", "train_wall": "184", "gb_free": "10", "wall": "13319"}
[2025-02-02 20:27:05,300][train_inner][INFO] - {"epoch": 37, "update": 36.388, "loss": "70.569", "nll_loss": "1.837", "total": "663.21", "n_correct": "490.765", "ppl": "3.57", "accuracy": "73.998", "wps": "705.7", "ups": "1.06", "wpb": "663.2", "bsz": "28.2", "num_updates": "14000", "lr": "0.000934", "gnorm": "12.835", "loss_scale": "256", "train_wall": "181", "gb_free": "10.1", "wall": "13507"}
[2025-02-02 20:30:19,147][train_inner][INFO] - {"epoch": 37, "update": 36.423, "loss": "69.268", "nll_loss": "1.802", "total": "660.725", "n_correct": "492.18", "ppl": "3.49", "accuracy": "74.491", "wps": "682", "ups": "1.03", "wpb": "660.7", "bsz": "28.3", "num_updates": "14200", "lr": "0.0009472", "gnorm": "12.173", "loss_scale": "256", "train_wall": "187", "gb_free": "9.9", "wall": "13701"}
[2025-02-02 20:33:26,303][train_inner][INFO] - {"epoch": 37, "update": 36.457, "loss": "71.523", "nll_loss": "1.857", "total": "661.235", "n_correct": "487.76", "ppl": "3.62", "accuracy": "73.765", "wps": "706.8", "ups": "1.07", "wpb": "661.2", "bsz": "27.9", "num_updates": "14400", "lr": "0.0009604", "gnorm": "12.485", "loss_scale": "256", "train_wall": "180", "gb_free": "9.9", "wall": "13888"}
[2025-02-02 20:36:36,267][train_inner][INFO] - {"epoch": 37, "update": 36.491, "loss": "71.167", "nll_loss": "1.848", "total": "667.27", "n_correct": "492.44", "ppl": "3.6", "accuracy": "73.799", "wps": "702.6", "ups": "1.05", "wpb": "667.3", "bsz": "28.2", "num_updates": "14600", "lr": "0.0009736", "gnorm": "12.952", "loss_scale": "256", "train_wall": "183", "gb_free": "10", "wall": "14078"}
[2025-02-02 20:39:44,578][train_inner][INFO] - {"epoch": 37, "update": 36.525, "loss": "72.432", "nll_loss": "1.814", "total": "661.38", "n_correct": "492.105", "ppl": "3.52", "accuracy": "74.406", "wps": "702.5", "ups": "1.06", "wpb": "661.4", "bsz": "27.2", "num_updates": "14800", "lr": "0.0009868", "gnorm": "12.454", "loss_scale": "256", "train_wall": "181", "gb_free": "10", "wall": "14266"}
[2025-02-02 20:42:55,063][train_inner][INFO] - {"epoch": 37, "update": 36.559, "loss": "71.03", "nll_loss": "1.791", "total": "662.375", "n_correct": "494.675", "ppl": "3.46", "accuracy": "74.682", "wps": "695.5", "ups": "1.05", "wpb": "662.4", "bsz": "27.6", "num_updates": "15000", "lr": "0.001", "gnorm": "12.721", "loss_scale": "256", "train_wall": "184", "gb_free": "10.1", "wall": "14457"}
[2025-02-02 20:46:32,354][train_inner][INFO] - {"epoch": 37, "update": 36.593, "loss": "69.877", "nll_loss": "1.833", "total": "667.525", "n_correct": "494.925", "ppl": "3.56", "accuracy": "74.143", "wps": "614.6", "ups": "0.92", "wpb": "667.5", "bsz": "28.6", "num_updates": "15200", "lr": "0.000980227", "gnorm": "11.963", "loss_scale": "256", "train_wall": "210", "gb_free": "10", "wall": "14674"}
[2025-02-02 20:50:18,317][train_inner][INFO] - {"epoch": 37, "update": 36.627, "loss": "71.491", "nll_loss": "1.807", "total": "667.02", "n_correct": "497.24", "ppl": "3.5", "accuracy": "74.546", "wps": "590.4", "ups": "0.89", "wpb": "667", "bsz": "27.8", "num_updates": "15400", "lr": "0.000960844", "gnorm": "12.736", "loss_scale": "256", "train_wall": "219", "gb_free": "10", "wall": "14900"}
[2025-02-02 20:53:27,615][train_inner][INFO] - {"epoch": 37, "update": 36.661, "loss": "71.737", "nll_loss": "1.859", "total": "665.375", "n_correct": "490.535", "ppl": "3.63", "accuracy": "73.723", "wps": "703", "ups": "1.06", "wpb": "665.4", "bsz": "28", "num_updates": "15600", "lr": "0.000941845", "gnorm": "12.119", "loss_scale": "256", "train_wall": "182", "gb_free": "10", "wall": "15089"}
[2025-02-02 20:56:33,570][train_inner][INFO] - {"epoch": 37, "update": 36.695, "loss": "73.624", "nll_loss": "1.85", "total": "661.845", "n_correct": "488.06", "ppl": "3.6", "accuracy": "73.742", "wps": "711.9", "ups": "1.08", "wpb": "661.8", "bsz": "27.1", "num_updates": "15800", "lr": "0.000923221", "gnorm": "12.331", "loss_scale": "256", "train_wall": "178", "gb_free": "10", "wall": "15275"}
[2025-02-02 20:59:45,707][train_inner][INFO] - {"epoch": 37, "update": 36.73, "loss": "70.426", "nll_loss": "1.795", "total": "664.675", "n_correct": "495.42", "ppl": "3.47", "accuracy": "74.536", "wps": "691.9", "ups": "1.04", "wpb": "664.7", "bsz": "28", "num_updates": "16000", "lr": "0.000904966", "gnorm": "11.592", "loss_scale": "256", "train_wall": "185", "gb_free": "9.9", "wall": "15467"}
[2025-02-02 21:00:31,911][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-02 21:02:59,324][train_inner][INFO] - {"epoch": 37, "update": 36.764, "loss": "69.799", "nll_loss": "1.785", "total": "664.165", "n_correct": "496.635", "ppl": "3.45", "accuracy": "74.776", "wps": "686.1", "ups": "1.03", "wpb": "664.2", "bsz": "28.1", "num_updates": "16200", "lr": "0.000887072", "gnorm": "11.717", "loss_scale": "128", "train_wall": "187", "gb_free": "10", "wall": "15661"}
[2025-02-02 21:06:10,295][train_inner][INFO] - {"epoch": 37, "update": 36.798, "loss": "68.178", "nll_loss": "1.761", "total": "665.575", "n_correct": "500.905", "ppl": "3.39", "accuracy": "75.259", "wps": "697.2", "ups": "1.05", "wpb": "665.6", "bsz": "28.7", "num_updates": "16400", "lr": "0.000869531", "gnorm": "11.486", "loss_scale": "128", "train_wall": "184", "gb_free": "10", "wall": "15852"}
[2025-02-02 21:09:20,472][train_inner][INFO] - {"epoch": 37, "update": 36.832, "loss": "70.857", "nll_loss": "1.777", "total": "668.2", "n_correct": "499.465", "ppl": "3.43", "accuracy": "74.748", "wps": "702.9", "ups": "1.05", "wpb": "668.2", "bsz": "27.8", "num_updates": "16600", "lr": "0.000852338", "gnorm": "12.073", "loss_scale": "128", "train_wall": "183", "gb_free": "10.1", "wall": "16042"}
[2025-02-02 21:12:27,044][train_inner][INFO] - {"epoch": 37, "update": 36.866, "loss": "70.407", "nll_loss": "1.758", "total": "667.96", "n_correct": "500.85", "ppl": "3.38", "accuracy": "74.982", "wps": "716.1", "ups": "1.07", "wpb": "668", "bsz": "27.8", "num_updates": "16800", "lr": "0.000835484", "gnorm": "11.458", "loss_scale": "128", "train_wall": "179", "gb_free": "10", "wall": "16228"}
[2025-02-02 21:15:32,087][train_inner][INFO] - {"epoch": 37, "update": 36.9, "loss": "71.807", "nll_loss": "1.781", "total": "663.085", "n_correct": "496.345", "ppl": "3.44", "accuracy": "74.854", "wps": "716.8", "ups": "1.08", "wpb": "663.1", "bsz": "27.2", "num_updates": "17000", "lr": "0.000818964", "gnorm": "11.903", "loss_scale": "128", "train_wall": "178", "gb_free": "9.9", "wall": "16414"}
[2025-02-02 21:18:41,809][train_inner][INFO] - {"epoch": 37, "update": 36.934, "loss": "69.797", "nll_loss": "1.771", "total": "665.355", "n_correct": "499.3", "ppl": "3.41", "accuracy": "75.043", "wps": "701.7", "ups": "1.05", "wpb": "665.4", "bsz": "28", "num_updates": "17200", "lr": "0.00080277", "gnorm": "11.48", "loss_scale": "128", "train_wall": "183", "gb_free": "10", "wall": "16603"}
[2025-02-02 21:21:47,524][train_inner][INFO] - {"epoch": 37, "update": 36.969, "loss": "71.137", "nll_loss": "1.749", "total": "663.18", "n_correct": "499.325", "ppl": "3.36", "accuracy": "75.293", "wps": "714.2", "ups": "1.08", "wpb": "663.2", "bsz": "27.2", "num_updates": "17400", "lr": "0.000786896", "gnorm": "11.727", "loss_scale": "128", "train_wall": "179", "gb_free": "10", "wall": "16789"}
[2025-02-02 21:24:24,767][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-02-02 21:24:24,771][train][INFO] - {"epoch": 37, "train_loss": "70.8", "train_nll_loss": "1.803", "train_total": "665.839", "train_n_correct": "495.851", "train_ppl": "3.49", "train_accuracy": "74.47", "train_wps": "690.4", "train_ups": "1.04", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "17584", "train_lr": "0.00077257", "train_gnorm": "12.448", "train_loss_scale": "128", "train_train_wall": "5406", "train_gb_free": "9.9", "train_wall": "16946"}
[2025-02-02 21:24:25,831][fairseq.trainer][INFO] - begin training epoch 38
[2025-02-02 21:24:25,831][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-02 21:25:15,980][train_inner][INFO] - {"epoch": 38, "update": 37.003, "loss": "71.704", "nll_loss": "1.734", "total": "672.01", "n_correct": "506.31", "ppl": "3.33", "accuracy": "75.343", "wps": "644.8", "ups": "0.96", "wpb": "672", "bsz": "27.3", "num_updates": "17600", "lr": "0.000771337", "gnorm": "11.812", "loss_scale": "128", "train_wall": "175", "gb_free": "10", "wall": "16997"}
[2025-02-02 21:28:26,363][train_inner][INFO] - {"epoch": 38, "update": 37.037, "loss": "69.34", "nll_loss": "1.664", "total": "670.505", "n_correct": "510.36", "ppl": "3.17", "accuracy": "76.116", "wps": "704.4", "ups": "1.05", "wpb": "670.5", "bsz": "27.6", "num_updates": "17800", "lr": "0.000756085", "gnorm": "11.6", "loss_scale": "128", "train_wall": "184", "gb_free": "10", "wall": "17188"}
[2025-02-02 21:31:38,413][train_inner][INFO] - {"epoch": 38, "update": 37.071, "loss": "67.589", "nll_loss": "1.658", "total": "673.905", "n_correct": "513.89", "ppl": "3.16", "accuracy": "76.256", "wps": "701.9", "ups": "1.04", "wpb": "673.9", "bsz": "28.4", "num_updates": "18000", "lr": "0.000741134", "gnorm": "11.422", "loss_scale": "128", "train_wall": "185", "gb_free": "10", "wall": "17380"}
[2025-02-02 21:34:48,344][train_inner][INFO] - {"epoch": 38, "update": 37.105, "loss": "66.603", "nll_loss": "1.639", "total": "665.805", "n_correct": "509.625", "ppl": "3.12", "accuracy": "76.543", "wps": "701.1", "ups": "1.05", "wpb": "665.8", "bsz": "28.3", "num_updates": "18200", "lr": "0.00072648", "gnorm": "11.347", "loss_scale": "128", "train_wall": "182", "gb_free": "10", "wall": "17570"}
[2025-02-02 21:38:04,673][train_inner][INFO] - {"epoch": 38, "update": 37.139, "loss": "63.751", "nll_loss": "1.568", "total": "666.795", "n_correct": "516.875", "ppl": "2.96", "accuracy": "77.516", "wps": "679.3", "ups": "1.02", "wpb": "666.8", "bsz": "28.9", "num_updates": "18400", "lr": "0.000712115", "gnorm": "11.024", "loss_scale": "128", "train_wall": "189", "gb_free": "10", "wall": "17766"}
[2025-02-02 21:41:14,102][train_inner][INFO] - {"epoch": 38, "update": 37.173, "loss": "68.65", "nll_loss": "1.674", "total": "664.73", "n_correct": "505.81", "ppl": "3.19", "accuracy": "76.093", "wps": "701.9", "ups": "1.06", "wpb": "664.7", "bsz": "27.7", "num_updates": "18600", "lr": "0.000698034", "gnorm": "11.106", "loss_scale": "128", "train_wall": "182", "gb_free": "9.9", "wall": "17956"}
[2025-02-02 21:44:20,615][train_inner][INFO] - {"epoch": 38, "update": 37.207, "loss": "69.113", "nll_loss": "1.647", "total": "667.67", "n_correct": "510.785", "ppl": "3.13", "accuracy": "76.503", "wps": "716", "ups": "1.07", "wpb": "667.7", "bsz": "27.4", "num_updates": "18800", "lr": "0.000684231", "gnorm": "11.82", "loss_scale": "128", "train_wall": "180", "gb_free": "9.9", "wall": "18142"}
[2025-02-02 21:47:30,538][train_inner][INFO] - {"epoch": 38, "update": 37.242, "loss": "65.955", "nll_loss": "1.609", "total": "665.635", "n_correct": "511.665", "ppl": "3.05", "accuracy": "76.869", "wps": "701.1", "ups": "1.05", "wpb": "665.6", "bsz": "28.3", "num_updates": "19000", "lr": "0.000670702", "gnorm": "11.19", "loss_scale": "128", "train_wall": "183", "gb_free": "10.1", "wall": "18332"}
[2025-02-02 21:50:42,546][train_inner][INFO] - {"epoch": 38, "update": 37.276, "loss": "67.947", "nll_loss": "1.661", "total": "667.47", "n_correct": "509.58", "ppl": "3.16", "accuracy": "76.345", "wps": "695.3", "ups": "1.04", "wpb": "667.5", "bsz": "28", "num_updates": "19200", "lr": "0.00065744", "gnorm": "10.955", "loss_scale": "128", "train_wall": "185", "gb_free": "10.1", "wall": "18524"}
[2025-02-02 21:53:50,663][train_inner][INFO] - {"epoch": 38, "update": 37.31, "loss": "66.64", "nll_loss": "1.566", "total": "665.795", "n_correct": "516.715", "ppl": "2.96", "accuracy": "77.609", "wps": "707.9", "ups": "1.06", "wpb": "665.8", "bsz": "27.6", "num_updates": "19400", "lr": "0.00064444", "gnorm": "11.031", "loss_scale": "128", "train_wall": "182", "gb_free": "10.1", "wall": "18712"}
[2025-02-02 21:57:02,752][train_inner][INFO] - {"epoch": 38, "update": 37.344, "loss": "64.423", "nll_loss": "1.577", "total": "664.395", "n_correct": "514.075", "ppl": "2.98", "accuracy": "77.375", "wps": "691.8", "ups": "1.04", "wpb": "664.4", "bsz": "28.6", "num_updates": "19600", "lr": "0.000631697", "gnorm": "10.783", "loss_scale": "128", "train_wall": "185", "gb_free": "10", "wall": "18904"}
[2025-02-02 22:00:11,679][train_inner][INFO] - {"epoch": 38, "update": 37.378, "loss": "68.653", "nll_loss": "1.61", "total": "670.395", "n_correct": "517.38", "ppl": "3.05", "accuracy": "77.175", "wps": "709.7", "ups": "1.06", "wpb": "670.4", "bsz": "27.4", "num_updates": "19800", "lr": "0.000619206", "gnorm": "11.621", "loss_scale": "128", "train_wall": "182", "gb_free": "10.1", "wall": "19093"}
[2025-02-02 22:03:26,143][train_inner][INFO] - {"epoch": 38, "update": 37.412, "loss": "65.773", "nll_loss": "1.614", "total": "674.71", "n_correct": "517.9", "ppl": "3.06", "accuracy": "76.759", "wps": "693.9", "ups": "1.03", "wpb": "674.7", "bsz": "28.8", "num_updates": "20000", "lr": "0.000606962", "gnorm": "10.797", "loss_scale": "128", "train_wall": "187", "gb_free": "10", "wall": "19288"}
[2025-02-02 22:06:33,107][train_inner][INFO] - {"epoch": 38, "update": 37.446, "loss": "66.628", "nll_loss": "1.576", "total": "656.2", "n_correct": "508.18", "ppl": "2.98", "accuracy": "77.443", "wps": "702.2", "ups": "1.07", "wpb": "656.2", "bsz": "27.3", "num_updates": "20200", "lr": "0.000594961", "gnorm": "11.132", "loss_scale": "256", "train_wall": "180", "gb_free": "10", "wall": "19475"}
[2025-02-02 22:09:39,574][train_inner][INFO] - {"epoch": 38, "update": 37.48, "loss": "66.659", "nll_loss": "1.611", "total": "659.365", "n_correct": "506.905", "ppl": "3.05", "accuracy": "76.878", "wps": "707.4", "ups": "1.07", "wpb": "659.4", "bsz": "27.7", "num_updates": "20400", "lr": "0.000583196", "gnorm": "11.024", "loss_scale": "256", "train_wall": "179", "gb_free": "10", "wall": "19661"}
[2025-02-02 22:12:49,429][train_inner][INFO] - {"epoch": 38, "update": 37.515, "loss": "66.892", "nll_loss": "1.57", "total": "665.045", "n_correct": "516.225", "ppl": "2.97", "accuracy": "77.623", "wps": "700.8", "ups": "1.05", "wpb": "665", "bsz": "27.5", "num_updates": "20600", "lr": "0.000571664", "gnorm": "10.915", "loss_scale": "256", "train_wall": "183", "gb_free": "10", "wall": "19851"}
[2025-02-02 22:15:59,543][train_inner][INFO] - {"epoch": 38, "update": 37.549, "loss": "66.348", "nll_loss": "1.571", "total": "669.235", "n_correct": "518.805", "ppl": "2.97", "accuracy": "77.522", "wps": "704.1", "ups": "1.05", "wpb": "669.2", "bsz": "27.9", "num_updates": "20800", "lr": "0.000560361", "gnorm": "10.365", "loss_scale": "256", "train_wall": "182", "gb_free": "10", "wall": "20041"}
[2025-02-02 22:19:10,732][train_inner][INFO] - {"epoch": 38, "update": 37.583, "loss": "64.289", "nll_loss": "1.54", "total": "667.595", "n_correct": "519.905", "ppl": "2.91", "accuracy": "77.877", "wps": "698.4", "ups": "1.05", "wpb": "667.6", "bsz": "28.5", "num_updates": "21000", "lr": "0.00054928", "gnorm": "10.561", "loss_scale": "256", "train_wall": "184", "gb_free": "10.1", "wall": "20232"}
[2025-02-02 22:22:18,073][train_inner][INFO] - {"epoch": 38, "update": 37.617, "loss": "66.952", "nll_loss": "1.581", "total": "659.735", "n_correct": "510.48", "ppl": "2.99", "accuracy": "77.377", "wps": "704.5", "ups": "1.07", "wpb": "659.7", "bsz": "27.3", "num_updates": "21200", "lr": "0.000538419", "gnorm": "10.73", "loss_scale": "256", "train_wall": "181", "gb_free": "10", "wall": "20419"}
[2025-02-02 22:25:31,133][train_inner][INFO] - {"epoch": 38, "update": 37.651, "loss": "65.799", "nll_loss": "1.592", "total": "671.485", "n_correct": "518.46", "ppl": "3.02", "accuracy": "77.211", "wps": "695.7", "ups": "1.04", "wpb": "671.5", "bsz": "28.4", "num_updates": "21400", "lr": "0.000527773", "gnorm": "10.642", "loss_scale": "256", "train_wall": "186", "gb_free": "9.9", "wall": "20613"}
[2025-02-02 22:28:44,602][train_inner][INFO] - {"epoch": 38, "update": 37.685, "loss": "63.107", "nll_loss": "1.501", "total": "664.91", "n_correct": "522.075", "ppl": "2.83", "accuracy": "78.518", "wps": "687.4", "ups": "1.03", "wpb": "664.9", "bsz": "28.5", "num_updates": "21600", "lr": "0.000517337", "gnorm": "10.474", "loss_scale": "256", "train_wall": "186", "gb_free": "10", "wall": "20806"}
[2025-02-02 22:31:56,252][train_inner][INFO] - {"epoch": 38, "update": 37.719, "loss": "64.801", "nll_loss": "1.581", "total": "664.815", "n_correct": "514.77", "ppl": "2.99", "accuracy": "77.431", "wps": "693.8", "ups": "1.04", "wpb": "664.8", "bsz": "28.5", "num_updates": "21800", "lr": "0.000507107", "gnorm": "10.988", "loss_scale": "256", "train_wall": "184", "gb_free": "10", "wall": "20998"}
[2025-02-02 22:32:42,992][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-02 22:35:22,560][train_inner][INFO] - {"epoch": 38, "update": 37.753, "loss": "67.45", "nll_loss": "1.57", "total": "668.515", "n_correct": "518.08", "ppl": "2.97", "accuracy": "77.497", "wps": "648.1", "ups": "0.97", "wpb": "668.5", "bsz": "27.4", "num_updates": "22000", "lr": "0.00049708", "gnorm": "11.077", "loss_scale": "128", "train_wall": "200", "gb_free": "9.9", "wall": "21204"}
[2025-02-02 22:38:45,586][train_inner][INFO] - {"epoch": 38, "update": 37.788, "loss": "66.171", "nll_loss": "1.536", "total": "658.625", "n_correct": "513.98", "ppl": "2.9", "accuracy": "78.038", "wps": "648.9", "ups": "0.99", "wpb": "658.6", "bsz": "27.2", "num_updates": "22200", "lr": "0.000487251", "gnorm": "10.603", "loss_scale": "128", "train_wall": "196", "gb_free": "9.9", "wall": "21407"}
[2025-02-02 22:43:14,235][train_inner][INFO] - {"epoch": 38, "update": 37.822, "loss": "67.306", "nll_loss": "1.557", "total": "659.125", "n_correct": "512.755", "ppl": "2.94", "accuracy": "77.793", "wps": "490.7", "ups": "0.74", "wpb": "659.1", "bsz": "26.9", "num_updates": "22400", "lr": "0.000477616", "gnorm": "11.318", "loss_scale": "128", "train_wall": "258", "gb_free": "9.9", "wall": "21676"}
[2025-02-02 22:48:19,472][train_inner][INFO] - {"epoch": 38, "update": 37.856, "loss": "62.025", "nll_loss": "1.544", "total": "659.22", "n_correct": "513.165", "ppl": "2.92", "accuracy": "77.844", "wps": "432", "ups": "0.66", "wpb": "659.2", "bsz": "29.1", "num_updates": "22600", "lr": "0.000468172", "gnorm": "10.043", "loss_scale": "128", "train_wall": "293", "gb_free": "10.1", "wall": "21981"}
[2025-02-02 22:53:22,780][train_inner][INFO] - {"epoch": 38, "update": 37.89, "loss": "62.342", "nll_loss": "1.544", "total": "670.835", "n_correct": "522.355", "ppl": "2.92", "accuracy": "77.866", "wps": "442.4", "ups": "0.66", "wpb": "670.8", "bsz": "29.5", "num_updates": "22800", "lr": "0.000458915", "gnorm": "10.581", "loss_scale": "128", "train_wall": "292", "gb_free": "9.9", "wall": "22284"}
[2025-02-02 22:58:10,994][train_inner][INFO] - {"epoch": 38, "update": 37.924, "loss": "67.68", "nll_loss": "1.556", "total": "666.425", "n_correct": "517.485", "ppl": "2.94", "accuracy": "77.651", "wps": "462.5", "ups": "0.69", "wpb": "666.4", "bsz": "27.1", "num_updates": "23000", "lr": "0.000449841", "gnorm": "10.896", "loss_scale": "128", "train_wall": "277", "gb_free": "10.1", "wall": "22572"}
[2025-02-02 23:03:01,246][train_inner][INFO] - {"epoch": 38, "update": 37.958, "loss": "67.227", "nll_loss": "1.562", "total": "664.345", "n_correct": "514.775", "ppl": "2.95", "accuracy": "77.486", "wps": "457.8", "ups": "0.69", "wpb": "664.3", "bsz": "27.2", "num_updates": "23200", "lr": "0.000440946", "gnorm": "10.951", "loss_scale": "128", "train_wall": "278", "gb_free": "10.1", "wall": "22863"}
[2025-02-02 23:07:51,886][train_inner][INFO] - {"epoch": 38, "update": 37.992, "loss": "65.576", "nll_loss": "1.511", "total": "664.73", "n_correct": "520.465", "ppl": "2.85", "accuracy": "78.297", "wps": "457.4", "ups": "0.69", "wpb": "664.7", "bsz": "27.5", "num_updates": "23400", "lr": "0.000432227", "gnorm": "10.66", "loss_scale": "128", "train_wall": "280", "gb_free": "10", "wall": "23153"}
[2025-02-02 23:08:33,610][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-02 23:10:19,188][valid][INFO] - {"epoch": 38, "valid_loss": "27.761", "valid_nll_loss": "0.916", "valid_total": "678.4", "valid_n_correct": "588", "valid_ppl": "1.89", "valid_accuracy": "86.675", "valid_wps": "322.9", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "23445", "valid_best_accuracy": "86.675"}
[2025-02-02 23:10:19,192][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 23445 updates
[2025-02-02 23:10:19,193][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 23:10:25,186][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 23:10:28,990][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 23445 updates, score 86.675) (writing took 9.796756722964346 seconds)
[2025-02-02 23:10:28,994][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-02-02 23:10:29,005][train][INFO] - {"epoch": 38, "train_loss": "66.242", "train_nll_loss": "1.586", "train_total": "665.82", "train_n_correct": "514.612", "train_ppl": "3", "train_accuracy": "77.29", "train_wps": "613.2", "train_ups": "0.92", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "23445", "train_lr": "0.000430289", "train_gnorm": "10.956", "train_loss_scale": "128", "train_train_wall": "5987", "train_gb_free": "9.9", "train_wall": "23310"}
[2025-02-02 23:10:29,716][fairseq.trainer][INFO] - begin training epoch 39
[2025-02-02 23:10:29,717][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-02 23:15:09,793][train_inner][INFO] - {"epoch": 39, "update": 38.026, "loss": "68.017", "nll_loss": "1.461", "total": "667.68", "n_correct": "526.25", "ppl": "2.75", "accuracy": "78.818", "wps": "305", "ups": "0.46", "wpb": "667.7", "bsz": "26.2", "num_updates": "23600", "lr": "0.00042368", "gnorm": "11.074", "loss_scale": "128", "train_wall": "255", "gb_free": "10.1", "wall": "23591"}
[2025-02-02 23:20:01,822][train_inner][INFO] - {"epoch": 39, "update": 38.061, "loss": "62.589", "nll_loss": "1.428", "total": "670.35", "n_correct": "531.455", "ppl": "2.69", "accuracy": "79.28", "wps": "459.2", "ups": "0.69", "wpb": "670.4", "bsz": "28.3", "num_updates": "23800", "lr": "0.000415302", "gnorm": "10.503", "loss_scale": "128", "train_wall": "281", "gb_free": "10", "wall": "23883"}
[2025-02-02 23:24:52,728][train_inner][INFO] - {"epoch": 39, "update": 38.095, "loss": "65.004", "nll_loss": "1.441", "total": "669.07", "n_correct": "530.18", "ppl": "2.72", "accuracy": "79.241", "wps": "460", "ups": "0.69", "wpb": "669.1", "bsz": "27.3", "num_updates": "24000", "lr": "0.000407091", "gnorm": "10.843", "loss_scale": "128", "train_wall": "280", "gb_free": "10", "wall": "24174"}
[2025-02-02 23:29:45,925][train_inner][INFO] - {"epoch": 39, "update": 38.129, "loss": "64.342", "nll_loss": "1.402", "total": "665.43", "n_correct": "529.445", "ppl": "2.64", "accuracy": "79.564", "wps": "454", "ups": "0.68", "wpb": "665.4", "bsz": "27.1", "num_updates": "24200", "lr": "0.000399041", "gnorm": "10.745", "loss_scale": "128", "train_wall": "281", "gb_free": "10", "wall": "24467"}
[2025-02-02 23:34:46,268][train_inner][INFO] - {"epoch": 39, "update": 38.163, "loss": "61.668", "nll_loss": "1.41", "total": "667.39", "n_correct": "532.025", "ppl": "2.66", "accuracy": "79.717", "wps": "444.4", "ups": "0.67", "wpb": "667.4", "bsz": "28.4", "num_updates": "24400", "lr": "0.000391151", "gnorm": "10.485", "loss_scale": "128", "train_wall": "288", "gb_free": "9.9", "wall": "24768"}
[2025-02-02 23:39:38,172][train_inner][INFO] - {"epoch": 39, "update": 38.197, "loss": "63.934", "nll_loss": "1.422", "total": "669.26", "n_correct": "531.63", "ppl": "2.68", "accuracy": "79.435", "wps": "458.6", "ups": "0.69", "wpb": "669.3", "bsz": "27.6", "num_updates": "24600", "lr": "0.000383416", "gnorm": "10.361", "loss_scale": "128", "train_wall": "280", "gb_free": "10", "wall": "25060"}
[2025-02-02 23:43:53,522][train_inner][INFO] - {"epoch": 39, "update": 38.231, "loss": "62.787", "nll_loss": "1.445", "total": "664.625", "n_correct": "525.77", "ppl": "2.72", "accuracy": "79.108", "wps": "520.6", "ups": "0.78", "wpb": "664.6", "bsz": "28.1", "num_updates": "24800", "lr": "0.000375835", "gnorm": "10.372", "loss_scale": "128", "train_wall": "246", "gb_free": "10.1", "wall": "25315"}
[2025-02-02 23:48:42,177][train_inner][INFO] - {"epoch": 39, "update": 38.265, "loss": "65.026", "nll_loss": "1.476", "total": "666.12", "n_correct": "523.875", "ppl": "2.78", "accuracy": "78.646", "wps": "461.5", "ups": "0.69", "wpb": "666.1", "bsz": "27.5", "num_updates": "25000", "lr": "0.000368403", "gnorm": "10.619", "loss_scale": "128", "train_wall": "278", "gb_free": "10.1", "wall": "25604"}
[2025-02-02 23:53:35,338][train_inner][INFO] - {"epoch": 39, "update": 38.299, "loss": "63.414", "nll_loss": "1.432", "total": "668.4", "n_correct": "530.72", "ppl": "2.7", "accuracy": "79.402", "wps": "456.1", "ups": "0.68", "wpb": "668.4", "bsz": "27.9", "num_updates": "25200", "lr": "0.000361119", "gnorm": "11.115", "loss_scale": "128", "train_wall": "283", "gb_free": "10", "wall": "25897"}
[2025-02-02 23:58:29,585][train_inner][INFO] - {"epoch": 39, "update": 38.334, "loss": "61.948", "nll_loss": "1.43", "total": "665.88", "n_correct": "528.795", "ppl": "2.69", "accuracy": "79.413", "wps": "452.7", "ups": "0.68", "wpb": "665.9", "bsz": "28.4", "num_updates": "25400", "lr": "0.000353978", "gnorm": "10.609", "loss_scale": "128", "train_wall": "285", "gb_free": "9.9", "wall": "26191"}
[2025-02-03 00:03:24,046][train_inner][INFO] - {"epoch": 39, "update": 38.368, "loss": "62.101", "nll_loss": "1.412", "total": "663.655", "n_correct": "528.35", "ppl": "2.66", "accuracy": "79.612", "wps": "450.8", "ups": "0.68", "wpb": "663.7", "bsz": "28.1", "num_updates": "25600", "lr": "0.000346979", "gnorm": "10.392", "loss_scale": "128", "train_wall": "284", "gb_free": "10", "wall": "26485"}
[2025-02-03 00:08:08,701][train_inner][INFO] - {"epoch": 39, "update": 38.402, "loss": "63.904", "nll_loss": "1.443", "total": "658.87", "n_correct": "521.12", "ppl": "2.72", "accuracy": "79.093", "wps": "463", "ups": "0.7", "wpb": "658.9", "bsz": "27.4", "num_updates": "25800", "lr": "0.000340118", "gnorm": "10.486", "loss_scale": "128", "train_wall": "275", "gb_free": "9.9", "wall": "26770"}
[2025-02-03 00:12:54,580][train_inner][INFO] - {"epoch": 39, "update": 38.436, "loss": "64.237", "nll_loss": "1.427", "total": "663.345", "n_correct": "526.705", "ppl": "2.69", "accuracy": "79.401", "wps": "464.2", "ups": "0.7", "wpb": "663.3", "bsz": "27.3", "num_updates": "26000", "lr": "0.000333392", "gnorm": "10.625", "loss_scale": "256", "train_wall": "275", "gb_free": "10.1", "wall": "27056"}
[2025-02-03 00:17:23,544][train_inner][INFO] - {"epoch": 39, "update": 38.47, "loss": "61.681", "nll_loss": "1.434", "total": "662.565", "n_correct": "524.93", "ppl": "2.7", "accuracy": "79.227", "wps": "492.8", "ups": "0.74", "wpb": "662.6", "bsz": "28.4", "num_updates": "26200", "lr": "0.0003268", "gnorm": "10.529", "loss_scale": "256", "train_wall": "259", "gb_free": "9.9", "wall": "27325"}
[2025-02-03 00:22:14,725][train_inner][INFO] - {"epoch": 39, "update": 38.504, "loss": "61.277", "nll_loss": "1.431", "total": "662.655", "n_correct": "525.6", "ppl": "2.7", "accuracy": "79.317", "wps": "455.2", "ups": "0.69", "wpb": "662.7", "bsz": "28.6", "num_updates": "26400", "lr": "0.000320338", "gnorm": "10.37", "loss_scale": "256", "train_wall": "281", "gb_free": "9.9", "wall": "27616"}
[2025-02-03 00:27:03,770][train_inner][INFO] - {"epoch": 39, "update": 38.538, "loss": "62.607", "nll_loss": "1.42", "total": "667.135", "n_correct": "530.27", "ppl": "2.68", "accuracy": "79.485", "wps": "461.6", "ups": "0.69", "wpb": "667.1", "bsz": "28.1", "num_updates": "26600", "lr": "0.000314004", "gnorm": "10.317", "loss_scale": "256", "train_wall": "278", "gb_free": "10", "wall": "27905"}
[2025-02-03 00:31:48,710][train_inner][INFO] - {"epoch": 39, "update": 38.572, "loss": "62.288", "nll_loss": "1.446", "total": "664.92", "n_correct": "526.11", "ppl": "2.73", "accuracy": "79.124", "wps": "466.7", "ups": "0.7", "wpb": "664.9", "bsz": "28.4", "num_updates": "26800", "lr": "0.000307795", "gnorm": "10.49", "loss_scale": "256", "train_wall": "274", "gb_free": "10.1", "wall": "28190"}
[2025-02-03 00:36:46,163][train_inner][INFO] - {"epoch": 39, "update": 38.606, "loss": "60.56", "nll_loss": "1.403", "total": "666.965", "n_correct": "532.13", "ppl": "2.65", "accuracy": "79.784", "wps": "448.5", "ups": "0.67", "wpb": "667", "bsz": "28.9", "num_updates": "27000", "lr": "0.000301709", "gnorm": "10.132", "loss_scale": "256", "train_wall": "287", "gb_free": "10.1", "wall": "28488"}
[2025-02-03 00:41:34,029][train_inner][INFO] - {"epoch": 39, "update": 38.641, "loss": "61.441", "nll_loss": "1.392", "total": "663.725", "n_correct": "530.495", "ppl": "2.62", "accuracy": "79.927", "wps": "461.2", "ups": "0.69", "wpb": "663.7", "bsz": "28.2", "num_updates": "27200", "lr": "0.000295743", "gnorm": "10.323", "loss_scale": "256", "train_wall": "276", "gb_free": "10.1", "wall": "28775"}
[2025-02-03 00:46:17,973][train_inner][INFO] - {"epoch": 39, "update": 38.675, "loss": "64.808", "nll_loss": "1.398", "total": "669.395", "n_correct": "533.655", "ppl": "2.63", "accuracy": "79.722", "wps": "471.7", "ups": "0.7", "wpb": "669.4", "bsz": "27", "num_updates": "27400", "lr": "0.000289895", "gnorm": "10.605", "loss_scale": "256", "train_wall": "273", "gb_free": "9.9", "wall": "29059"}
[2025-02-03 00:50:20,703][train_inner][INFO] - {"epoch": 39, "update": 38.709, "loss": "59.594", "nll_loss": "1.374", "total": "670.635", "n_correct": "537.69", "ppl": "2.59", "accuracy": "80.176", "wps": "552.6", "ups": "0.82", "wpb": "670.6", "bsz": "29.2", "num_updates": "27600", "lr": "0.000284163", "gnorm": "9.701", "loss_scale": "256", "train_wall": "234", "gb_free": "10", "wall": "29302"}
[2025-02-03 00:55:10,029][train_inner][INFO] - {"epoch": 39, "update": 38.743, "loss": "58.982", "nll_loss": "1.401", "total": "663.925", "n_correct": "530.425", "ppl": "2.64", "accuracy": "79.892", "wps": "459", "ups": "0.69", "wpb": "663.9", "bsz": "29.5", "num_updates": "27800", "lr": "0.000278544", "gnorm": "9.859", "loss_scale": "256", "train_wall": "277", "gb_free": "9.9", "wall": "29591"}
[2025-02-03 01:00:01,009][train_inner][INFO] - {"epoch": 39, "update": 38.777, "loss": "63.041", "nll_loss": "1.44", "total": "670.615", "n_correct": "531.085", "ppl": "2.71", "accuracy": "79.194", "wps": "460.9", "ups": "0.69", "wpb": "670.6", "bsz": "28.2", "num_updates": "28000", "lr": "0.000273036", "gnorm": "10.645", "loss_scale": "256", "train_wall": "280", "gb_free": "9.9", "wall": "29882"}
[2025-02-03 01:04:48,078][train_inner][INFO] - {"epoch": 39, "update": 38.811, "loss": "61.478", "nll_loss": "1.386", "total": "669.165", "n_correct": "536.125", "ppl": "2.61", "accuracy": "80.119", "wps": "466.2", "ups": "0.7", "wpb": "669.2", "bsz": "28.3", "num_updates": "28200", "lr": "0.000267637", "gnorm": "10.123", "loss_scale": "256", "train_wall": "276", "gb_free": "9.9", "wall": "30170"}
[2025-02-03 01:09:27,936][train_inner][INFO] - {"epoch": 39, "update": 38.845, "loss": "63.619", "nll_loss": "1.392", "total": "664.825", "n_correct": "530.825", "ppl": "2.62", "accuracy": "79.844", "wps": "475.2", "ups": "0.71", "wpb": "664.8", "bsz": "27.3", "num_updates": "28400", "lr": "0.000262345", "gnorm": "10.856", "loss_scale": "256", "train_wall": "269", "gb_free": "9.9", "wall": "30449"}
[2025-02-03 01:14:17,453][train_inner][INFO] - {"epoch": 39, "update": 38.879, "loss": "63.13", "nll_loss": "1.42", "total": "662.35", "n_correct": "526.55", "ppl": "2.68", "accuracy": "79.497", "wps": "457.6", "ups": "0.69", "wpb": "662.4", "bsz": "27.6", "num_updates": "28600", "lr": "0.000257158", "gnorm": "10.936", "loss_scale": "256", "train_wall": "278", "gb_free": "10", "wall": "30739"}
[2025-02-03 01:18:56,756][train_inner][INFO] - {"epoch": 39, "update": 38.914, "loss": "65.596", "nll_loss": "1.431", "total": "664.73", "n_correct": "527.215", "ppl": "2.7", "accuracy": "79.313", "wps": "476", "ups": "0.72", "wpb": "664.7", "bsz": "26.8", "num_updates": "28800", "lr": "0.000252073", "gnorm": "11.165", "loss_scale": "256", "train_wall": "268", "gb_free": "9.9", "wall": "31018"}
[2025-02-03 01:23:17,597][train_inner][INFO] - {"epoch": 39, "update": 38.948, "loss": "63.521", "nll_loss": "1.401", "total": "658.13", "n_correct": "525.82", "ppl": "2.64", "accuracy": "79.896", "wps": "504.7", "ups": "0.77", "wpb": "658.1", "bsz": "27.1", "num_updates": "29000", "lr": "0.000247089", "gnorm": "10.695", "loss_scale": "256", "train_wall": "250", "gb_free": "9.9", "wall": "31279"}
[2025-02-03 01:28:08,433][train_inner][INFO] - {"epoch": 39, "update": 38.982, "loss": "62.089", "nll_loss": "1.402", "total": "666.31", "n_correct": "531.88", "ppl": "2.64", "accuracy": "79.825", "wps": "458.4", "ups": "0.69", "wpb": "666.3", "bsz": "28.1", "num_updates": "29200", "lr": "0.000242203", "gnorm": "10.232", "loss_scale": "256", "train_wall": "279", "gb_free": "10", "wall": "31570"}
[2025-02-03 01:30:24,829][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2025-02-03 01:30:24,835][train][INFO] - {"epoch": 39, "train_loss": "62.733", "train_nll_loss": "1.419", "train_total": "665.831", "train_n_correct": "529.419", "train_ppl": "2.67", "train_accuracy": "79.513", "train_wps": "464.9", "train_ups": "0.7", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "29307", "train_lr": "0.000239629", "train_gnorm": "10.502", "train_loss_scale": "256", "train_train_wall": "8019", "train_gb_free": "10", "train_wall": "31706"}
[2025-02-03 01:30:26,842][fairseq.trainer][INFO] - begin training epoch 40
[2025-02-03 01:30:26,843][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 01:34:08,381][train_inner][INFO] - {"epoch": 40, "update": 39.016, "loss": "58.621", "nll_loss": "1.345", "total": "665.8", "n_correct": "535.62", "ppl": "2.54", "accuracy": "80.448", "wps": "370", "ups": "0.56", "wpb": "665.8", "bsz": "29.2", "num_updates": "29400", "lr": "0.000237414", "gnorm": "10.061", "loss_scale": "256", "train_wall": "266", "gb_free": "10", "wall": "31930"}
[2025-02-03 01:39:02,726][train_inner][INFO] - {"epoch": 40, "update": 39.05, "loss": "60.097", "nll_loss": "1.33", "total": "655.965", "n_correct": "529.345", "ppl": "2.51", "accuracy": "80.697", "wps": "445.7", "ups": "0.68", "wpb": "656", "bsz": "27.9", "num_updates": "29600", "lr": "0.000232719", "gnorm": "10.3", "loss_scale": "256", "train_wall": "283", "gb_free": "9.9", "wall": "32224"}
[2025-02-03 01:43:46,464][train_inner][INFO] - {"epoch": 40, "update": 39.084, "loss": "62.639", "nll_loss": "1.34", "total": "657.64", "n_correct": "529.27", "ppl": "2.53", "accuracy": "80.48", "wps": "463.6", "ups": "0.7", "wpb": "657.6", "bsz": "26.9", "num_updates": "29800", "lr": "0.000228117", "gnorm": "10.792", "loss_scale": "256", "train_wall": "272", "gb_free": "10", "wall": "32508"}
[2025-02-03 01:48:33,363][train_inner][INFO] - {"epoch": 40, "update": 39.118, "loss": "62.496", "nll_loss": "1.33", "total": "659.66", "n_correct": "532.17", "ppl": "2.51", "accuracy": "80.673", "wps": "459.9", "ups": "0.7", "wpb": "659.7", "bsz": "27", "num_updates": "30000", "lr": "0.000223607", "gnorm": "11.247", "loss_scale": "256", "train_wall": "276", "gb_free": "10.1", "wall": "32795"}
[2025-02-03 01:53:13,291][train_inner][INFO] - {"epoch": 40, "update": 39.152, "loss": "56.668", "nll_loss": "1.297", "total": "661.715", "n_correct": "537.97", "ppl": "2.46", "accuracy": "81.299", "wps": "473", "ups": "0.71", "wpb": "661.7", "bsz": "29.5", "num_updates": "30200", "lr": "0.000219185", "gnorm": "10.138", "loss_scale": "512", "train_wall": "269", "gb_free": "9.9", "wall": "33075"}
[2025-02-03 01:57:01,790][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
[2025-02-03 01:57:49,646][train_inner][INFO] - {"epoch": 40, "update": 39.187, "loss": "58.509", "nll_loss": "1.323", "total": "660.545", "n_correct": "533.305", "ppl": "2.5", "accuracy": "80.737", "wps": "478.1", "ups": "0.72", "wpb": "660.5", "bsz": "28.8", "num_updates": "30400", "lr": "0.000214851", "gnorm": "10.216", "loss_scale": "256", "train_wall": "265", "gb_free": "9.9", "wall": "33351"}
[2025-02-03 02:02:42,927][train_inner][INFO] - {"epoch": 40, "update": 39.221, "loss": "60.567", "nll_loss": "1.303", "total": "665.595", "n_correct": "539.84", "ppl": "2.47", "accuracy": "81.106", "wps": "453.9", "ups": "0.68", "wpb": "665.6", "bsz": "27.8", "num_updates": "30600", "lr": "0.000210603", "gnorm": "10.41", "loss_scale": "256", "train_wall": "281", "gb_free": "10", "wall": "33644"}
[2025-02-03 02:07:32,937][train_inner][INFO] - {"epoch": 40, "update": 39.255, "loss": "61.427", "nll_loss": "1.285", "total": "669.275", "n_correct": "544.705", "ppl": "2.44", "accuracy": "81.387", "wps": "461.6", "ups": "0.69", "wpb": "669.3", "bsz": "27.4", "num_updates": "30800", "lr": "0.000206439", "gnorm": "10.692", "loss_scale": "256", "train_wall": "278", "gb_free": "10", "wall": "33934"}
[2025-02-03 02:12:27,003][train_inner][INFO] - {"epoch": 40, "update": 39.289, "loss": "59.364", "nll_loss": "1.307", "total": "669.165", "n_correct": "541.57", "ppl": "2.47", "accuracy": "80.932", "wps": "455.2", "ups": "0.68", "wpb": "669.2", "bsz": "28.6", "num_updates": "31000", "lr": "0.000202357", "gnorm": "10.484", "loss_scale": "256", "train_wall": "282", "gb_free": "9.9", "wall": "34228"}
[2025-02-03 02:17:17,878][train_inner][INFO] - {"epoch": 40, "update": 39.323, "loss": "60.539", "nll_loss": "1.304", "total": "666.005", "n_correct": "539.55", "ppl": "2.47", "accuracy": "81.013", "wps": "457.9", "ups": "0.69", "wpb": "666", "bsz": "27.9", "num_updates": "31200", "lr": "0.000198355", "gnorm": "10.684", "loss_scale": "256", "train_wall": "280", "gb_free": "10", "wall": "34519"}
[2025-02-03 02:22:13,944][train_inner][INFO] - {"epoch": 40, "update": 39.357, "loss": "59.406", "nll_loss": "1.335", "total": "671.075", "n_correct": "540.805", "ppl": "2.52", "accuracy": "80.588", "wps": "453.3", "ups": "0.68", "wpb": "671.1", "bsz": "28.9", "num_updates": "31400", "lr": "0.000194433", "gnorm": "10.508", "loss_scale": "256", "train_wall": "286", "gb_free": "10.1", "wall": "34815"}
[2025-02-03 02:27:01,800][train_inner][INFO] - {"epoch": 40, "update": 39.391, "loss": "59.608", "nll_loss": "1.318", "total": "668.335", "n_correct": "539.955", "ppl": "2.49", "accuracy": "80.791", "wps": "464.4", "ups": "0.69", "wpb": "668.3", "bsz": "28.5", "num_updates": "31600", "lr": "0.000190589", "gnorm": "10.775", "loss_scale": "256", "train_wall": "278", "gb_free": "10", "wall": "35103"}
[2025-02-03 02:31:39,661][train_inner][INFO] - {"epoch": 40, "update": 39.425, "loss": "61.707", "nll_loss": "1.317", "total": "667.895", "n_correct": "539.625", "ppl": "2.49", "accuracy": "80.795", "wps": "480.8", "ups": "0.72", "wpb": "667.9", "bsz": "27.5", "num_updates": "31800", "lr": "0.00018682", "gnorm": "11.001", "loss_scale": "256", "train_wall": "266", "gb_free": "9.9", "wall": "35381"}
[2025-02-03 02:36:35,750][train_inner][INFO] - {"epoch": 40, "update": 39.46, "loss": "60.888", "nll_loss": "1.339", "total": "662.635", "n_correct": "533.595", "ppl": "2.53", "accuracy": "80.526", "wps": "447.7", "ups": "0.68", "wpb": "662.6", "bsz": "27.9", "num_updates": "32000", "lr": "0.000183126", "gnorm": "10.442", "loss_scale": "256", "train_wall": "283", "gb_free": "10.1", "wall": "35677"}
[2025-02-03 02:41:23,552][train_inner][INFO] - {"epoch": 40, "update": 39.494, "loss": "63.636", "nll_loss": "1.324", "total": "666.925", "n_correct": "539.05", "ppl": "2.5", "accuracy": "80.826", "wps": "463.5", "ups": "0.7", "wpb": "666.9", "bsz": "26.7", "num_updates": "32200", "lr": "0.000179505", "gnorm": "10.906", "loss_scale": "256", "train_wall": "277", "gb_free": "9.9", "wall": "35965"}
[2025-02-03 02:46:19,451][train_inner][INFO] - {"epoch": 40, "update": 39.528, "loss": "61.047", "nll_loss": "1.327", "total": "670.285", "n_correct": "541.105", "ppl": "2.51", "accuracy": "80.728", "wps": "453.1", "ups": "0.68", "wpb": "670.3", "bsz": "28", "num_updates": "32400", "lr": "0.000175955", "gnorm": "10.423", "loss_scale": "256", "train_wall": "284", "gb_free": "9.9", "wall": "36261"}
[2025-02-03 02:51:18,699][train_inner][INFO] - {"epoch": 40, "update": 39.562, "loss": "58.98", "nll_loss": "1.319", "total": "662.97", "n_correct": "536.45", "ppl": "2.5", "accuracy": "80.916", "wps": "443.1", "ups": "0.67", "wpb": "663", "bsz": "28.6", "num_updates": "32600", "lr": "0.000172476", "gnorm": "10.409", "loss_scale": "256", "train_wall": "286", "gb_free": "10.1", "wall": "36560"}
[2025-02-03 02:56:11,277][train_inner][INFO] - {"epoch": 40, "update": 39.596, "loss": "61.209", "nll_loss": "1.31", "total": "672.935", "n_correct": "545.425", "ppl": "2.48", "accuracy": "81.052", "wps": "460", "ups": "0.68", "wpb": "672.9", "bsz": "27.9", "num_updates": "32800", "lr": "0.000169066", "gnorm": "11.005", "loss_scale": "256", "train_wall": "280", "gb_free": "9.9", "wall": "36853"}
[2025-02-03 03:00:45,868][train_inner][INFO] - {"epoch": 40, "update": 39.63, "loss": "57.747", "nll_loss": "1.289", "total": "673.765", "n_correct": "547.725", "ppl": "2.44", "accuracy": "81.293", "wps": "490.8", "ups": "0.73", "wpb": "673.8", "bsz": "29.4", "num_updates": "33000", "lr": "0.000165723", "gnorm": "10.351", "loss_scale": "256", "train_wall": "265", "gb_free": "9.9", "wall": "37127"}
[2025-02-03 03:05:34,072][train_inner][INFO] - {"epoch": 40, "update": 39.664, "loss": "60.894", "nll_loss": "1.311", "total": "661.44", "n_correct": "536.535", "ppl": "2.48", "accuracy": "81.116", "wps": "459", "ups": "0.69", "wpb": "661.4", "bsz": "27.6", "num_updates": "33200", "lr": "0.000162446", "gnorm": "10.439", "loss_scale": "256", "train_wall": "276", "gb_free": "10", "wall": "37416"}
[2025-02-03 03:10:33,247][train_inner][INFO] - {"epoch": 40, "update": 39.698, "loss": "60.419", "nll_loss": "1.301", "total": "660.045", "n_correct": "535.94", "ppl": "2.46", "accuracy": "81.197", "wps": "441.3", "ups": "0.67", "wpb": "660", "bsz": "27.7", "num_updates": "33400", "lr": "0.000159234", "gnorm": "10.596", "loss_scale": "256", "train_wall": "286", "gb_free": "10.1", "wall": "37715"}
[2025-02-03 03:15:35,089][train_inner][INFO] - {"epoch": 40, "update": 39.733, "loss": "60.111", "nll_loss": "1.285", "total": "662.105", "n_correct": "538.57", "ppl": "2.44", "accuracy": "81.342", "wps": "438.7", "ups": "0.66", "wpb": "662.1", "bsz": "27.7", "num_updates": "33600", "lr": "0.000156085", "gnorm": "10.542", "loss_scale": "256", "train_wall": "289", "gb_free": "9.9", "wall": "38017"}
[2025-02-03 03:20:35,696][train_inner][INFO] - {"epoch": 40, "update": 39.767, "loss": "58.373", "nll_loss": "1.314", "total": "671.2", "n_correct": "542.82", "ppl": "2.49", "accuracy": "80.873", "wps": "446.7", "ups": "0.67", "wpb": "671.2", "bsz": "29.2", "num_updates": "33800", "lr": "0.000152999", "gnorm": "10.008", "loss_scale": "256", "train_wall": "288", "gb_free": "10", "wall": "38317"}
[2025-02-03 03:25:30,146][train_inner][INFO] - {"epoch": 40, "update": 39.801, "loss": "63.463", "nll_loss": "1.324", "total": "670.27", "n_correct": "542.09", "ppl": "2.5", "accuracy": "80.876", "wps": "455.3", "ups": "0.68", "wpb": "670.3", "bsz": "26.9", "num_updates": "34000", "lr": "0.000149973", "gnorm": "11.161", "loss_scale": "256", "train_wall": "283", "gb_free": "10", "wall": "38612"}
[2025-02-03 03:30:30,312][train_inner][INFO] - {"epoch": 40, "update": 39.835, "loss": "59.68", "nll_loss": "1.319", "total": "669.985", "n_correct": "542.015", "ppl": "2.49", "accuracy": "80.9", "wps": "446.4", "ups": "0.67", "wpb": "670", "bsz": "28.6", "num_updates": "34200", "lr": "0.000147008", "gnorm": "10.576", "loss_scale": "256", "train_wall": "288", "gb_free": "9.9", "wall": "38912"}
[2025-02-03 03:35:04,366][train_inner][INFO] - {"epoch": 40, "update": 39.869, "loss": "64.464", "nll_loss": "1.314", "total": "673.945", "n_correct": "545.955", "ppl": "2.49", "accuracy": "81.009", "wps": "491.9", "ups": "0.73", "wpb": "673.9", "bsz": "26.6", "num_updates": "34400", "lr": "0.000144101", "gnorm": "11.49", "loss_scale": "256", "train_wall": "262", "gb_free": "9.9", "wall": "39186"}
[2025-02-03 03:36:03,201][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-03 03:40:04,808][train_inner][INFO] - {"epoch": 40, "update": 39.903, "loss": "59.594", "nll_loss": "1.266", "total": "660.82", "n_correct": "539.835", "ppl": "2.41", "accuracy": "81.692", "wps": "439.9", "ups": "0.67", "wpb": "660.8", "bsz": "27.7", "num_updates": "34600", "lr": "0.000141252", "gnorm": "10.428", "loss_scale": "128", "train_wall": "288", "gb_free": "10", "wall": "39486"}
[2025-02-03 03:45:03,493][train_inner][INFO] - {"epoch": 40, "update": 39.937, "loss": "61.391", "nll_loss": "1.311", "total": "668.475", "n_correct": "541.61", "ppl": "2.48", "accuracy": "81.022", "wps": "447.7", "ups": "0.67", "wpb": "668.5", "bsz": "27.6", "num_updates": "34800", "lr": "0.000138459", "gnorm": "10.635", "loss_scale": "128", "train_wall": "286", "gb_free": "10", "wall": "39785"}
[2025-02-03 03:49:58,353][train_inner][INFO] - {"epoch": 40, "update": 39.972, "loss": "61.797", "nll_loss": "1.329", "total": "657.375", "n_correct": "530.455", "ppl": "2.51", "accuracy": "80.693", "wps": "445.9", "ups": "0.68", "wpb": "657.4", "bsz": "27.2", "num_updates": "35000", "lr": "0.000135721", "gnorm": "11.308", "loss_scale": "128", "train_wall": "282", "gb_free": "10", "wall": "40080"}
[2025-02-03 03:53:47,888][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 03:55:30,306][valid][INFO] - {"epoch": 40, "valid_loss": "27.085", "valid_nll_loss": "0.855", "valid_total": "678.4", "valid_n_correct": "594.3", "valid_ppl": "1.81", "valid_accuracy": "87.603", "valid_wps": "420.4", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "35167", "valid_best_accuracy": "87.603"}
[2025-02-03 03:55:30,313][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 35167 updates
[2025-02-03 03:55:30,316][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 03:55:39,171][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 03:55:42,612][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 40 @ 35167 updates, score 87.603) (writing took 12.298399731516838 seconds)
[2025-02-03 03:55:42,613][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2025-02-03 03:55:42,623][train][INFO] - {"epoch": 40, "train_loss": "60.538", "train_nll_loss": "1.313", "train_total": "665.793", "train_n_correct": "538.983", "train_ppl": "2.48", "train_accuracy": "80.954", "train_wps": "447.5", "train_ups": "0.67", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "35167", "train_lr": "0.000133476", "train_gnorm": "10.638", "train_loss_scale": "128", "train_train_wall": "8173", "train_gb_free": "9.9", "train_wall": "40424"}
[2025-02-03 03:55:44,277][fairseq.trainer][INFO] - begin training epoch 41
[2025-02-03 03:55:44,279][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 03:57:54,347][train_inner][INFO] - {"epoch": 41, "update": 40.006, "loss": "60.212", "nll_loss": "1.305", "total": "673.115", "n_correct": "546.305", "ppl": "2.47", "accuracy": "81.161", "wps": "282.8", "ups": "0.42", "wpb": "673.1", "bsz": "28.3", "num_updates": "35200", "lr": "0.000133037", "gnorm": "10.633", "loss_scale": "128", "train_wall": "266", "gb_free": "10", "wall": "40556"}
[2025-02-03 04:02:48,739][train_inner][INFO] - {"epoch": 41, "update": 40.04, "loss": "59.098", "nll_loss": "1.242", "total": "667.305", "n_correct": "546.41", "ppl": "2.36", "accuracy": "81.883", "wps": "453.4", "ups": "0.68", "wpb": "667.3", "bsz": "28", "num_updates": "35400", "lr": "0.000130407", "gnorm": "10.383", "loss_scale": "128", "train_wall": "283", "gb_free": "10", "wall": "40850"}
[2025-02-03 04:07:09,571][train_inner][INFO] - {"epoch": 41, "update": 40.074, "loss": "56.91", "nll_loss": "1.219", "total": "669.04", "n_correct": "550.58", "ppl": "2.33", "accuracy": "82.294", "wps": "513", "ups": "0.77", "wpb": "669", "bsz": "28.9", "num_updates": "35600", "lr": "0.000127828", "gnorm": "10.106", "loss_scale": "128", "train_wall": "251", "gb_free": "10", "wall": "41111"}
[2025-02-03 04:12:01,396][train_inner][INFO] - {"epoch": 41, "update": 40.108, "loss": "56.745", "nll_loss": "1.238", "total": "661.51", "n_correct": "541.985", "ppl": "2.36", "accuracy": "81.931", "wps": "453.4", "ups": "0.69", "wpb": "661.5", "bsz": "28.9", "num_updates": "35800", "lr": "0.0001253", "gnorm": "10.742", "loss_scale": "128", "train_wall": "280", "gb_free": "10", "wall": "41403"}
[2025-02-03 04:16:49,555][train_inner][INFO] - {"epoch": 41, "update": 40.142, "loss": "61.056", "nll_loss": "1.249", "total": "670.78", "n_correct": "548.505", "ppl": "2.38", "accuracy": "81.771", "wps": "465.6", "ups": "0.69", "wpb": "670.8", "bsz": "27.3", "num_updates": "36000", "lr": "0.000122823", "gnorm": "10.973", "loss_scale": "128", "train_wall": "277", "gb_free": "10.1", "wall": "41691"}
[2025-02-03 04:21:49,314][train_inner][INFO] - {"epoch": 41, "update": 40.176, "loss": "58.944", "nll_loss": "1.257", "total": "666.16", "n_correct": "544.2", "ppl": "2.39", "accuracy": "81.692", "wps": "444.5", "ups": "0.67", "wpb": "666.2", "bsz": "28.2", "num_updates": "36200", "lr": "0.000120394", "gnorm": "10.882", "loss_scale": "128", "train_wall": "289", "gb_free": "10", "wall": "41991"}
[2025-02-03 04:26:35,086][train_inner][INFO] - {"epoch": 41, "update": 40.21, "loss": "62.94", "nll_loss": "1.298", "total": "671.015", "n_correct": "544.205", "ppl": "2.46", "accuracy": "81.102", "wps": "469.8", "ups": "0.7", "wpb": "671", "bsz": "27", "num_updates": "36400", "lr": "0.000118014", "gnorm": "11.549", "loss_scale": "128", "train_wall": "274", "gb_free": "10", "wall": "42276"}
[2025-02-03 04:31:22,796][train_inner][INFO] - {"epoch": 41, "update": 40.244, "loss": "60.833", "nll_loss": "1.259", "total": "663.565", "n_correct": "542.69", "ppl": "2.39", "accuracy": "81.784", "wps": "461.3", "ups": "0.7", "wpb": "663.6", "bsz": "27.2", "num_updates": "36600", "lr": "0.00011568", "gnorm": "11.061", "loss_scale": "128", "train_wall": "277", "gb_free": "9.9", "wall": "42564"}
[2025-02-03 04:36:19,127][train_inner][INFO] - {"epoch": 41, "update": 40.279, "loss": "57.786", "nll_loss": "1.249", "total": "662.685", "n_correct": "542.08", "ppl": "2.38", "accuracy": "81.801", "wps": "447.3", "ups": "0.67", "wpb": "662.7", "bsz": "28.5", "num_updates": "36800", "lr": "0.000113393", "gnorm": "10.664", "loss_scale": "128", "train_wall": "285", "gb_free": "10", "wall": "42861"}
[2025-02-03 04:40:56,700][train_inner][INFO] - {"epoch": 41, "update": 40.313, "loss": "59.343", "nll_loss": "1.268", "total": "662.945", "n_correct": "540.43", "ppl": "2.41", "accuracy": "81.52", "wps": "477.7", "ups": "0.72", "wpb": "662.9", "bsz": "28", "num_updates": "37000", "lr": "0.00011115", "gnorm": "10.795", "loss_scale": "128", "train_wall": "267", "gb_free": "10", "wall": "43138"}
[2025-02-03 04:45:59,956][train_inner][INFO] - {"epoch": 41, "update": 40.347, "loss": "58.272", "nll_loss": "1.259", "total": "665.405", "n_correct": "543.335", "ppl": "2.39", "accuracy": "81.655", "wps": "438.9", "ups": "0.66", "wpb": "665.4", "bsz": "28.5", "num_updates": "37200", "lr": "0.000108953", "gnorm": "10.292", "loss_scale": "128", "train_wall": "291", "gb_free": "10.1", "wall": "43441"}
[2025-02-03 04:50:58,611][train_inner][INFO] - {"epoch": 41, "update": 40.381, "loss": "58.908", "nll_loss": "1.229", "total": "666.54", "n_correct": "547.99", "ppl": "2.34", "accuracy": "82.214", "wps": "446.4", "ups": "0.67", "wpb": "666.5", "bsz": "27.9", "num_updates": "37400", "lr": "0.000106798", "gnorm": "10.898", "loss_scale": "128", "train_wall": "285", "gb_free": "9.9", "wall": "43740"}
[2025-02-03 04:56:05,430][train_inner][INFO] - {"epoch": 41, "update": 40.415, "loss": "58.129", "nll_loss": "1.226", "total": "668.555", "n_correct": "549.335", "ppl": "2.34", "accuracy": "82.168", "wps": "435.8", "ups": "0.65", "wpb": "668.6", "bsz": "28.4", "num_updates": "37600", "lr": "0.000104687", "gnorm": "10.465", "loss_scale": "128", "train_wall": "296", "gb_free": "9.9", "wall": "44047"}
[2025-02-03 05:01:04,850][train_inner][INFO] - {"epoch": 41, "update": 40.449, "loss": "60.656", "nll_loss": "1.271", "total": "667.25", "n_correct": "543.875", "ppl": "2.41", "accuracy": "81.51", "wps": "445.7", "ups": "0.67", "wpb": "667.2", "bsz": "27.6", "num_updates": "37800", "lr": "0.000102617", "gnorm": "11.012", "loss_scale": "128", "train_wall": "287", "gb_free": "10", "wall": "44346"}
[2025-02-03 05:05:59,105][train_inner][INFO] - {"epoch": 41, "update": 40.483, "loss": "60.62", "nll_loss": "1.275", "total": "660.125", "n_correct": "538.365", "ppl": "2.42", "accuracy": "81.555", "wps": "448.7", "ups": "0.68", "wpb": "660.1", "bsz": "27.3", "num_updates": "38000", "lr": "0.000100587", "gnorm": "11.031", "loss_scale": "128", "train_wall": "282", "gb_free": "10", "wall": "44641"}
[2025-02-03 05:10:45,746][train_inner][INFO] - {"epoch": 41, "update": 40.517, "loss": "59.672", "nll_loss": "1.251", "total": "668.4", "n_correct": "546.98", "ppl": "2.38", "accuracy": "81.834", "wps": "466.4", "ups": "0.7", "wpb": "668.4", "bsz": "27.9", "num_updates": "38200", "lr": "9.85985e-05", "gnorm": "11.233", "loss_scale": "128", "train_wall": "276", "gb_free": "9.9", "wall": "44927"}
[2025-02-03 05:15:14,427][train_inner][INFO] - {"epoch": 41, "update": 40.552, "loss": "60.422", "nll_loss": "1.272", "total": "669.21", "n_correct": "544.75", "ppl": "2.41", "accuracy": "81.402", "wps": "498.2", "ups": "0.74", "wpb": "669.2", "bsz": "27.8", "num_updates": "38400", "lr": "9.66488e-05", "gnorm": "11.054", "loss_scale": "128", "train_wall": "259", "gb_free": "10", "wall": "45196"}
[2025-02-03 05:19:36,559][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-03 05:20:20,719][train_inner][INFO] - {"epoch": 41, "update": 40.586, "loss": "58.969", "nll_loss": "1.247", "total": "670.145", "n_correct": "548.19", "ppl": "2.37", "accuracy": "81.802", "wps": "437.6", "ups": "0.65", "wpb": "670.1", "bsz": "28.2", "num_updates": "38600", "lr": "9.47378e-05", "gnorm": "10.631", "loss_scale": "128", "train_wall": "293", "gb_free": "9.9", "wall": "45502"}
[2025-02-03 05:25:15,312][train_inner][INFO] - {"epoch": 41, "update": 40.62, "loss": "61.345", "nll_loss": "1.259", "total": "662.85", "n_correct": "541.135", "ppl": "2.39", "accuracy": "81.638", "wps": "450.1", "ups": "0.68", "wpb": "662.9", "bsz": "27", "num_updates": "38800", "lr": "9.28645e-05", "gnorm": "11.273", "loss_scale": "128", "train_wall": "282", "gb_free": "9.9", "wall": "45797"}
[2025-02-03 05:30:22,135][train_inner][INFO] - {"epoch": 41, "update": 40.654, "loss": "58.016", "nll_loss": "1.248", "total": "662.575", "n_correct": "542", "ppl": "2.37", "accuracy": "81.802", "wps": "432", "ups": "0.65", "wpb": "662.6", "bsz": "28.4", "num_updates": "39000", "lr": "9.10282e-05", "gnorm": "10.805", "loss_scale": "128", "train_wall": "294", "gb_free": "10", "wall": "46104"}
[2025-02-03 05:35:26,573][train_inner][INFO] - {"epoch": 41, "update": 40.688, "loss": "58.698", "nll_loss": "1.222", "total": "668.16", "n_correct": "549.505", "ppl": "2.33", "accuracy": "82.242", "wps": "439", "ups": "0.66", "wpb": "668.2", "bsz": "28", "num_updates": "39200", "lr": "8.92283e-05", "gnorm": "10.887", "loss_scale": "128", "train_wall": "293", "gb_free": "10", "wall": "46408"}
[2025-02-03 05:40:34,828][train_inner][INFO] - {"epoch": 41, "update": 40.722, "loss": "56.76", "nll_loss": "1.239", "total": "662.35", "n_correct": "543.96", "ppl": "2.36", "accuracy": "82.126", "wps": "429.8", "ups": "0.65", "wpb": "662.4", "bsz": "28.9", "num_updates": "39400", "lr": "8.74639e-05", "gnorm": "10.442", "loss_scale": "128", "train_wall": "296", "gb_free": "9.9", "wall": "46716"}
[2025-02-03 05:45:13,987][train_inner][INFO] - {"epoch": 41, "update": 40.756, "loss": "59.243", "nll_loss": "1.264", "total": "660.365", "n_correct": "538.685", "ppl": "2.4", "accuracy": "81.574", "wps": "473.1", "ups": "0.72", "wpb": "660.4", "bsz": "27.8", "num_updates": "39600", "lr": "8.57345e-05", "gnorm": "10.817", "loss_scale": "128", "train_wall": "269", "gb_free": "9.9", "wall": "46995"}
[2025-02-03 05:50:15,259][train_inner][INFO] - {"epoch": 41, "update": 40.791, "loss": "58.42", "nll_loss": "1.275", "total": "668.11", "n_correct": "545.295", "ppl": "2.42", "accuracy": "81.618", "wps": "443.6", "ups": "0.66", "wpb": "668.1", "bsz": "28.7", "num_updates": "39800", "lr": "8.40392e-05", "gnorm": "10.743", "loss_scale": "128", "train_wall": "289", "gb_free": "10.1", "wall": "47297"}
[2025-02-03 05:55:08,948][train_inner][INFO] - {"epoch": 41, "update": 40.825, "loss": "60.979", "nll_loss": "1.239", "total": "663.46", "n_correct": "543.68", "ppl": "2.36", "accuracy": "81.946", "wps": "451.8", "ups": "0.68", "wpb": "663.5", "bsz": "26.9", "num_updates": "40000", "lr": "8.23774e-05", "gnorm": "10.983", "loss_scale": "128", "train_wall": "282", "gb_free": "9.9", "wall": "47590"}
[2025-02-03 06:00:10,310][train_inner][INFO] - {"epoch": 41, "update": 40.859, "loss": "57.224", "nll_loss": "1.224", "total": "666.875", "n_correct": "548.4", "ppl": "2.34", "accuracy": "82.234", "wps": "442.6", "ups": "0.66", "wpb": "666.9", "bsz": "28.7", "num_updates": "40200", "lr": "8.07486e-05", "gnorm": "10.324", "loss_scale": "128", "train_wall": "290", "gb_free": "10", "wall": "47892"}
[2025-02-03 06:05:08,824][train_inner][INFO] - {"epoch": 41, "update": 40.893, "loss": "59.338", "nll_loss": "1.244", "total": "666.615", "n_correct": "546.67", "ppl": "2.37", "accuracy": "82.007", "wps": "446.6", "ups": "0.67", "wpb": "666.6", "bsz": "27.9", "num_updates": "40400", "lr": "7.91519e-05", "gnorm": "10.745", "loss_scale": "128", "train_wall": "286", "gb_free": "10.1", "wall": "48190"}
[2025-02-03 06:10:07,097][train_inner][INFO] - {"epoch": 41, "update": 40.927, "loss": "59.38", "nll_loss": "1.244", "total": "657.145", "n_correct": "538.61", "ppl": "2.37", "accuracy": "81.962", "wps": "440.7", "ups": "0.67", "wpb": "657.1", "bsz": "27.5", "num_updates": "40600", "lr": "7.75868e-05", "gnorm": "10.612", "loss_scale": "128", "train_wall": "286", "gb_free": "9.9", "wall": "48489"}
[2025-02-03 06:14:51,948][train_inner][INFO] - {"epoch": 41, "update": 40.961, "loss": "62.422", "nll_loss": "1.22", "total": "672.265", "n_correct": "553.71", "ppl": "2.33", "accuracy": "82.365", "wps": "472.1", "ups": "0.7", "wpb": "672.3", "bsz": "26.5", "num_updates": "40800", "lr": "7.60526e-05", "gnorm": "11.227", "loss_scale": "128", "train_wall": "273", "gb_free": "9.9", "wall": "48773"}
[2025-02-03 06:19:18,516][train_inner][INFO] - {"epoch": 41, "update": 40.995, "loss": "58.091", "nll_loss": "1.248", "total": "663.54", "n_correct": "543.69", "ppl": "2.38", "accuracy": "81.938", "wps": "497.9", "ups": "0.75", "wpb": "663.5", "bsz": "28.4", "num_updates": "41000", "lr": "7.45488e-05", "gnorm": "10.96", "loss_scale": "128", "train_wall": "256", "gb_free": "9.9", "wall": "49040"}
[2025-02-03 06:19:37,568][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2025-02-03 06:19:37,571][train][INFO] - {"epoch": 41, "train_loss": "59.245", "train_nll_loss": "1.25", "train_total": "665.826", "train_n_correct": "544.898", "train_ppl": "2.38", "train_accuracy": "81.838", "train_wps": "451.9", "train_ups": "0.68", "train_wpb": "665.8", "train_bsz": "27.9", "train_num_updates": "41028", "train_lr": "7.43407e-05", "train_gnorm": "10.819", "train_loss_scale": "128", "train_train_wall": "8212", "train_gb_free": "10", "train_wall": "49059"}
[2025-02-03 06:19:38,527][fairseq.trainer][INFO] - begin training epoch 42
[2025-02-03 06:19:38,532][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 06:25:11,963][train_inner][INFO] - {"epoch": 42, "update": 41.029, "loss": "58.318", "nll_loss": "1.247", "total": "671.105", "n_correct": "548.69", "ppl": "2.37", "accuracy": "81.759", "wps": "379.8", "ups": "0.57", "wpb": "671.1", "bsz": "28.6", "num_updates": "41200", "lr": "7.30747e-05", "gnorm": "11.083", "loss_scale": "128", "train_wall": "279", "gb_free": "10", "wall": "49393"}
[2025-02-03 06:30:10,490][train_inner][INFO] - {"epoch": 42, "update": 41.063, "loss": "57.993", "nll_loss": "1.207", "total": "664.795", "n_correct": "548.695", "ppl": "2.31", "accuracy": "82.536", "wps": "445.4", "ups": "0.67", "wpb": "664.8", "bsz": "28.1", "num_updates": "41400", "lr": "7.16298e-05", "gnorm": "11.036", "loss_scale": "128", "train_wall": "285", "gb_free": "10", "wall": "49692"}
[2025-02-03 06:35:06,768][train_inner][INFO] - {"epoch": 42, "update": 41.098, "loss": "59.629", "nll_loss": "1.201", "total": "663.42", "n_correct": "547.325", "ppl": "2.3", "accuracy": "82.501", "wps": "447.9", "ups": "0.68", "wpb": "663.4", "bsz": "27.2", "num_updates": "41600", "lr": "7.02134e-05", "gnorm": "10.779", "loss_scale": "128", "train_wall": "283", "gb_free": "10", "wall": "49988"}
[2025-02-03 06:40:10,877][train_inner][INFO] - {"epoch": 42, "update": 41.132, "loss": "57.309", "nll_loss": "1.197", "total": "669.54", "n_correct": "552.505", "ppl": "2.29", "accuracy": "82.52", "wps": "440.4", "ups": "0.66", "wpb": "669.5", "bsz": "28.5", "num_updates": "41800", "lr": "6.88251e-05", "gnorm": "10.806", "loss_scale": "128", "train_wall": "291", "gb_free": "10", "wall": "50292"}
[2025-02-03 06:45:11,666][train_inner][INFO] - {"epoch": 42, "update": 41.166, "loss": "57.213", "nll_loss": "1.216", "total": "672.035", "n_correct": "553.605", "ppl": "2.32", "accuracy": "82.377", "wps": "446.9", "ups": "0.66", "wpb": "672", "bsz": "28.9", "num_updates": "42000", "lr": "6.74641e-05", "gnorm": "10.685", "loss_scale": "128", "train_wall": "288", "gb_free": "10", "wall": "50593"}
[2025-02-03 06:50:08,059][train_inner][INFO] - {"epoch": 42, "update": 41.2, "loss": "59.288", "nll_loss": "1.218", "total": "670.095", "n_correct": "551.215", "ppl": "2.33", "accuracy": "82.259", "wps": "452.2", "ups": "0.67", "wpb": "670.1", "bsz": "27.8", "num_updates": "42200", "lr": "6.61301e-05", "gnorm": "11", "loss_scale": "128", "train_wall": "285", "gb_free": "9.9", "wall": "50890"}
[2025-02-03 06:54:55,555][train_inner][INFO] - {"epoch": 42, "update": 41.234, "loss": "57.076", "nll_loss": "1.202", "total": "666.155", "n_correct": "549.745", "ppl": "2.3", "accuracy": "82.525", "wps": "463.5", "ups": "0.7", "wpb": "666.2", "bsz": "28.6", "num_updates": "42400", "lr": "6.48225e-05", "gnorm": "11.019", "loss_scale": "128", "train_wall": "275", "gb_free": "9.9", "wall": "51177"}
[2025-02-03 06:59:58,013][train_inner][INFO] - {"epoch": 42, "update": 41.268, "loss": "57.107", "nll_loss": "1.199", "total": "660.325", "n_correct": "545.655", "ppl": "2.3", "accuracy": "82.634", "wps": "436.7", "ups": "0.66", "wpb": "660.3", "bsz": "28.2", "num_updates": "42600", "lr": "6.35408e-05", "gnorm": "10.399", "loss_scale": "128", "train_wall": "290", "gb_free": "10", "wall": "51479"}
[2025-02-03 07:04:54,065][train_inner][INFO] - {"epoch": 42, "update": 41.302, "loss": "58.126", "nll_loss": "1.17", "total": "663.79", "n_correct": "550.895", "ppl": "2.25", "accuracy": "82.992", "wps": "448.5", "ups": "0.68", "wpb": "663.8", "bsz": "27.6", "num_updates": "42800", "lr": "6.22843e-05", "gnorm": "10.708", "loss_scale": "256", "train_wall": "285", "gb_free": "9.9", "wall": "51775"}
[2025-02-03 07:09:49,471][train_inner][INFO] - {"epoch": 42, "update": 41.336, "loss": "60.881", "nll_loss": "1.237", "total": "658.93", "n_correct": "539.43", "ppl": "2.36", "accuracy": "81.865", "wps": "446.2", "ups": "0.68", "wpb": "658.9", "bsz": "26.8", "num_updates": "43000", "lr": "6.10528e-05", "gnorm": "11.463", "loss_scale": "256", "train_wall": "283", "gb_free": "9.9", "wall": "52071"}
[2025-02-03 07:14:50,669][train_inner][INFO] - {"epoch": 42, "update": 41.371, "loss": "58.945", "nll_loss": "1.176", "total": "665.03", "n_correct": "551.38", "ppl": "2.26", "accuracy": "82.911", "wps": "441.7", "ups": "0.66", "wpb": "665", "bsz": "27.3", "num_updates": "43200", "lr": "5.98455e-05", "gnorm": "10.92", "loss_scale": "256", "train_wall": "289", "gb_free": "10", "wall": "52372"}
[2025-02-03 07:19:46,596][train_inner][INFO] - {"epoch": 42, "update": 41.405, "loss": "60.24", "nll_loss": "1.244", "total": "667.315", "n_correct": "545.98", "ppl": "2.37", "accuracy": "81.817", "wps": "451.1", "ups": "0.68", "wpb": "667.3", "bsz": "27.5", "num_updates": "43400", "lr": "5.86622e-05", "gnorm": "11.62", "loss_scale": "256", "train_wall": "284", "gb_free": "10", "wall": "52668"}
[2025-02-03 07:24:16,447][train_inner][INFO] - {"epoch": 42, "update": 41.439, "loss": "60.511", "nll_loss": "1.245", "total": "662.355", "n_correct": "542.315", "ppl": "2.37", "accuracy": "81.877", "wps": "490.9", "ups": "0.74", "wpb": "662.4", "bsz": "27.2", "num_updates": "43600", "lr": "5.75022e-05", "gnorm": "11.109", "loss_scale": "256", "train_wall": "260", "gb_free": "9.9", "wall": "52938"}
[2025-02-03 07:26:02,908][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-03 07:29:01,965][train_inner][INFO] - {"epoch": 42, "update": 41.473, "loss": "58.033", "nll_loss": "1.204", "total": "668.585", "n_correct": "551.255", "ppl": "2.3", "accuracy": "82.451", "wps": "468.4", "ups": "0.7", "wpb": "668.6", "bsz": "28.2", "num_updates": "43800", "lr": "5.63652e-05", "gnorm": "10.917", "loss_scale": "128", "train_wall": "275", "gb_free": "9.9", "wall": "53223"}
[2025-02-03 07:34:05,075][train_inner][INFO] - {"epoch": 42, "update": 41.507, "loss": "56.798", "nll_loss": "1.224", "total": "667.55", "n_correct": "548.805", "ppl": "2.34", "accuracy": "82.212", "wps": "440.5", "ups": "0.66", "wpb": "667.6", "bsz": "29", "num_updates": "44000", "lr": "5.52507e-05", "gnorm": "10.809", "loss_scale": "128", "train_wall": "291", "gb_free": "10", "wall": "53527"}
[2025-02-03 07:38:57,573][train_inner][INFO] - {"epoch": 42, "update": 41.541, "loss": "60.475", "nll_loss": "1.214", "total": "669.015", "n_correct": "550.865", "ppl": "2.32", "accuracy": "82.34", "wps": "457.5", "ups": "0.68", "wpb": "669", "bsz": "27.2", "num_updates": "44200", "lr": "5.41582e-05", "gnorm": "11.63", "loss_scale": "128", "train_wall": "280", "gb_free": "9.9", "wall": "53819"}
[2025-02-03 07:43:57,870][train_inner][INFO] - {"epoch": 42, "update": 41.575, "loss": "57.373", "nll_loss": "1.193", "total": "662.43", "n_correct": "547.145", "ppl": "2.29", "accuracy": "82.597", "wps": "441.2", "ups": "0.67", "wpb": "662.4", "bsz": "28.1", "num_updates": "44400", "lr": "5.30873e-05", "gnorm": "10.775", "loss_scale": "128", "train_wall": "288", "gb_free": "10.1", "wall": "54119"}
[2025-02-03 07:48:47,468][train_inner][INFO] - {"epoch": 42, "update": 41.61, "loss": "59.552", "nll_loss": "1.19", "total": "666.095", "n_correct": "550.835", "ppl": "2.28", "accuracy": "82.696", "wps": "460", "ups": "0.69", "wpb": "666.1", "bsz": "27.2", "num_updates": "44600", "lr": "5.20376e-05", "gnorm": "10.986", "loss_scale": "128", "train_wall": "278", "gb_free": "10", "wall": "54409"}
[2025-02-03 07:53:39,677][train_inner][INFO] - {"epoch": 42, "update": 41.644, "loss": "60.006", "nll_loss": "1.207", "total": "666.415", "n_correct": "549.475", "ppl": "2.31", "accuracy": "82.452", "wps": "456.2", "ups": "0.68", "wpb": "666.4", "bsz": "27.2", "num_updates": "44800", "lr": "5.10086e-05", "gnorm": "11.09", "loss_scale": "128", "train_wall": "282", "gb_free": "9.9", "wall": "54701"}
[2025-02-03 07:58:10,804][train_inner][INFO] - {"epoch": 42, "update": 41.678, "loss": "61.262", "nll_loss": "1.223", "total": "664.35", "n_correct": "546.025", "ppl": "2.33", "accuracy": "82.189", "wps": "490.1", "ups": "0.74", "wpb": "664.4", "bsz": "26.7", "num_updates": "45000", "lr": "5e-05", "gnorm": "11.252", "loss_scale": "128", "train_wall": "261", "gb_free": "10", "wall": "54972"}
[2025-02-03 07:58:10,814][fairseq_cli.train][INFO] - Stopping training due to num_updates: 45000 >= max_update: 45000
[2025-02-03 07:58:10,819][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 07:59:54,383][valid][INFO] - {"epoch": 42, "valid_loss": "26.982", "valid_nll_loss": "0.843", "valid_total": "678.4", "valid_n_correct": "596.05", "valid_ppl": "1.79", "valid_accuracy": "87.861", "valid_wps": "385.6", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "45000", "valid_best_accuracy": "87.861"}
[2025-02-03 07:59:54,390][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 45000 updates
[2025-02-03 07:59:54,393][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 08:00:03,146][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 08:00:06,673][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 42 @ 45000 updates, score 87.861) (writing took 12.282798424363136 seconds)
[2025-02-03 08:00:06,677][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2025-02-03 08:00:06,687][train][INFO] - {"epoch": 42, "train_loss": "58.787", "train_nll_loss": "1.21", "train_total": "665.845", "train_n_correct": "548.52", "train_ppl": "2.31", "train_accuracy": "82.379", "train_wps": "438.7", "train_ups": "0.66", "train_wpb": "665.8", "train_bsz": "27.8", "train_num_updates": "45000", "train_lr": "5e-05", "train_gnorm": "11.001", "train_loss_scale": "128", "train_train_wall": "5614", "train_gb_free": "10", "train_wall": "55088"}
[2025-02-03 08:00:06,689][fairseq_cli.train][INFO] - done training in 55081.4 seconds
