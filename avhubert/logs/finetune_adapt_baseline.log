2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19439
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19439
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19439
[W202 22:38:06.871813949 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 2
[W202 22:38:06.876991334 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 3
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19439
[W202 22:38:06.357828028 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 1
[W202 22:38:06.366948026 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-02-02 22:38:06 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 0
[2025-02-02 22:38:08,721][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/workspace/hubert/av_hubert-main/avhubert', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19439', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 2, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 30000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'av_hubert_seq2seq', 'w2v_path': '/workspace/AV_HuBERT_pretrained/base_vox_iter5.pt', 'apply_mask': False, 'mask_selection': 'static', 'mask_length': 10, 'mask_other': 0, 'mask_prob': 0.75, 'mask_channel_selection': 'static', 'mask_channel_length': 64, 'mask_channel_other': 0, 'mask_channel_prob': 0.5, 'layerdrop': 0.1, 'dropout': 0.0, 'activation_dropout': 0.1, 'attention_dropout': 0.0, 'feature_grad_mult': 1.0, 'decoder_layers': 6, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.0, 'decoder_activation_dropout': 0.1, 'freeze_finetune_updates': 24000, 'share_decoder_input_output_embed': True, 'decoder_normalize_before': True, 'prompting': True}, 'task': {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/workspace/lrs2/29h_data', 'label_dir': '/workspace/lrs2/29h_data', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'fine_tuning': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video', 'audio'], 'image_aug': True, 'pad_audio': True, 'random_crop': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': True, 'ignore_prefix_size': 0, 'sentence_avg': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 10000, 'hold_steps': 0, 'decay_steps': 20000, 'phase_ratio': None, 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 30000, 'lr': [0.001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2025-02-02 22:38:08,749][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/avhubert/finetune_adapt_baseline
[2025-02-02 22:38:08,750][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/29h_data', 'labels': ['wrd'], 'label_dir': '/workspace/lrs2/29h_data', 'label_rate': -1, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 500, 'min_sample_size': None, 'max_trim_sample_size': '${task.max_sample_size}', 'single_target': True, 'random_crop': False, 'pad_audio': True, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['video', 'audio'], 'is_s2s': True, 'tokenizer_bpe_name': 'sentencepiece', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': True}
[2025-02-02 22:38:10,662][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/avhubert/finetune_adapt_baseline
[2025-02-02 22:38:10,662][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/29h_data', 'labels': ['km'], 'label_dir': '/checkpoint/bshi/data/lrs3//video/hubert/stitch-iters/envox-iter4-l12c2000/', 'label_rate': 25, 'sample_rate': 25, 'normalize': True, 'enable_padding': False, 'max_sample_size': 2000, 'min_sample_size': 5, 'max_trim_sample_size': 400, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['audio', 'video'], 'is_s2s': False, 'tokenizer_bpe_name': None, 'tokenizer_bpe_model': None, 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': False}
[2025-02-02 22:38:10,696][avhubert.hubert][INFO] - HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
BUILD BASELINE MODEL SUCCESS
[2025-02-02 22:38:24,084][fairseq_cli.train][INFO] - AVHubertSeq2Seq(
  (encoder): HubertEncoderWrapper(
    (w2v_model): AVHubertModel(
      (feature_extractor_audio): SubModel(
        (proj): Linear(in_features=104, out_features=768, bias=True)
      )
      (feature_extractor_video): SubModel(
        (resnet): ResEncoder(
          (frontend3D): Sequential(
            (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
            (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): PReLU(num_parameters=64)
            (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
          )
          (trunk): ResNet(
            (layer1): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer2): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer3): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer4): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (avgpool): AdaptiveAvgPool2d(output_size=1)
          )
        )
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
      (post_extract_proj): Linear(in_features=1536, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(1000, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
[2025-02-02 22:38:24,091][fairseq_cli.train][INFO] - task: AVHubertPretrainingTask
[2025-02-02 22:38:24,092][fairseq_cli.train][INFO] - model: AVHubertSeq2Seq
[2025-02-02 22:38:24,092][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2025-02-02 22:38:24,096][fairseq_cli.train][INFO] - num. shared model params: 160,613,608 (num. trained: 160,613,608)
[2025-02-02 22:38:24,098][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2025-02-02 22:38:24,100][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2025-02-02 22:38:24,114][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 1082, skipped 0 short and 0 long and 0 unaligned, longest-loaded=153, shortest-loaded=14
[2025-02-02 22:38:24,115][avhubert.hubert_dataset][INFO] - /workspace/lrs2/29h_data/valid.wrd is sequence label. skipped
[2025-02-02 22:38:24,116][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    <avhubert.utils.CenterCrop object at 0x7f57ada71340>
    Normalize(mean=0.421, std=0.165)
)
[2025-02-02 22:38:24,116][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-02-02 22:38:24,116][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2025-02-02 22:38:32,949][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv1.bias
[2025-02-02 22:38:32,951][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv2.bias
[2025-02-02 22:38:32,951][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv1.bias
[2025-02-02 22:38:32,952][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv2.bias
[2025-02-02 22:38:32,952][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv1.bias
[2025-02-02 22:38:32,953][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv2.bias
[2025-02-02 22:38:32,953][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.0.bias
[2025-02-02 22:38:32,954][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv1.bias
[2025-02-02 22:38:32,954][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv2.bias
[2025-02-02 22:38:32,955][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv1.bias
[2025-02-02 22:38:32,955][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv2.bias
[2025-02-02 22:38:32,955][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.0.bias
[2025-02-02 22:38:32,956][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv1.bias
[2025-02-02 22:38:32,970][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv2.bias
[2025-02-02 22:38:32,970][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv1.bias
[2025-02-02 22:38:32,970][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv2.bias
[2025-02-02 22:38:32,971][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.0.bias
[2025-02-02 22:38:32,971][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv1.bias
[2025-02-02 22:38:32,971][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv2.bias
[2025-02-02 22:38:34,190][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2025-02-02 22:38:34,191][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 22:38:34,191][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 22:38:34,192][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 22:38:34,192][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2025-02-02 22:38:34,193][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2025-02-02 22:38:34,194][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2025-02-02 22:38:34,195][fairseq_cli.train][INFO] - max tokens per device = 1000 and max sentences per device = None
[2025-02-02 22:38:34,199][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2025-02-02 22:38:34,200][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2025-02-02 22:38:34,201][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-02-02 22:38:34,203][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2025-02-02 22:38:34,838][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 45840, skipped 0 short and 0 long and 0 unaligned, longest-loaded=154, shortest-loaded=0
[2025-02-02 22:38:34,882][avhubert.hubert_dataset][INFO] - /workspace/lrs2/29h_data/train.wrd is sequence label. skipped
[2025-02-02 22:38:34,882][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    RandomCrop(size=(88, 88))
    <avhubert.utils.HorizontalFlip object at 0x7f57c393dee0>
    Normalize(mean=0.421, std=0.165)
)
[2025-02-02 22:38:34,883][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-02-02 22:38:34,883][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
BUILD BASELINE MODEL SUCCESS
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
BUILD BASELINE MODEL SUCCESS
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
BUILD BASELINE MODEL SUCCESS
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
[2025-02-02 22:38:40,718][fairseq.trainer][INFO] - begin training epoch 1
[2025-02-02 22:38:40,719][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
[2025-02-02 22:47:58,572][train_inner][INFO] - {"epoch": 1, "update": 0.249, "loss": "105.018", "nll_loss": "8.247", "total": "711.38", "n_correct": "67.385", "ppl": "303.74", "accuracy": "9.472", "wps": "302.4", "ups": "0.42", "wpb": "711.4", "bsz": "57.7", "num_updates": "200", "lr": "2.98e-05", "gnorm": "35.81", "loss_scale": "128", "train_wall": "464", "gb_free": "8", "wall": "564"}
[2025-02-02 22:55:42,904][train_inner][INFO] - {"epoch": 1, "update": 0.498, "loss": "102.468", "nll_loss": "7.793", "total": "717.53", "n_correct": "83.63", "ppl": "221.81", "accuracy": "11.655", "wps": "309.2", "ups": "0.43", "wpb": "717.5", "bsz": "56.9", "num_updates": "400", "lr": "4.96e-05", "gnorm": "23.707", "loss_scale": "128", "train_wall": "439", "gb_free": "8", "wall": "1029"}
[2025-02-02 23:03:26,315][train_inner][INFO] - {"epoch": 1, "update": 0.746, "loss": "97.807", "nll_loss": "7.442", "total": "705.65", "n_correct": "112.005", "ppl": "173.9", "accuracy": "15.873", "wps": "304.6", "ups": "0.43", "wpb": "705.6", "bsz": "56.5", "num_updates": "600", "lr": "6.94e-05", "gnorm": "24.667", "loss_scale": "128", "train_wall": "449", "gb_free": "8", "wall": "1492"}
[2025-02-02 23:10:21,720][train_inner][INFO] - {"epoch": 1, "update": 0.995, "loss": "92.943", "nll_loss": "7.007", "total": "714.165", "n_correct": "139.15", "ppl": "128.65", "accuracy": "19.484", "wps": "343.8", "ups": "0.48", "wpb": "714.2", "bsz": "57.3", "num_updates": "800", "lr": "8.92e-05", "gnorm": "26.826", "loss_scale": "128", "train_wall": "404", "gb_free": "8", "wall": "1908"}
[2025-02-02 23:10:22,931][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-02-02 23:10:22,935][train][INFO] - {"epoch": 1, "train_loss": "99.6", "train_nll_loss": "7.62", "train_total": "711.575", "train_n_correct": "100.58", "train_ppl": "196.75", "train_accuracy": "14.135", "train_wps": "315.2", "train_ups": "0.44", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "804", "train_lr": "8.9596e-05", "train_gnorm": "27.822", "train_loss_scale": "128", "train_train_wall": "1757", "train_gb_free": "8", "train_wall": "1909"}
[2025-02-02 23:10:24,790][fairseq.trainer][INFO] - begin training epoch 2
[2025-02-02 23:10:24,791][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2025-02-02 23:18:46,647][train_inner][INFO] - {"epoch": 2, "update": 1.244, "loss": "90.466", "nll_loss": "6.701", "total": "708.685", "n_correct": "150.44", "ppl": "104.04", "accuracy": "21.228", "wps": "280.7", "ups": "0.4", "wpb": "708.7", "bsz": "56.4", "num_updates": "1000", "lr": "0.000109", "gnorm": "29.099", "loss_scale": "128", "train_wall": "425", "gb_free": "8", "wall": "2412"}
[2025-02-02 23:26:47,284][train_inner][INFO] - {"epoch": 2, "update": 1.493, "loss": "82.79", "nll_loss": "6.3", "total": "724.9", "n_correct": "170.865", "ppl": "78.79", "accuracy": "23.571", "wps": "301.7", "ups": "0.42", "wpb": "724.9", "bsz": "60", "num_updates": "1200", "lr": "0.0001288", "gnorm": "27.822", "loss_scale": "128", "train_wall": "469", "gb_free": "8", "wall": "2893"}
[2025-02-02 23:34:23,131][train_inner][INFO] - {"epoch": 2, "update": 1.741, "loss": "82.628", "nll_loss": "6.048", "total": "707.985", "n_correct": "171.7", "ppl": "66.17", "accuracy": "24.252", "wps": "310.6", "ups": "0.44", "wpb": "708", "bsz": "56.9", "num_updates": "1400", "lr": "0.0001486", "gnorm": "28.259", "loss_scale": "128", "train_wall": "442", "gb_free": "8", "wall": "3349"}
[2025-02-02 23:41:25,960][train_inner][INFO] - {"epoch": 2, "update": 1.99, "loss": "82.751", "nll_loss": "5.796", "total": "706.075", "n_correct": "180.315", "ppl": "55.56", "accuracy": "25.538", "wps": "334", "ups": "0.47", "wpb": "706.1", "bsz": "54.9", "num_updates": "1600", "lr": "0.0001684", "gnorm": "30.845", "loss_scale": "128", "train_wall": "409", "gb_free": "8", "wall": "3772"}
[2025-02-02 23:41:29,267][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-02 23:43:08,362][valid][INFO] - {"epoch": 2, "valid_loss": "75.435", "valid_nll_loss": "5.287", "valid_total": "678.4", "valid_n_correct": "199.35", "valid_ppl": "39.04", "valid_accuracy": "29.385", "valid_wps": "328.2", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "1608"}
[2025-02-02 23:43:08,369][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1608 updates
[2025-02-02 23:43:08,371][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 23:43:19,781][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-02 23:43:24,184][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1608 updates, score 29.385) (writing took 15.815500053577125 seconds)
[2025-02-02 23:43:24,185][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-02-02 23:43:24,192][train][INFO] - {"epoch": 2, "train_loss": "84.504", "train_nll_loss": "6.203", "train_total": "711.575", "train_n_correct": "168.644", "train_ppl": "73.66", "train_accuracy": "23.7", "train_wps": "288.8", "train_ups": "0.41", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "1608", "train_lr": "0.000169192", "train_gnorm": "28.954", "train_loss_scale": "128", "train_train_wall": "1747", "train_gb_free": "8", "train_wall": "3890"}
[2025-02-02 23:43:26,753][fairseq.trainer][INFO] - begin training epoch 3
[2025-02-02 23:43:26,758][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-02 23:52:06,479][train_inner][INFO] - {"epoch": 3, "update": 2.239, "loss": "75.21", "nll_loss": "5.295", "total": "707.105", "n_correct": "206.065", "ppl": "39.25", "accuracy": "29.142", "wps": "220.8", "ups": "0.31", "wpb": "707.1", "bsz": "56.5", "num_updates": "1800", "lr": "0.0001882", "gnorm": "34.138", "loss_scale": "128", "train_wall": "365", "gb_free": "8", "wall": "4412"}
[2025-02-02 23:59:31,356][train_inner][INFO] - {"epoch": 3, "update": 2.488, "loss": "75.074", "nll_loss": "5.121", "total": "707.1", "n_correct": "216.6", "ppl": "34.8", "accuracy": "30.632", "wps": "317.9", "ups": "0.45", "wpb": "707.1", "bsz": "55.2", "num_updates": "2000", "lr": "0.000208", "gnorm": "39.983", "loss_scale": "128", "train_wall": "413", "gb_free": "8", "wall": "4857"}
[2025-02-03 00:07:13,452][train_inner][INFO] - {"epoch": 3, "update": 2.736, "loss": "69.016", "nll_loss": "4.787", "total": "713.525", "n_correct": "243.34", "ppl": "27.61", "accuracy": "34.104", "wps": "308.8", "ups": "0.43", "wpb": "713.5", "bsz": "57.7", "num_updates": "2200", "lr": "0.0002278", "gnorm": "38.444", "loss_scale": "128", "train_wall": "450", "gb_free": "8", "wall": "5319"}
[2025-02-03 00:15:08,040][train_inner][INFO] - {"epoch": 3, "update": 2.985, "loss": "65.539", "nll_loss": "4.514", "total": "715.79", "n_correct": "263.675", "ppl": "22.84", "accuracy": "36.837", "wps": "301.7", "ups": "0.42", "wpb": "715.8", "bsz": "58.5", "num_updates": "2400", "lr": "0.0002476", "gnorm": "41.415", "loss_scale": "128", "train_wall": "463", "gb_free": "8", "wall": "5794"}
[2025-02-03 00:15:14,305][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-02-03 00:15:14,323][train][INFO] - {"epoch": 3, "train_loss": "71.029", "train_nll_loss": "4.916", "train_total": "711.575", "train_n_correct": "233.511", "train_ppl": "30.19", "train_accuracy": "32.816", "train_wps": "299.5", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "2412", "train_lr": "0.000248788", "train_gnorm": "38.657", "train_loss_scale": "128", "train_train_wall": "1693", "train_gb_free": "8", "train_wall": "5800"}
[2025-02-03 00:15:18,328][fairseq.trainer][INFO] - begin training epoch 4
[2025-02-03 00:15:18,329][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 00:23:44,072][train_inner][INFO] - {"epoch": 4, "update": 3.234, "loss": "64.323", "nll_loss": "4.243", "total": "710.885", "n_correct": "281.86", "ppl": "18.93", "accuracy": "39.649", "wps": "275.5", "ups": "0.39", "wpb": "710.9", "bsz": "56.7", "num_updates": "2600", "lr": "0.0002674", "gnorm": "43.284", "loss_scale": "128", "train_wall": "431", "gb_free": "8", "wall": "6310"}
[2025-02-03 00:31:35,697][train_inner][INFO] - {"epoch": 4, "update": 3.483, "loss": "62.254", "nll_loss": "4.159", "total": "717.025", "n_correct": "293.76", "ppl": "17.86", "accuracy": "40.969", "wps": "304.1", "ups": "0.42", "wpb": "717", "bsz": "58.2", "num_updates": "2800", "lr": "0.0002872", "gnorm": "40.107", "loss_scale": "128", "train_wall": "460", "gb_free": "8", "wall": "6781"}
[2025-02-03 00:39:16,830][train_inner][INFO] - {"epoch": 4, "update": 3.731, "loss": "64.13", "nll_loss": "4.07", "total": "704.55", "n_correct": "298.27", "ppl": "16.79", "accuracy": "42.335", "wps": "305.6", "ups": "0.43", "wpb": "704.6", "bsz": "54.7", "num_updates": "3000", "lr": "0.000307", "gnorm": "41.986", "loss_scale": "128", "train_wall": "392", "gb_free": "8", "wall": "7243"}
[2025-02-03 00:46:59,122][train_inner][INFO] - {"epoch": 4, "update": 3.98, "loss": "59.301", "nll_loss": "3.892", "total": "716.28", "n_correct": "320.45", "ppl": "14.84", "accuracy": "44.738", "wps": "309.9", "ups": "0.43", "wpb": "716.3", "bsz": "58.3", "num_updates": "3200", "lr": "0.0003268", "gnorm": "37.705", "loss_scale": "128", "train_wall": "416", "gb_free": "8", "wall": "7705"}
[2025-02-03 00:47:10,178][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 00:48:47,683][valid][INFO] - {"epoch": 4, "valid_loss": "57.893", "valid_nll_loss": "3.61", "valid_total": "678.4", "valid_n_correct": "323.8", "valid_ppl": "12.21", "valid_accuracy": "47.73", "valid_wps": "354", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "3216", "valid_best_accuracy": "47.73"}
[2025-02-03 00:48:47,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 3216 updates
[2025-02-03 00:48:47,688][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 00:49:03,318][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 00:49:08,622][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 3216 updates, score 47.73) (writing took 20.935083022341132 seconds)
[2025-02-03 00:49:08,623][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2025-02-03 00:49:08,632][train][INFO] - {"epoch": 4, "train_loss": "62.233", "train_nll_loss": "4.077", "train_total": "711.575", "train_n_correct": "299.432", "train_ppl": "16.88", "train_accuracy": "42.08", "train_wps": "281.2", "train_ups": "0.4", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "3216", "train_lr": "0.000328384", "train_gnorm": "40.549", "train_loss_scale": "128", "train_train_wall": "1704", "train_gb_free": "8", "train_wall": "7834"}
[2025-02-03 00:49:11,204][fairseq.trainer][INFO] - begin training epoch 5
[2025-02-03 00:49:11,204][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 00:57:31,337][train_inner][INFO] - {"epoch": 5, "update": 4.229, "loss": "58.537", "nll_loss": "3.655", "total": "702.925", "n_correct": "334.03", "ppl": "12.59", "accuracy": "47.52", "wps": "222.4", "ups": "0.32", "wpb": "702.9", "bsz": "55.5", "num_updates": "3400", "lr": "0.0003466", "gnorm": "37.685", "loss_scale": "128", "train_wall": "408", "gb_free": "8", "wall": "8337"}
[2025-02-03 01:05:20,640][train_inner][INFO] - {"epoch": 5, "update": 4.478, "loss": "56.326", "nll_loss": "3.571", "total": "716.63", "n_correct": "351.96", "ppl": "11.89", "accuracy": "49.113", "wps": "305.4", "ups": "0.43", "wpb": "716.6", "bsz": "57.9", "num_updates": "3600", "lr": "0.0003664", "gnorm": "33.547", "loss_scale": "128", "train_wall": "409", "gb_free": "8", "wall": "8806"}
[2025-02-03 01:13:19,843][train_inner][INFO] - {"epoch": 5, "update": 4.726, "loss": "53.486", "nll_loss": "3.409", "total": "716.855", "n_correct": "369.815", "ppl": "10.62", "accuracy": "51.589", "wps": "299.3", "ups": "0.42", "wpb": "716.9", "bsz": "59.1", "num_updates": "3800", "lr": "0.0003862", "gnorm": "32.056", "loss_scale": "128", "train_wall": "386", "gb_free": "8", "wall": "9286"}
[2025-02-03 01:20:50,497][train_inner][INFO] - {"epoch": 5, "update": 4.975, "loss": "57", "nll_loss": "3.477", "total": "709.54", "n_correct": "359.385", "ppl": "11.14", "accuracy": "50.65", "wps": "314.9", "ups": "0.44", "wpb": "709.5", "bsz": "55.6", "num_updates": "4000", "lr": "0.000406", "gnorm": "35.979", "loss_scale": "128", "train_wall": "438", "gb_free": "8", "wall": "9736"}
[2025-02-03 01:21:10,250][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2025-02-03 01:21:10,254][train][INFO] - {"epoch": 5, "train_loss": "56.285", "train_nll_loss": "3.523", "train_total": "711.575", "train_n_correct": "354.331", "train_ppl": "11.49", "train_accuracy": "49.795", "train_wps": "297.7", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "4020", "train_lr": "0.00040798", "train_gnorm": "34.892", "train_loss_scale": "128", "train_train_wall": "1651", "train_gb_free": "8", "train_wall": "9756"}
[2025-02-03 01:21:13,979][fairseq.trainer][INFO] - begin training epoch 6
[2025-02-03 01:21:13,980][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 01:29:27,960][train_inner][INFO] - {"epoch": 6, "update": 5.224, "loss": "53.578", "nll_loss": "3.252", "total": "705.11", "n_correct": "378.775", "ppl": "9.53", "accuracy": "53.719", "wps": "272.5", "ups": "0.39", "wpb": "705.1", "bsz": "56.3", "num_updates": "4200", "lr": "0.0004258", "gnorm": "30.581", "loss_scale": "256", "train_wall": "448", "gb_free": "8", "wall": "10254"}
[2025-02-03 01:36:24,695][train_inner][INFO] - {"epoch": 6, "update": 5.473, "loss": "53.019", "nll_loss": "3.146", "total": "708.415", "n_correct": "392.275", "ppl": "8.85", "accuracy": "55.374", "wps": "340", "ups": "0.48", "wpb": "708.4", "bsz": "55.9", "num_updates": "4400", "lr": "0.0004456", "gnorm": "29.426", "loss_scale": "256", "train_wall": "402", "gb_free": "8", "wall": "10670"}
[2025-02-03 01:44:18,905][train_inner][INFO] - {"epoch": 6, "update": 5.721, "loss": "51.228", "nll_loss": "3.076", "total": "724.805", "n_correct": "408.8", "ppl": "8.43", "accuracy": "56.401", "wps": "305.7", "ups": "0.42", "wpb": "724.8", "bsz": "58.3", "num_updates": "4600", "lr": "0.0004654", "gnorm": "28.098", "loss_scale": "256", "train_wall": "461", "gb_free": "8", "wall": "11145"}
[2025-02-03 01:51:54,550][train_inner][INFO] - {"epoch": 6, "update": 5.97, "loss": "49.779", "nll_loss": "2.995", "total": "706.895", "n_correct": "409.975", "ppl": "7.97", "accuracy": "57.997", "wps": "310.3", "ups": "0.44", "wpb": "706.9", "bsz": "57.5", "num_updates": "4800", "lr": "0.0004852", "gnorm": "24.337", "loss_scale": "256", "train_wall": "432", "gb_free": "8", "wall": "11600"}
[2025-02-03 01:52:21,963][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 01:53:59,387][valid][INFO] - {"epoch": 6, "valid_loss": "47.509", "valid_nll_loss": "2.65", "valid_total": "678.4", "valid_n_correct": "423.1", "valid_ppl": "6.27", "valid_accuracy": "62.367", "valid_wps": "329.6", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "4824", "valid_best_accuracy": "62.367"}
[2025-02-03 01:53:59,393][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 4824 updates
[2025-02-03 01:53:59,396][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 01:54:15,071][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 01:54:20,357][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 4824 updates, score 62.367) (writing took 20.963564870879054 seconds)
[2025-02-03 01:54:20,359][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2025-02-03 01:54:20,412][train][INFO] - {"epoch": 6, "train_loss": "51.788", "train_nll_loss": "3.108", "train_total": "711.575", "train_n_correct": "398.642", "train_ppl": "8.62", "train_accuracy": "56.022", "train_wps": "287.5", "train_ups": "0.4", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "4824", "train_lr": "0.000487576", "train_gnorm": "27.692", "train_loss_scale": "256", "train_train_wall": "1745", "train_gb_free": "8", "train_wall": "11746"}
[2025-02-03 01:54:22,776][fairseq.trainer][INFO] - begin training epoch 7
[2025-02-03 01:54:22,776][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 02:02:45,530][train_inner][INFO] - {"epoch": 7, "update": 6.219, "loss": "48.262", "nll_loss": "2.873", "total": "712.52", "n_correct": "423.2", "ppl": "7.33", "accuracy": "59.395", "wps": "218.9", "ups": "0.31", "wpb": "712.5", "bsz": "58.2", "num_updates": "5000", "lr": "0.000505", "gnorm": "24.559", "loss_scale": "256", "train_wall": "362", "gb_free": "8", "wall": "12251"}
[2025-02-03 02:10:49,694][train_inner][INFO] - {"epoch": 7, "update": 6.468, "loss": "48.223", "nll_loss": "2.809", "total": "709.3", "n_correct": "429.84", "ppl": "7.01", "accuracy": "60.601", "wps": "293", "ups": "0.41", "wpb": "709.3", "bsz": "57.2", "num_updates": "5200", "lr": "0.0005248", "gnorm": "22.707", "loss_scale": "256", "train_wall": "439", "gb_free": "8", "wall": "12735"}
[2025-02-03 02:18:24,704][train_inner][INFO] - {"epoch": 7, "update": 6.716, "loss": "48.442", "nll_loss": "2.824", "total": "721.945", "n_correct": "436.215", "ppl": "7.08", "accuracy": "60.422", "wps": "317.3", "ups": "0.44", "wpb": "721.9", "bsz": "58.1", "num_updates": "5400", "lr": "0.0005446", "gnorm": "21.753", "loss_scale": "256", "train_wall": "434", "gb_free": "8", "wall": "13191"}
[2025-02-03 02:26:01,493][train_inner][INFO] - {"epoch": 7, "update": 6.965, "loss": "49.215", "nll_loss": "2.771", "total": "705.155", "n_correct": "433.665", "ppl": "6.83", "accuracy": "61.499", "wps": "308.8", "ups": "0.44", "wpb": "705.2", "bsz": "55.2", "num_updates": "5600", "lr": "0.0005644", "gnorm": "20.457", "loss_scale": "256", "train_wall": "395", "gb_free": "8", "wall": "13647"}
[2025-02-03 02:26:27,794][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2025-02-03 02:26:27,802][train][INFO] - {"epoch": 7, "train_loss": "48.516", "train_nll_loss": "2.81", "train_total": "711.575", "train_n_correct": "431.313", "train_ppl": "7.01", "train_accuracy": "60.614", "train_wps": "296.8", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "5628", "train_lr": "0.000567172", "train_gnorm": "22.322", "train_loss_scale": "256", "train_train_wall": "1634", "train_gb_free": "8", "train_wall": "13674"}
[2025-02-03 02:26:31,767][fairseq.trainer][INFO] - begin training epoch 8
[2025-02-03 02:26:31,768][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 02:34:38,215][train_inner][INFO] - {"epoch": 8, "update": 7.214, "loss": "46.306", "nll_loss": "2.604", "total": "711.955", "n_correct": "453.07", "ppl": "6.08", "accuracy": "63.637", "wps": "275.6", "ups": "0.39", "wpb": "712", "bsz": "57", "num_updates": "5800", "lr": "0.0005842", "gnorm": "17.66", "loss_scale": "256", "train_wall": "400", "gb_free": "8", "wall": "14164"}
[2025-02-03 02:42:22,686][train_inner][INFO] - {"epoch": 8, "update": 7.463, "loss": "46.116", "nll_loss": "2.599", "total": "711.76", "n_correct": "453.265", "ppl": "6.06", "accuracy": "63.682", "wps": "306.5", "ups": "0.43", "wpb": "711.8", "bsz": "57.1", "num_updates": "6000", "lr": "0.000604", "gnorm": "17.716", "loss_scale": "256", "train_wall": "423", "gb_free": "8", "wall": "14628"}
[2025-02-03 02:49:59,468][train_inner][INFO] - {"epoch": 8, "update": 7.711, "loss": "47.075", "nll_loss": "2.608", "total": "709.175", "n_correct": "451.745", "ppl": "6.09", "accuracy": "63.7", "wps": "310.5", "ups": "0.44", "wpb": "709.2", "bsz": "55.9", "num_updates": "6200", "lr": "0.0006238", "gnorm": "17.039", "loss_scale": "256", "train_wall": "418", "gb_free": "8", "wall": "15085"}
[2025-02-03 02:57:34,094][train_inner][INFO] - {"epoch": 8, "update": 7.96, "loss": "45.595", "nll_loss": "2.572", "total": "709.4", "n_correct": "455.93", "ppl": "5.95", "accuracy": "64.27", "wps": "312.1", "ups": "0.44", "wpb": "709.4", "bsz": "57.2", "num_updates": "6400", "lr": "0.0006436", "gnorm": "15.965", "loss_scale": "256", "train_wall": "425", "gb_free": "8", "wall": "15540"}
[2025-02-03 02:58:20,921][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 02:59:59,129][valid][INFO] - {"epoch": 8, "valid_loss": "43.963", "valid_nll_loss": "2.324", "valid_total": "678.4", "valid_n_correct": "459.05", "valid_ppl": "5.01", "valid_accuracy": "67.667", "valid_wps": "334.5", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "6432", "valid_best_accuracy": "67.667"}
[2025-02-03 02:59:59,137][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 6432 updates
[2025-02-03 02:59:59,140][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 03:00:15,317][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 03:00:20,559][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 6432 updates, score 67.667) (writing took 21.421768113039434 seconds)
[2025-02-03 03:00:20,561][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2025-02-03 03:00:20,595][train][INFO] - {"epoch": 8, "train_loss": "46.044", "train_nll_loss": "2.585", "train_total": "711.575", "train_n_correct": "455.173", "train_ppl": "6", "train_accuracy": "63.967", "train_wps": "281.4", "train_ups": "0.4", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "6432", "train_lr": "0.000646768", "train_gnorm": "16.982", "train_loss_scale": "256", "train_train_wall": "1687", "train_gb_free": "8", "train_wall": "15706"}
[2025-02-03 03:00:22,604][fairseq.trainer][INFO] - begin training epoch 9
[2025-02-03 03:00:22,605][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 03:08:10,231][train_inner][INFO] - {"epoch": 9, "update": 8.209, "loss": "44.823", "nll_loss": "2.402", "total": "710.25", "n_correct": "472.855", "ppl": "5.29", "accuracy": "66.576", "wps": "223.3", "ups": "0.31", "wpb": "710.2", "bsz": "56", "num_updates": "6600", "lr": "0.0006634", "gnorm": "15.017", "loss_scale": "256", "train_wall": "392", "gb_free": "8", "wall": "16176"}
[2025-02-03 03:16:15,885][train_inner][INFO] - {"epoch": 9, "update": 8.458, "loss": "43.922", "nll_loss": "2.446", "total": "716.43", "n_correct": "473.835", "ppl": "5.45", "accuracy": "66.138", "wps": "295", "ups": "0.41", "wpb": "716.4", "bsz": "58.2", "num_updates": "6800", "lr": "0.0006832", "gnorm": "13.302", "loss_scale": "256", "train_wall": "448", "gb_free": "8", "wall": "16662"}
[2025-02-03 03:23:59,828][train_inner][INFO] - {"epoch": 9, "update": 8.706, "loss": "44.155", "nll_loss": "2.439", "total": "716.515", "n_correct": "474.4", "ppl": "5.42", "accuracy": "66.209", "wps": "308.9", "ups": "0.43", "wpb": "716.5", "bsz": "57.8", "num_updates": "7000", "lr": "0.000703", "gnorm": "13.119", "loss_scale": "256", "train_wall": "450", "gb_free": "8", "wall": "17126"}
[2025-02-03 03:31:39,293][train_inner][INFO] - {"epoch": 9, "update": 8.955, "loss": "43.459", "nll_loss": "2.4", "total": "710.195", "n_correct": "474.045", "ppl": "5.28", "accuracy": "66.749", "wps": "309.2", "ups": "0.44", "wpb": "710.2", "bsz": "57.6", "num_updates": "7200", "lr": "0.0007228", "gnorm": "12.589", "loss_scale": "256", "train_wall": "445", "gb_free": "8", "wall": "17585"}
[2025-02-03 03:32:32,121][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2025-02-03 03:32:32,128][train][INFO] - {"epoch": 9, "train_loss": "44.241", "train_nll_loss": "2.422", "train_total": "711.575", "train_n_correct": "472.682", "train_ppl": "5.36", "train_accuracy": "66.428", "train_wps": "296.2", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "7236", "train_lr": "0.000726364", "train_gnorm": "13.432", "train_loss_scale": "256", "train_train_wall": "1729", "train_gb_free": "8", "train_wall": "17638"}
[2025-02-03 03:32:36,345][fairseq.trainer][INFO] - begin training epoch 10
[2025-02-03 03:32:36,346][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 03:40:12,630][train_inner][INFO] - {"epoch": 10, "update": 9.204, "loss": "43.373", "nll_loss": "2.252", "total": "702.44", "n_correct": "482.535", "ppl": "4.76", "accuracy": "68.694", "wps": "273.7", "ups": "0.39", "wpb": "702.4", "bsz": "55", "num_updates": "7400", "lr": "0.0007426", "gnorm": "12.171", "loss_scale": "256", "train_wall": "413", "gb_free": "8", "wall": "18098"}
[2025-02-03 03:47:41,859][train_inner][INFO] - {"epoch": 10, "update": 9.453, "loss": "43.222", "nll_loss": "2.295", "total": "706.495", "n_correct": "482.545", "ppl": "4.91", "accuracy": "68.301", "wps": "314.5", "ups": "0.45", "wpb": "706.5", "bsz": "56.1", "num_updates": "7600", "lr": "0.0007624", "gnorm": "11.694", "loss_scale": "256", "train_wall": "437", "gb_free": "8", "wall": "18548"}
[2025-02-03 03:55:24,745][train_inner][INFO] - {"epoch": 10, "update": 9.701, "loss": "42.663", "nll_loss": "2.301", "total": "718.915", "n_correct": "490.775", "ppl": "4.93", "accuracy": "68.266", "wps": "310.6", "ups": "0.43", "wpb": "718.9", "bsz": "57.9", "num_updates": "7800", "lr": "0.0007822", "gnorm": "10.962", "loss_scale": "256", "train_wall": "438", "gb_free": "8", "wall": "19011"}
[2025-02-03 04:02:55,802][train_inner][INFO] - {"epoch": 10, "update": 9.95, "loss": "43.031", "nll_loss": "2.35", "total": "711.735", "n_correct": "482.5", "ppl": "5.1", "accuracy": "67.792", "wps": "315.6", "ups": "0.44", "wpb": "711.7", "bsz": "57.5", "num_updates": "8000", "lr": "0.000802", "gnorm": "10.695", "loss_scale": "256", "train_wall": "408", "gb_free": "8", "wall": "19462"}
[2025-02-03 04:03:57,345][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 04:05:35,983][valid][INFO] - {"epoch": 10, "valid_loss": "42.072", "valid_nll_loss": "2.177", "valid_total": "678.4", "valid_n_correct": "472.4", "valid_ppl": "4.52", "valid_accuracy": "69.634", "valid_wps": "281", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "8040", "valid_best_accuracy": "69.634"}
[2025-02-03 04:05:35,992][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 8040 updates
[2025-02-03 04:05:35,994][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 04:05:51,247][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 04:05:56,909][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 8040 updates, score 69.634) (writing took 20.91723646223545 seconds)
[2025-02-03 04:05:56,914][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2025-02-03 04:05:56,923][train][INFO] - {"epoch": 10, "train_loss": "42.787", "train_nll_loss": "2.291", "train_total": "711.575", "train_n_correct": "486.568", "train_ppl": "4.9", "train_accuracy": "68.379", "train_wps": "285.4", "train_ups": "0.4", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "8040", "train_lr": "0.00080596", "train_gnorm": "11.186", "train_loss_scale": "256", "train_train_wall": "1716", "train_gb_free": "8", "train_wall": "19643"}
[2025-02-03 04:05:59,316][fairseq.trainer][INFO] - begin training epoch 11
[2025-02-03 04:05:59,317][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 04:14:03,953][train_inner][INFO] - {"epoch": 11, "update": 10.199, "loss": "40.905", "nll_loss": "2.166", "total": "718.895", "n_correct": "503.615", "ppl": "4.49", "accuracy": "70.054", "wps": "215.2", "ups": "0.3", "wpb": "718.9", "bsz": "58.3", "num_updates": "8200", "lr": "0.0008218", "gnorm": "9.463", "loss_scale": "512", "train_wall": "366", "gb_free": "8", "wall": "20130"}
[2025-02-03 04:21:32,898][train_inner][INFO] - {"epoch": 11, "update": 10.448, "loss": "42.225", "nll_loss": "2.185", "total": "709.925", "n_correct": "495.215", "ppl": "4.55", "accuracy": "69.756", "wps": "316.3", "ups": "0.45", "wpb": "709.9", "bsz": "56.1", "num_updates": "8400", "lr": "0.0008416", "gnorm": "9.991", "loss_scale": "512", "train_wall": "396", "gb_free": "8", "wall": "20579"}
[2025-02-03 04:29:10,413][train_inner][INFO] - {"epoch": 11, "update": 10.697, "loss": "41.15", "nll_loss": "2.171", "total": "708.87", "n_correct": "497.165", "ppl": "4.5", "accuracy": "70.135", "wps": "309.9", "ups": "0.44", "wpb": "708.9", "bsz": "57.2", "num_updates": "8600", "lr": "0.0008614", "gnorm": "9", "loss_scale": "512", "train_wall": "437", "gb_free": "8", "wall": "21036"}
[2025-02-03 04:36:46,361][train_inner][INFO] - {"epoch": 11, "update": 10.945, "loss": "42.993", "nll_loss": "2.276", "total": "709.205", "n_correct": "486.42", "ppl": "4.84", "accuracy": "68.587", "wps": "311.1", "ups": "0.44", "wpb": "709.2", "bsz": "56.2", "num_updates": "8800", "lr": "0.0008812", "gnorm": "9.647", "loss_scale": "512", "train_wall": "400", "gb_free": "8", "wall": "21492"}
[2025-02-03 04:38:08,402][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2025-02-03 04:38:08,407][train][INFO] - {"epoch": 11, "train_loss": "41.72", "train_nll_loss": "2.195", "train_total": "711.575", "train_n_correct": "495.973", "train_ppl": "4.58", "train_accuracy": "69.701", "train_wps": "296.2", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "8844", "train_lr": "0.000885556", "train_gnorm": "9.459", "train_loss_scale": "512", "train_train_wall": "1610", "train_gb_free": "8", "train_wall": "21574"}
[2025-02-03 04:38:11,918][fairseq.trainer][INFO] - begin training epoch 12
[2025-02-03 04:38:11,919][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 04:46:01,291][train_inner][INFO] - {"epoch": 12, "update": 11.194, "loss": "39.462", "nll_loss": "2.101", "total": "723.54", "n_correct": "513.555", "ppl": "4.29", "accuracy": "70.978", "wps": "260.8", "ups": "0.36", "wpb": "723.5", "bsz": "59.8", "num_updates": "9000", "lr": "0.000901", "gnorm": "8.389", "loss_scale": "512", "train_wall": "465", "gb_free": "8", "wall": "22047"}
[2025-02-03 04:53:38,488][train_inner][INFO] - {"epoch": 12, "update": 11.443, "loss": "41.438", "nll_loss": "2.082", "total": "704.85", "n_correct": "502.215", "ppl": "4.23", "accuracy": "71.251", "wps": "308.4", "ups": "0.44", "wpb": "704.9", "bsz": "55.2", "num_updates": "9200", "lr": "0.0009208", "gnorm": "8.61", "loss_scale": "512", "train_wall": "413", "gb_free": "8", "wall": "22504"}
[2025-02-03 05:01:11,256][train_inner][INFO] - {"epoch": 12, "update": 11.692, "loss": "40.571", "nll_loss": "2.097", "total": "704.57", "n_correct": "502.05", "ppl": "4.28", "accuracy": "71.256", "wps": "311.2", "ups": "0.44", "wpb": "704.6", "bsz": "56.5", "num_updates": "9400", "lr": "0.0009406", "gnorm": "8.141", "loss_scale": "512", "train_wall": "403", "gb_free": "8", "wall": "22957"}
[2025-02-03 05:08:58,415][train_inner][INFO] - {"epoch": 12, "update": 11.94, "loss": "40.819", "nll_loss": "2.153", "total": "710.84", "n_correct": "500.33", "ppl": "4.45", "accuracy": "70.386", "wps": "304.4", "ups": "0.43", "wpb": "710.8", "bsz": "57.5", "num_updates": "9600", "lr": "0.0009604", "gnorm": "7.815", "loss_scale": "512", "train_wall": "436", "gb_free": "8", "wall": "23424"}
[2025-02-03 05:10:27,574][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 05:12:03,025][valid][INFO] - {"epoch": 12, "valid_loss": "40.55", "valid_nll_loss": "2.029", "valid_total": "678.4", "valid_n_correct": "488.55", "valid_ppl": "4.08", "valid_accuracy": "72.015", "valid_wps": "341.9", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "9648", "valid_best_accuracy": "72.015"}
[2025-02-03 05:12:03,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 9648 updates
[2025-02-03 05:12:03,032][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 05:12:17,111][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 05:12:22,574][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 9648 updates, score 72.015) (writing took 19.54300220310688 seconds)
[2025-02-03 05:12:22,575][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2025-02-03 05:12:22,587][train][INFO] - {"epoch": 12, "train_loss": "40.712", "train_nll_loss": "2.104", "train_total": "711.575", "train_n_correct": "505.34", "train_ppl": "4.3", "train_accuracy": "71.017", "train_wps": "278.5", "train_ups": "0.39", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "9648", "train_lr": "0.000965152", "train_gnorm": "8.253", "train_loss_scale": "512", "train_train_wall": "1734", "train_gb_free": "8", "train_wall": "23628"}
[2025-02-03 05:12:24,666][fairseq.trainer][INFO] - begin training epoch 13
[2025-02-03 05:12:24,668][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 05:19:41,478][train_inner][INFO] - {"epoch": 13, "update": 12.189, "loss": "41.445", "nll_loss": "2.027", "total": "716.6", "n_correct": "515.665", "ppl": "4.08", "accuracy": "71.96", "wps": "222.9", "ups": "0.31", "wpb": "716.6", "bsz": "55.3", "num_updates": "9800", "lr": "0.0009802", "gnorm": "8.071", "loss_scale": "512", "train_wall": "416", "gb_free": "8", "wall": "24067"}
[2025-02-03 05:27:06,741][train_inner][INFO] - {"epoch": 13, "update": 12.438, "loss": "40.011", "nll_loss": "2.031", "total": "704.68", "n_correct": "507.815", "ppl": "4.09", "accuracy": "72.063", "wps": "316.5", "ups": "0.45", "wpb": "704.7", "bsz": "56.3", "num_updates": "10000", "lr": "0.001", "gnorm": "7.508", "loss_scale": "512", "train_wall": "433", "gb_free": "8", "wall": "24513"}
[2025-02-03 05:35:08,048][train_inner][INFO] - {"epoch": 13, "update": 12.687, "loss": "39.06", "nll_loss": "2.021", "total": "710.48", "n_correct": "512.555", "ppl": "4.06", "accuracy": "72.142", "wps": "295.3", "ups": "0.42", "wpb": "710.5", "bsz": "58", "num_updates": "10200", "lr": "0.000970487", "gnorm": "7.206", "loss_scale": "512", "train_wall": "465", "gb_free": "8", "wall": "24994"}
[2025-02-03 05:42:46,836][train_inner][INFO] - {"epoch": 13, "update": 12.935, "loss": "39.523", "nll_loss": "2.04", "total": "716.05", "n_correct": "514.76", "ppl": "4.11", "accuracy": "71.889", "wps": "312.2", "ups": "0.44", "wpb": "716.1", "bsz": "58", "num_updates": "10400", "lr": "0.000941845", "gnorm": "7.074", "loss_scale": "512", "train_wall": "440", "gb_free": "8", "wall": "25453"}
[2025-02-03 05:44:18,775][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2025-02-03 05:44:18,779][train][INFO] - {"epoch": 13, "train_loss": "39.791", "train_nll_loss": "2.021", "train_total": "711.575", "train_n_correct": "513.31", "train_ppl": "4.06", "train_accuracy": "72.137", "train_wps": "298.6", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "10452", "train_lr": "0.000934537", "train_gnorm": "7.357", "train_loss_scale": "512", "train_train_wall": "1752", "train_gb_free": "8", "train_wall": "25545"}
[2025-02-03 05:44:21,973][fairseq.trainer][INFO] - begin training epoch 14
[2025-02-03 05:44:21,974][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 05:51:40,398][train_inner][INFO] - {"epoch": 14, "update": 13.184, "loss": "39.125", "nll_loss": "1.911", "total": "717.52", "n_correct": "527.75", "ppl": "3.76", "accuracy": "73.552", "wps": "269", "ups": "0.37", "wpb": "717.5", "bsz": "56.7", "num_updates": "10600", "lr": "0.000914048", "gnorm": "6.922", "loss_scale": "512", "train_wall": "403", "gb_free": "8", "wall": "25986"}
[2025-02-03 05:59:05,980][train_inner][INFO] - {"epoch": 14, "update": 13.433, "loss": "38.581", "nll_loss": "1.881", "total": "705.195", "n_correct": "522.27", "ppl": "3.68", "accuracy": "74.06", "wps": "316.5", "ups": "0.45", "wpb": "705.2", "bsz": "56.1", "num_updates": "10800", "lr": "0.000887072", "gnorm": "6.666", "loss_scale": "512", "train_wall": "428", "gb_free": "8", "wall": "26432"}
[2025-02-03 06:06:43,748][train_inner][INFO] - {"epoch": 14, "update": 13.682, "loss": "39.061", "nll_loss": "1.89", "total": "712.295", "n_correct": "527.91", "ppl": "3.71", "accuracy": "74.114", "wps": "311.2", "ups": "0.44", "wpb": "712.3", "bsz": "56", "num_updates": "11000", "lr": "0.000860892", "gnorm": "6.765", "loss_scale": "512", "train_wall": "444", "gb_free": "8", "wall": "26890"}
[2025-02-03 06:14:37,138][train_inner][INFO] - {"epoch": 14, "update": 13.93, "loss": "37.567", "nll_loss": "1.897", "total": "715.295", "n_correct": "529.3", "ppl": "3.72", "accuracy": "73.997", "wps": "302.2", "ups": "0.42", "wpb": "715.3", "bsz": "58.6", "num_updates": "11200", "lr": "0.000835484", "gnorm": "6.135", "loss_scale": "512", "train_wall": "455", "gb_free": "8", "wall": "27363"}
[2025-02-03 06:16:21,487][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 06:17:58,552][valid][INFO] - {"epoch": 14, "valid_loss": "38.841", "valid_nll_loss": "1.892", "valid_total": "678.4", "valid_n_correct": "504.35", "valid_ppl": "3.71", "valid_accuracy": "74.344", "valid_wps": "366.2", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "11256", "valid_best_accuracy": "74.344"}
[2025-02-03 06:17:58,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 11256 updates
[2025-02-03 06:17:58,563][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 06:18:13,338][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 06:18:18,808][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 11256 updates, score 74.344) (writing took 20.24762579612434 seconds)
[2025-02-03 06:18:18,811][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2025-02-03 06:18:18,822][train][INFO] - {"epoch": 14, "train_loss": "38.305", "train_nll_loss": "1.886", "train_total": "711.575", "train_n_correct": "527.088", "train_ppl": "3.7", "train_accuracy": "74.074", "train_wps": "280.4", "train_ups": "0.39", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "11256", "train_lr": "0.000828505", "train_gnorm": "6.543", "train_loss_scale": "512", "train_train_wall": "1745", "train_gb_free": "8", "train_wall": "27585"}
[2025-02-03 06:18:20,838][fairseq.trainer][INFO] - begin training epoch 15
[2025-02-03 06:18:20,838][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 06:25:03,079][train_inner][INFO] - {"epoch": 15, "update": 14.179, "loss": "36.171", "nll_loss": "1.761", "total": "706.175", "n_correct": "535.45", "ppl": "3.39", "accuracy": "75.824", "wps": "225.7", "ups": "0.32", "wpb": "706.2", "bsz": "57.8", "num_updates": "11400", "lr": "0.000810826", "gnorm": "6.114", "loss_scale": "512", "train_wall": "404", "gb_free": "8", "wall": "27989"}
[2025-02-03 06:32:42,082][train_inner][INFO] - {"epoch": 15, "update": 14.428, "loss": "37.08", "nll_loss": "1.768", "total": "710.84", "n_correct": "538.055", "ppl": "3.41", "accuracy": "75.693", "wps": "309.7", "ups": "0.44", "wpb": "710.8", "bsz": "56.9", "num_updates": "11600", "lr": "0.000786896", "gnorm": "6.314", "loss_scale": "512", "train_wall": "445", "gb_free": "8", "wall": "28448"}
[2025-02-03 06:40:26,695][train_inner][INFO] - {"epoch": 15, "update": 14.677, "loss": "36.747", "nll_loss": "1.781", "total": "715.885", "n_correct": "541.035", "ppl": "3.44", "accuracy": "75.576", "wps": "308.2", "ups": "0.43", "wpb": "715.9", "bsz": "58", "num_updates": "11800", "lr": "0.000763673", "gnorm": "5.984", "loss_scale": "512", "train_wall": "450", "gb_free": "8", "wall": "28912"}
[2025-02-03 06:48:17,949][train_inner][INFO] - {"epoch": 15, "update": 14.925, "loss": "37.039", "nll_loss": "1.755", "total": "718.14", "n_correct": "545.69", "ppl": "3.37", "accuracy": "75.987", "wps": "304.8", "ups": "0.42", "wpb": "718.1", "bsz": "57.2", "num_updates": "12000", "lr": "0.000741134", "gnorm": "6.07", "loss_scale": "512", "train_wall": "456", "gb_free": "8", "wall": "29384"}
[2025-02-03 06:50:03,797][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2025-02-03 06:50:03,802][train][INFO] - {"epoch": 15, "train_loss": "36.94", "train_nll_loss": "1.762", "train_total": "711.575", "train_n_correct": "539.653", "train_ppl": "3.39", "train_accuracy": "75.839", "train_wps": "300.3", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "12060", "train_lr": "0.000734504", "train_gnorm": "6.138", "train_loss_scale": "512", "train_train_wall": "1757", "train_gb_free": "8", "train_wall": "29490"}
[2025-02-03 06:50:07,564][fairseq.trainer][INFO] - begin training epoch 16
[2025-02-03 06:50:07,564][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 06:56:53,922][train_inner][INFO] - {"epoch": 16, "update": 15.174, "loss": "36.166", "nll_loss": "1.671", "total": "700.865", "n_correct": "539.83", "ppl": "3.19", "accuracy": "77.023", "wps": "271.7", "ups": "0.39", "wpb": "700.9", "bsz": "55.8", "num_updates": "12200", "lr": "0.000719261", "gnorm": "5.974", "loss_scale": "512", "train_wall": "420", "gb_free": "8", "wall": "29900"}
[2025-02-03 07:04:46,516][train_inner][INFO] - {"epoch": 16, "update": 15.423, "loss": "35.272", "nll_loss": "1.617", "total": "717.39", "n_correct": "558.425", "ppl": "3.07", "accuracy": "77.841", "wps": "303.6", "ups": "0.42", "wpb": "717.4", "bsz": "57.6", "num_updates": "12400", "lr": "0.000698034", "gnorm": "5.913", "loss_scale": "1024", "train_wall": "408", "gb_free": "8", "wall": "30372"}
[2025-02-03 07:12:34,155][train_inner][INFO] - {"epoch": 16, "update": 15.672, "loss": "35.166", "nll_loss": "1.628", "total": "706.175", "n_correct": "548.71", "ppl": "3.09", "accuracy": "77.702", "wps": "302", "ups": "0.43", "wpb": "706.2", "bsz": "57.1", "num_updates": "12600", "lr": "0.000677433", "gnorm": "5.801", "loss_scale": "1024", "train_wall": "431", "gb_free": "8", "wall": "30840"}
[2025-02-03 07:20:19,351][train_inner][INFO] - {"epoch": 16, "update": 15.92, "loss": "36.404", "nll_loss": "1.697", "total": "712.38", "n_correct": "547.21", "ppl": "3.24", "accuracy": "76.814", "wps": "306.3", "ups": "0.43", "wpb": "712.4", "bsz": "56.8", "num_updates": "12800", "lr": "0.00065744", "gnorm": "5.88", "loss_scale": "1024", "train_wall": "428", "gb_free": "8", "wall": "31305"}
[2025-02-03 07:22:13,423][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 07:23:50,406][valid][INFO] - {"epoch": 16, "valid_loss": "38.085", "valid_nll_loss": "1.814", "valid_total": "678.4", "valid_n_correct": "514.15", "valid_ppl": "3.52", "valid_accuracy": "75.789", "valid_wps": "347.6", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "12864", "valid_best_accuracy": "75.789"}
[2025-02-03 07:23:50,414][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 12864 updates
[2025-02-03 07:23:50,417][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 07:24:05,250][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 07:24:09,968][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 12864 updates, score 75.789) (writing took 19.55374812707305 seconds)
[2025-02-03 07:24:09,971][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2025-02-03 07:24:09,982][train][INFO] - {"epoch": 16, "train_loss": "35.641", "train_nll_loss": "1.643", "train_total": "711.575", "train_n_correct": "551.3", "train_ppl": "3.12", "train_accuracy": "77.476", "train_wps": "279.6", "train_ups": "0.39", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "12864", "train_lr": "0.000651167", "train_gnorm": "5.891", "train_loss_scale": "1024", "train_train_wall": "1694", "train_gb_free": "8", "train_wall": "31536"}
[2025-02-03 07:24:11,889][fairseq.trainer][INFO] - begin training epoch 17
[2025-02-03 07:24:11,889][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 07:30:48,870][train_inner][INFO] - {"epoch": 17, "update": 16.169, "loss": "35.134", "nll_loss": "1.557", "total": "704.505", "n_correct": "553.51", "ppl": "2.94", "accuracy": "78.567", "wps": "223.8", "ups": "0.32", "wpb": "704.5", "bsz": "55.8", "num_updates": "13000", "lr": "0.000638036", "gnorm": "6.1", "loss_scale": "1024", "train_wall": "379", "gb_free": "8", "wall": "31935"}
[2025-02-03 07:38:25,301][train_inner][INFO] - {"epoch": 17, "update": 16.418, "loss": "34.416", "nll_loss": "1.538", "total": "714.035", "n_correct": "563.4", "ppl": "2.9", "accuracy": "78.904", "wps": "312.9", "ups": "0.44", "wpb": "714", "bsz": "57.3", "num_updates": "13200", "lr": "0.000619206", "gnorm": "5.756", "loss_scale": "1024", "train_wall": "441", "gb_free": "8", "wall": "32391"}
[2025-02-03 07:46:25,477][train_inner][INFO] - {"epoch": 17, "update": 16.667, "loss": "34.523", "nll_loss": "1.541", "total": "708.19", "n_correct": "558.84", "ppl": "2.91", "accuracy": "78.911", "wps": "295", "ups": "0.42", "wpb": "708.2", "bsz": "56.7", "num_updates": "13400", "lr": "0.000600931", "gnorm": "5.809", "loss_scale": "1024", "train_wall": "466", "gb_free": "8", "wall": "32871"}
[2025-02-03 07:53:57,930][train_inner][INFO] - {"epoch": 17, "update": 16.915, "loss": "34.963", "nll_loss": "1.554", "total": "720.945", "n_correct": "567.435", "ppl": "2.94", "accuracy": "78.707", "wps": "318.7", "ups": "0.44", "wpb": "720.9", "bsz": "57.3", "num_updates": "13600", "lr": "0.000583196", "gnorm": "5.929", "loss_scale": "1024", "train_wall": "440", "gb_free": "8", "wall": "33324"}
[2025-02-03 07:56:09,611][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2025-02-03 07:56:09,617][train][INFO] - {"epoch": 17, "train_loss": "34.498", "train_nll_loss": "1.538", "train_total": "711.575", "train_n_correct": "561.376", "train_ppl": "2.9", "train_accuracy": "78.892", "train_wps": "298", "train_ups": "0.42", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "13668", "train_lr": "0.000577286", "train_gnorm": "5.836", "train_loss_scale": "1024", "train_train_wall": "1743", "train_gb_free": "8", "train_wall": "33455"}
[2025-02-03 07:56:13,190][fairseq.trainer][INFO] - begin training epoch 18
[2025-02-03 07:56:13,190][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 08:01:40,924][train_inner][INFO] - {"epoch": 18, "update": 17.164, "loss": "33.052", "nll_loss": "1.441", "total": "712.27", "n_correct": "570.16", "ppl": "2.71", "accuracy": "80.048", "wps": "307.7", "ups": "0.43", "wpb": "712.3", "bsz": "57.8", "num_updates": "13800", "lr": "0.000565984", "gnorm": "5.845", "loss_scale": "1024", "train_wall": "389", "gb_free": "8", "wall": "33787"}
[2025-02-03 08:06:36,904][train_inner][INFO] - {"epoch": 18, "update": 17.413, "loss": "34.269", "nll_loss": "1.422", "total": "709.1", "n_correct": "570.475", "ppl": "2.68", "accuracy": "80.451", "wps": "479.2", "ups": "0.68", "wpb": "709.1", "bsz": "55.1", "num_updates": "14000", "lr": "0.00054928", "gnorm": "6.12", "loss_scale": "1024", "train_wall": "284", "gb_free": "8", "wall": "34083"}
[2025-02-03 08:11:48,606][train_inner][INFO] - {"epoch": 18, "update": 17.662, "loss": "33.161", "nll_loss": "1.427", "total": "709.775", "n_correct": "570.785", "ppl": "2.69", "accuracy": "80.418", "wps": "455.4", "ups": "0.64", "wpb": "709.8", "bsz": "57.1", "num_updates": "14200", "lr": "0.000533069", "gnorm": "5.838", "loss_scale": "1024", "train_wall": "301", "gb_free": "8", "wall": "34394"}
[2025-02-03 08:16:59,259][train_inner][INFO] - {"epoch": 18, "update": 17.91, "loss": "33.355", "nll_loss": "1.489", "total": "713.385", "n_correct": "566.885", "ppl": "2.81", "accuracy": "79.464", "wps": "459.4", "ups": "0.64", "wpb": "713.4", "bsz": "58.2", "num_updates": "14400", "lr": "0.000517337", "gnorm": "5.738", "loss_scale": "1024", "train_wall": "300", "gb_free": "8", "wall": "34705"}
[2025-02-03 08:18:29,824][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 08:19:21,156][valid][INFO] - {"epoch": 18, "valid_loss": "37.803", "valid_nll_loss": "1.794", "valid_total": "678.4", "valid_n_correct": "518.8", "valid_ppl": "3.47", "valid_accuracy": "76.474", "valid_wps": "509.1", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "14472", "valid_best_accuracy": "76.474"}
[2025-02-03 08:19:21,159][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 14472 updates
[2025-02-03 08:19:21,160][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 08:19:31,260][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 08:19:36,749][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 14472 updates, score 76.474) (writing took 15.589908461086452 seconds)
[2025-02-03 08:19:36,752][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2025-02-03 08:19:36,758][train][INFO] - {"epoch": 18, "train_loss": "33.456", "train_nll_loss": "1.442", "train_total": "711.575", "train_n_correct": "570.198", "train_ppl": "2.72", "train_accuracy": "80.132", "train_wps": "406.6", "train_ups": "0.57", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "14472", "train_lr": "0.000511788", "train_gnorm": "5.908", "train_loss_scale": "1024", "train_train_wall": "1236", "train_gb_free": "8", "train_wall": "34863"}
[2025-02-03 08:19:37,886][fairseq.trainer][INFO] - begin training epoch 19
[2025-02-03 08:19:37,887][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 08:23:42,395][train_inner][INFO] - {"epoch": 19, "update": 18.159, "loss": "33.14", "nll_loss": "1.351", "total": "710.83", "n_correct": "578.825", "ppl": "2.55", "accuracy": "81.429", "wps": "352.7", "ups": "0.5", "wpb": "710.8", "bsz": "55.8", "num_updates": "14600", "lr": "0.000502069", "gnorm": "5.985", "loss_scale": "1024", "train_wall": "269", "gb_free": "8", "wall": "35108"}
[2025-02-03 08:23:49,003][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
[2025-02-03 08:28:46,178][train_inner][INFO] - {"epoch": 19, "update": 18.409, "loss": "32.282", "nll_loss": "1.356", "total": "713.53", "n_correct": "579.25", "ppl": "2.56", "accuracy": "81.181", "wps": "469.8", "ups": "0.66", "wpb": "713.5", "bsz": "57.6", "num_updates": "14800", "lr": "0.000487251", "gnorm": "6.048", "loss_scale": "512", "train_wall": "296", "gb_free": "8", "wall": "35412"}
[2025-02-03 08:33:57,755][train_inner][INFO] - {"epoch": 19, "update": 18.658, "loss": "31.84", "nll_loss": "1.36", "total": "713.41", "n_correct": "579.505", "ppl": "2.57", "accuracy": "81.23", "wps": "458", "ups": "0.64", "wpb": "713.4", "bsz": "58.4", "num_updates": "15000", "lr": "0.000472871", "gnorm": "5.981", "loss_scale": "512", "train_wall": "301", "gb_free": "8", "wall": "35724"}
[2025-02-03 08:39:04,324][train_inner][INFO] - {"epoch": 19, "update": 18.907, "loss": "32.906", "nll_loss": "1.374", "total": "714.255", "n_correct": "578.855", "ppl": "2.59", "accuracy": "81.043", "wps": "466.1", "ups": "0.65", "wpb": "714.3", "bsz": "56.9", "num_updates": "15200", "lr": "0.000458915", "gnorm": "6.075", "loss_scale": "512", "train_wall": "294", "gb_free": "8", "wall": "36030"}
[2025-02-03 08:40:35,997][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2025-02-03 08:40:36,002][train][INFO] - {"epoch": 19, "train_loss": "32.474", "train_nll_loss": "1.352", "train_total": "711.518", "train_n_correct": "578.59", "train_ppl": "2.55", "train_accuracy": "81.318", "train_wps": "453.7", "train_ups": "0.64", "train_wpb": "711.5", "train_bsz": "57", "train_num_updates": "15275", "train_lr": "0.000453788", "train_gnorm": "6.086", "train_loss_scale": "512", "train_train_wall": "1159", "train_gb_free": "8", "train_wall": "36122"}
[2025-02-03 08:40:38,038][fairseq.trainer][INFO] - begin training epoch 20
[2025-02-03 08:40:38,043][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 08:44:36,848][train_inner][INFO] - {"epoch": 20, "update": 19.155, "loss": "32.281", "nll_loss": "1.316", "total": "701.455", "n_correct": "572.725", "ppl": "2.49", "accuracy": "81.648", "wps": "421.9", "ups": "0.6", "wpb": "701.5", "bsz": "55.9", "num_updates": "15400", "lr": "0.000445371", "gnorm": "6.54", "loss_scale": "512", "train_wall": "293", "gb_free": "8", "wall": "36363"}
[2025-02-03 08:49:48,610][train_inner][INFO] - {"epoch": 20, "update": 19.404, "loss": "30.16", "nll_loss": "1.243", "total": "714.4", "n_correct": "590.405", "ppl": "2.37", "accuracy": "82.643", "wps": "458.3", "ups": "0.64", "wpb": "714.4", "bsz": "59.4", "num_updates": "15600", "lr": "0.000432227", "gnorm": "5.927", "loss_scale": "512", "train_wall": "303", "gb_free": "8", "wall": "36674"}
[2025-02-03 08:54:57,763][train_inner][INFO] - {"epoch": 20, "update": 19.653, "loss": "31.03", "nll_loss": "1.24", "total": "706.44", "n_correct": "585.38", "ppl": "2.36", "accuracy": "82.863", "wps": "457.1", "ups": "0.65", "wpb": "706.4", "bsz": "57", "num_updates": "15800", "lr": "0.00041947", "gnorm": "6.145", "loss_scale": "512", "train_wall": "301", "gb_free": "8", "wall": "36984"}
[2025-02-03 08:59:59,130][train_inner][INFO] - {"epoch": 20, "update": 19.902, "loss": "33.061", "nll_loss": "1.304", "total": "716.68", "n_correct": "587.485", "ppl": "2.47", "accuracy": "81.973", "wps": "475.6", "ups": "0.66", "wpb": "716.7", "bsz": "55.5", "num_updates": "16000", "lr": "0.000407091", "gnorm": "6.622", "loss_scale": "512", "train_wall": "292", "gb_free": "8", "wall": "37285"}
[2025-02-03 09:01:38,546][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 09:02:31,460][valid][INFO] - {"epoch": 20, "valid_loss": "38.018", "valid_nll_loss": "1.81", "valid_total": "678.4", "valid_n_correct": "522.1", "valid_ppl": "3.51", "valid_accuracy": "76.96", "valid_wps": "616.9", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "16079", "valid_best_accuracy": "76.96"}
[2025-02-03 09:02:31,463][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 16079 updates
[2025-02-03 09:02:31,463][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 09:02:40,777][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 09:02:46,045][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 16079 updates, score 76.96) (writing took 14.582017662934959 seconds)
[2025-02-03 09:02:46,045][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2025-02-03 09:02:46,050][train][INFO] - {"epoch": 20, "train_loss": "31.484", "train_nll_loss": "1.26", "train_total": "711.575", "train_n_correct": "587.211", "train_ppl": "2.39", "train_accuracy": "82.523", "train_wps": "430.1", "train_ups": "0.6", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "16079", "train_lr": "0.000402302", "train_gnorm": "6.307", "train_loss_scale": "512", "train_train_wall": "1198", "train_gb_free": "8", "train_wall": "37452"}
[2025-02-03 09:02:47,070][fairseq.trainer][INFO] - begin training epoch 21
[2025-02-03 09:02:47,071][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 09:06:34,757][train_inner][INFO] - {"epoch": 21, "update": 20.15, "loss": "31.024", "nll_loss": "1.171", "total": "703.575", "n_correct": "589.945", "ppl": "2.25", "accuracy": "83.85", "wps": "355.7", "ups": "0.51", "wpb": "703.6", "bsz": "55.5", "num_updates": "16200", "lr": "0.000395076", "gnorm": "6.421", "loss_scale": "512", "train_wall": "269", "gb_free": "8", "wall": "37681"}
[2025-02-03 09:11:39,260][train_inner][INFO] - {"epoch": 21, "update": 20.399, "loss": "30.461", "nll_loss": "1.17", "total": "708.505", "n_correct": "593.085", "ppl": "2.25", "accuracy": "83.709", "wps": "465.4", "ups": "0.66", "wpb": "708.5", "bsz": "56.9", "num_updates": "16400", "lr": "0.000383416", "gnorm": "6.407", "loss_scale": "512", "train_wall": "294", "gb_free": "8", "wall": "37985"}
[2025-02-03 09:16:40,113][train_inner][INFO] - {"epoch": 21, "update": 20.648, "loss": "31.03", "nll_loss": "1.168", "total": "715.55", "n_correct": "600.055", "ppl": "2.25", "accuracy": "83.859", "wps": "475.7", "ups": "0.66", "wpb": "715.6", "bsz": "56.3", "num_updates": "16600", "lr": "0.0003721", "gnorm": "6.485", "loss_scale": "512", "train_wall": "292", "gb_free": "8", "wall": "38286"}
[2025-02-03 09:21:57,569][train_inner][INFO] - {"epoch": 21, "update": 20.897, "loss": "30.376", "nll_loss": "1.218", "total": "720.81", "n_correct": "599.765", "ppl": "2.33", "accuracy": "83.207", "wps": "454.1", "ups": "0.63", "wpb": "720.8", "bsz": "58.9", "num_updates": "16800", "lr": "0.000361119", "gnorm": "6.304", "loss_scale": "512", "train_wall": "308", "gb_free": "8", "wall": "38603"}
[2025-02-03 09:23:45,093][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2025-02-03 09:23:45,096][train][INFO] - {"epoch": 21, "train_loss": "30.538", "train_nll_loss": "1.173", "train_total": "711.575", "train_n_correct": "595.947", "train_ppl": "2.25", "train_accuracy": "83.75", "train_wps": "454.4", "train_ups": "0.64", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "16883", "train_lr": "0.000356657", "train_gnorm": "6.383", "train_loss_scale": "512", "train_train_wall": "1168", "train_gb_free": "8", "train_wall": "38711"}
[2025-02-03 09:23:47,127][fairseq.trainer][INFO] - begin training epoch 22
[2025-02-03 09:23:47,128][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 09:27:33,668][train_inner][INFO] - {"epoch": 22, "update": 21.146, "loss": "28.907", "nll_loss": "1.073", "total": "713.345", "n_correct": "608.16", "ppl": "2.1", "accuracy": "85.255", "wps": "424.5", "ups": "0.6", "wpb": "713.3", "bsz": "58.2", "num_updates": "17000", "lr": "0.000350461", "gnorm": "6.008", "loss_scale": "512", "train_wall": "298", "gb_free": "8", "wall": "38939"}
[2025-02-03 09:32:43,036][train_inner][INFO] - {"epoch": 22, "update": 21.394, "loss": "29.792", "nll_loss": "1.107", "total": "718.14", "n_correct": "607.695", "ppl": "2.15", "accuracy": "84.621", "wps": "464.3", "ups": "0.65", "wpb": "718.1", "bsz": "57.6", "num_updates": "17200", "lr": "0.000340118", "gnorm": "6.613", "loss_scale": "512", "train_wall": "300", "gb_free": "8", "wall": "39249"}
[2025-02-03 09:37:44,378][train_inner][INFO] - {"epoch": 22, "update": 21.643, "loss": "29.441", "nll_loss": "1.085", "total": "705.28", "n_correct": "599.45", "ppl": "2.12", "accuracy": "84.995", "wps": "468.2", "ups": "0.66", "wpb": "705.3", "bsz": "56.8", "num_updates": "17400", "lr": "0.00033008", "gnorm": "6.399", "loss_scale": "512", "train_wall": "292", "gb_free": "8", "wall": "39550"}
[2025-02-03 09:42:54,645][train_inner][INFO] - {"epoch": 22, "update": 21.892, "loss": "30.9", "nll_loss": "1.147", "total": "711.6", "n_correct": "597.49", "ppl": "2.21", "accuracy": "83.964", "wps": "458.8", "ups": "0.64", "wpb": "711.6", "bsz": "55.9", "num_updates": "17600", "lr": "0.000320338", "gnorm": "6.921", "loss_scale": "512", "train_wall": "300", "gb_free": "8", "wall": "39860"}
[2025-02-03 09:44:46,555][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 09:45:39,142][valid][INFO] - {"epoch": 22, "valid_loss": "38.478", "valid_nll_loss": "1.847", "valid_total": "678.4", "valid_n_correct": "522.6", "valid_ppl": "3.6", "valid_accuracy": "77.034", "valid_wps": "484.5", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "17687", "valid_best_accuracy": "77.034"}
[2025-02-03 09:45:39,145][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 17687 updates
[2025-02-03 09:45:39,146][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 09:45:48,586][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 09:45:54,104][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 17687 updates, score 77.034) (writing took 14.95917103625834 seconds)
[2025-02-03 09:45:54,106][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2025-02-03 09:45:54,113][train][INFO] - {"epoch": 22, "train_loss": "29.765", "train_nll_loss": "1.101", "train_total": "711.575", "train_n_correct": "602.892", "train_ppl": "2.15", "train_accuracy": "84.726", "train_wps": "430.5", "train_ups": "0.6", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "17687", "train_lr": "0.000316191", "train_gnorm": "6.528", "train_loss_scale": "512", "train_train_wall": "1198", "train_gb_free": "8", "train_wall": "40040"}
[2025-02-03 09:45:55,164][fairseq.trainer][INFO] - begin training epoch 23
[2025-02-03 09:45:55,164][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 09:49:37,350][train_inner][INFO] - {"epoch": 23, "update": 22.141, "loss": "29.034", "nll_loss": "1.036", "total": "702.625", "n_correct": "602.37", "ppl": "2.05", "accuracy": "85.731", "wps": "349", "ups": "0.5", "wpb": "702.6", "bsz": "56.3", "num_updates": "17800", "lr": "0.000310884", "gnorm": "6.419", "loss_scale": "512", "train_wall": "268", "gb_free": "8", "wall": "40263"}
[2025-02-03 09:54:44,221][train_inner][INFO] - {"epoch": 23, "update": 22.389, "loss": "28.749", "nll_loss": "1.004", "total": "711.58", "n_correct": "614.115", "ppl": "2.01", "accuracy": "86.303", "wps": "463.8", "ups": "0.65", "wpb": "711.6", "bsz": "56.9", "num_updates": "18000", "lr": "0.000301709", "gnorm": "6.412", "loss_scale": "512", "train_wall": "277", "gb_free": "8", "wall": "40570"}
[2025-02-03 09:59:58,650][train_inner][INFO] - {"epoch": 23, "update": 22.638, "loss": "28.849", "nll_loss": "1.042", "total": "721.21", "n_correct": "617.9", "ppl": "2.06", "accuracy": "85.675", "wps": "458.8", "ups": "0.64", "wpb": "721.2", "bsz": "58.3", "num_updates": "18200", "lr": "0.000292804", "gnorm": "6.477", "loss_scale": "512", "train_wall": "282", "gb_free": "8", "wall": "40884"}
[2025-02-03 10:05:07,588][train_inner][INFO] - {"epoch": 23, "update": 22.887, "loss": "29.293", "nll_loss": "1.039", "total": "711.085", "n_correct": "609.63", "ppl": "2.06", "accuracy": "85.732", "wps": "460.4", "ups": "0.65", "wpb": "711.1", "bsz": "56.6", "num_updates": "18400", "lr": "0.000284163", "gnorm": "6.578", "loss_scale": "512", "train_wall": "300", "gb_free": "8", "wall": "41193"}
[2025-02-03 10:07:07,208][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2025-02-03 10:07:07,212][train][INFO] - {"epoch": 23, "train_loss": "28.943", "train_nll_loss": "1.026", "train_total": "711.575", "train_n_correct": "611.541", "train_ppl": "2.04", "train_accuracy": "85.942", "train_wps": "449.4", "train_ups": "0.63", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "18491", "train_lr": "0.000280316", "train_gnorm": "6.504", "train_loss_scale": "512", "train_train_wall": "1133", "train_gb_free": "8", "train_wall": "41313"}
[2025-02-03 10:07:09,289][fairseq.trainer][INFO] - begin training epoch 24
[2025-02-03 10:07:09,307][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 10:10:43,434][train_inner][INFO] - {"epoch": 24, "update": 23.136, "loss": "28.62", "nll_loss": "0.998", "total": "707.555", "n_correct": "611.335", "ppl": "2", "accuracy": "86.401", "wps": "421.4", "ups": "0.6", "wpb": "707.6", "bsz": "56.7", "num_updates": "18600", "lr": "0.000275776", "gnorm": "6.614", "loss_scale": "512", "train_wall": "293", "gb_free": "8", "wall": "41529"}
[2025-02-03 10:16:58,390][train_inner][INFO] - {"epoch": 24, "update": 23.384, "loss": "27.837", "nll_loss": "0.943", "total": "716.61", "n_correct": "625.03", "ppl": "1.92", "accuracy": "87.22", "wps": "382.2", "ups": "0.53", "wpb": "716.6", "bsz": "57.9", "num_updates": "18800", "lr": "0.000267637", "gnorm": "6.475", "loss_scale": "1024", "train_wall": "357", "gb_free": "8", "wall": "41904"}
[2025-02-03 10:21:53,628][train_inner][INFO] - {"epoch": 24, "update": 23.633, "loss": "28.874", "nll_loss": "0.974", "total": "706.315", "n_correct": "611.99", "ppl": "1.96", "accuracy": "86.645", "wps": "478.6", "ups": "0.68", "wpb": "706.3", "bsz": "55.6", "num_updates": "19000", "lr": "0.000259739", "gnorm": "6.655", "loss_scale": "1024", "train_wall": "286", "gb_free": "8", "wall": "42199"}
[2025-02-03 10:26:59,426][train_inner][INFO] - {"epoch": 24, "update": 23.882, "loss": "28.412", "nll_loss": "0.976", "total": "713.31", "n_correct": "618.43", "ppl": "1.97", "accuracy": "86.699", "wps": "466.6", "ups": "0.65", "wpb": "713.3", "bsz": "57.1", "num_updates": "19200", "lr": "0.000252073", "gnorm": "6.721", "loss_scale": "1024", "train_wall": "298", "gb_free": "8", "wall": "42505"}
[2025-02-03 10:29:08,146][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 10:29:59,476][valid][INFO] - {"epoch": 24, "valid_loss": "38.934", "valid_nll_loss": "1.894", "valid_total": "678.4", "valid_n_correct": "523.25", "valid_ppl": "3.72", "valid_accuracy": "77.13", "valid_wps": "564.8", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "19295", "valid_best_accuracy": "77.13"}
[2025-02-03 10:29:59,479][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 19295 updates
[2025-02-03 10:29:59,480][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 10:30:08,563][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 10:30:13,493][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 19295 updates, score 77.13) (writing took 14.01442642416805 seconds)
[2025-02-03 10:30:13,495][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2025-02-03 10:30:13,504][train][INFO] - {"epoch": 24, "train_loss": "28.241", "train_nll_loss": "0.961", "train_total": "711.575", "train_n_correct": "618.478", "train_ppl": "1.95", "train_accuracy": "86.917", "train_wps": "412.7", "train_ups": "0.58", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "19295", "train_lr": "0.000248511", "train_gnorm": "6.552", "train_loss_scale": "1024", "train_train_wall": "1242", "train_gb_free": "8", "train_wall": "42699"}
[2025-02-03 10:30:14,565][fairseq.trainer][INFO] - begin training epoch 25
[2025-02-03 10:30:14,565][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 10:33:45,804][train_inner][INFO] - {"epoch": 25, "update": 24.131, "loss": "27.614", "nll_loss": "0.923", "total": "714.46", "n_correct": "625.7", "ppl": "1.9", "accuracy": "87.577", "wps": "351.7", "ups": "0.49", "wpb": "714.5", "bsz": "57.7", "num_updates": "19400", "lr": "0.000244633", "gnorm": "6.219", "loss_scale": "1024", "train_wall": "273", "gb_free": "8", "wall": "42912"}
[2025-02-03 10:39:08,893][train_inner][INFO] - {"epoch": 25, "update": 24.379, "loss": "27.353", "nll_loss": "0.917", "total": "715.95", "n_correct": "627.61", "ppl": "1.89", "accuracy": "87.661", "wps": "443.2", "ups": "0.62", "wpb": "716", "bsz": "58.2", "num_updates": "19600", "lr": "0.000237414", "gnorm": "6.406", "loss_scale": "1024", "train_wall": "292", "gb_free": "8", "wall": "43235"}
[2025-02-03 10:44:12,967][train_inner][INFO] - {"epoch": 25, "update": 24.628, "loss": "27.738", "nll_loss": "0.908", "total": "706.24", "n_correct": "620.495", "ppl": "1.88", "accuracy": "87.859", "wps": "464.5", "ups": "0.66", "wpb": "706.2", "bsz": "56.5", "num_updates": "19800", "lr": "0.000230407", "gnorm": "6.625", "loss_scale": "1024", "train_wall": "291", "gb_free": "8", "wall": "43539"}
[2025-02-03 10:45:56,090][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
[2025-02-03 10:49:12,542][train_inner][INFO] - {"epoch": 25, "update": 24.878, "loss": "27.928", "nll_loss": "0.888", "total": "709.7", "n_correct": "625.795", "ppl": "1.85", "accuracy": "88.177", "wps": "473.9", "ups": "0.67", "wpb": "709.7", "bsz": "55.9", "num_updates": "20000", "lr": "0.000223607", "gnorm": "6.692", "loss_scale": "512", "train_wall": "291", "gb_free": "8", "wall": "43838"}
[2025-02-03 10:51:24,658][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2025-02-03 10:51:24,662][train][INFO] - {"epoch": 25, "train_loss": "27.627", "train_nll_loss": "0.905", "train_total": "711.76", "train_n_correct": "625.665", "train_ppl": "1.87", "train_accuracy": "87.904", "train_wps": "449.6", "train_ups": "0.63", "train_wpb": "711.8", "train_bsz": "57", "train_num_updates": "20098", "train_lr": "0.000220348", "train_gnorm": "6.515", "train_loss_scale": "512", "train_train_wall": "1151", "train_gb_free": "8", "train_wall": "43970"}
[2025-02-03 10:51:26,635][fairseq.trainer][INFO] - begin training epoch 26
[2025-02-03 10:51:26,635][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 10:54:51,906][train_inner][INFO] - {"epoch": 26, "update": 25.127, "loss": "26.935", "nll_loss": "0.873", "total": "713.985", "n_correct": "631.81", "ppl": "1.83", "accuracy": "88.491", "wps": "420.8", "ups": "0.59", "wpb": "714", "bsz": "57.9", "num_updates": "20200", "lr": "0.000217007", "gnorm": "6.238", "loss_scale": "512", "train_wall": "292", "gb_free": "8", "wall": "44178"}
[2025-02-03 10:59:57,138][train_inner][INFO] - {"epoch": 26, "update": 25.376, "loss": "27.464", "nll_loss": "0.815", "total": "709.885", "n_correct": "634.645", "ppl": "1.76", "accuracy": "89.401", "wps": "465.2", "ups": "0.66", "wpb": "709.9", "bsz": "55.2", "num_updates": "20400", "lr": "0.000210603", "gnorm": "6.461", "loss_scale": "512", "train_wall": "295", "gb_free": "8", "wall": "44483"}
[2025-02-03 11:05:04,704][train_inner][INFO] - {"epoch": 26, "update": 25.624, "loss": "27.351", "nll_loss": "0.865", "total": "712.765", "n_correct": "631.77", "ppl": "1.82", "accuracy": "88.637", "wps": "463.5", "ups": "0.65", "wpb": "712.8", "bsz": "56.8", "num_updates": "20600", "lr": "0.000204387", "gnorm": "6.433", "loss_scale": "512", "train_wall": "298", "gb_free": "8", "wall": "44791"}
[2025-02-03 11:10:20,957][train_inner][INFO] - {"epoch": 26, "update": 25.873, "loss": "26.865", "nll_loss": "0.873", "total": "713.5", "n_correct": "630.99", "ppl": "1.83", "accuracy": "88.436", "wps": "451.2", "ups": "0.63", "wpb": "713.5", "bsz": "58.1", "num_updates": "20800", "lr": "0.000198355", "gnorm": "6.27", "loss_scale": "512", "train_wall": "281", "gb_free": "8", "wall": "45107"}
[2025-02-03 11:12:42,042][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 11:13:36,417][valid][INFO] - {"epoch": 26, "valid_loss": "39.281", "valid_nll_loss": "1.924", "valid_total": "678.4", "valid_n_correct": "524.75", "valid_ppl": "3.79", "valid_accuracy": "77.351", "valid_wps": "422.9", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "20902", "valid_best_accuracy": "77.351"}
[2025-02-03 11:13:36,420][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 20902 updates
[2025-02-03 11:13:36,420][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 11:13:45,380][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 11:13:50,754][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 20902 updates, score 77.351) (writing took 14.334462678991258 seconds)
[2025-02-03 11:13:50,756][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2025-02-03 11:13:50,762][train][INFO] - {"epoch": 26, "train_loss": "27.046", "train_nll_loss": "0.851", "train_total": "711.575", "train_n_correct": "632.127", "train_ppl": "1.8", "train_accuracy": "88.835", "train_wps": "425", "train_ups": "0.6", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "20902", "train_lr": "0.000195348", "train_gnorm": "6.35", "train_loss_scale": "512", "train_train_wall": "1163", "train_gb_free": "8", "train_wall": "45317"}
[2025-02-03 11:13:51,885][fairseq.trainer][INFO] - begin training epoch 27
[2025-02-03 11:13:51,885][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 11:17:07,171][train_inner][INFO] - {"epoch": 27, "update": 26.122, "loss": "26.631", "nll_loss": "0.829", "total": "708.105", "n_correct": "631.745", "ppl": "1.78", "accuracy": "89.216", "wps": "348.7", "ups": "0.49", "wpb": "708.1", "bsz": "57.1", "num_updates": "21000", "lr": "0.000192501", "gnorm": "6.26", "loss_scale": "512", "train_wall": "266", "gb_free": "8", "wall": "45513"}
[2025-02-03 11:22:13,584][train_inner][INFO] - {"epoch": 27, "update": 26.371, "loss": "26.601", "nll_loss": "0.784", "total": "708.12", "n_correct": "637.51", "ppl": "1.72", "accuracy": "90.029", "wps": "462.2", "ups": "0.65", "wpb": "708.1", "bsz": "56.2", "num_updates": "21200", "lr": "0.00018682", "gnorm": "6.175", "loss_scale": "512", "train_wall": "296", "gb_free": "8", "wall": "45819"}
[2025-02-03 11:27:30,624][train_inner][INFO] - {"epoch": 27, "update": 26.619, "loss": "26.401", "nll_loss": "0.829", "total": "717.79", "n_correct": "640.605", "ppl": "1.78", "accuracy": "89.247", "wps": "452.8", "ups": "0.63", "wpb": "717.8", "bsz": "58.4", "num_updates": "21400", "lr": "0.000181306", "gnorm": "6.247", "loss_scale": "512", "train_wall": "308", "gb_free": "8", "wall": "46136"}
[2025-02-03 11:32:39,055][train_inner][INFO] - {"epoch": 27, "update": 26.868, "loss": "26.471", "nll_loss": "0.818", "total": "715.435", "n_correct": "639.345", "ppl": "1.76", "accuracy": "89.365", "wps": "463.9", "ups": "0.65", "wpb": "715.4", "bsz": "57.8", "num_updates": "21600", "lr": "0.000175955", "gnorm": "6.196", "loss_scale": "512", "train_wall": "298", "gb_free": "8", "wall": "46445"}
[2025-02-03 11:35:01,964][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2025-02-03 11:35:01,968][train][INFO] - {"epoch": 27, "train_loss": "26.562", "train_nll_loss": "0.806", "train_total": "711.575", "train_n_correct": "637.734", "train_ppl": "1.75", "train_accuracy": "89.623", "train_wps": "450.1", "train_ups": "0.63", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "21706", "train_lr": "0.000173184", "train_gnorm": "6.211", "train_loss_scale": "512", "train_train_wall": "1182", "train_gb_free": "8", "train_wall": "46588"}
[2025-02-03 11:35:03,917][fairseq.trainer][INFO] - begin training epoch 28
[2025-02-03 11:35:03,918][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 11:38:18,666][train_inner][INFO] - {"epoch": 28, "update": 27.117, "loss": "26.133", "nll_loss": "0.779", "total": "708.165", "n_correct": "638.61", "ppl": "1.72", "accuracy": "90.178", "wps": "417.1", "ups": "0.59", "wpb": "708.2", "bsz": "57", "num_updates": "21800", "lr": "0.000170762", "gnorm": "6.031", "loss_scale": "512", "train_wall": "306", "gb_free": "8", "wall": "46784"}
[2025-02-03 11:43:22,102][train_inner][INFO] - {"epoch": 28, "update": 27.366, "loss": "26.371", "nll_loss": "0.736", "total": "706.95", "n_correct": "643.005", "ppl": "1.67", "accuracy": "90.955", "wps": "466.1", "ups": "0.66", "wpb": "707", "bsz": "55.4", "num_updates": "22000", "lr": "0.000165723", "gnorm": "6.039", "loss_scale": "512", "train_wall": "293", "gb_free": "8", "wall": "47088"}
[2025-02-03 11:48:22,416][train_inner][INFO] - {"epoch": 28, "update": 27.614, "loss": "26.032", "nll_loss": "0.781", "total": "710.185", "n_correct": "639.585", "ppl": "1.72", "accuracy": "90.059", "wps": "473.1", "ups": "0.67", "wpb": "710.2", "bsz": "57.4", "num_updates": "22200", "lr": "0.000160832", "gnorm": "6.159", "loss_scale": "512", "train_wall": "293", "gb_free": "8", "wall": "47388"}
[2025-02-03 11:53:32,177][train_inner][INFO] - {"epoch": 28, "update": 27.863, "loss": "26.184", "nll_loss": "0.778", "total": "721.015", "n_correct": "649.95", "ppl": "1.71", "accuracy": "90.144", "wps": "465.6", "ups": "0.65", "wpb": "721", "bsz": "57.9", "num_updates": "22400", "lr": "0.000156085", "gnorm": "6.167", "loss_scale": "512", "train_wall": "301", "gb_free": "8", "wall": "47698"}
[2025-02-03 11:56:00,671][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 11:56:53,851][valid][INFO] - {"epoch": 28, "valid_loss": "39.564", "valid_nll_loss": "1.955", "valid_total": "678.4", "valid_n_correct": "525", "valid_ppl": "3.88", "valid_accuracy": "77.388", "valid_wps": "477.1", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "22510", "valid_best_accuracy": "77.388"}
[2025-02-03 11:56:53,854][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 22510 updates
[2025-02-03 11:56:53,855][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 11:57:03,079][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 11:57:08,351][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 22510 updates, score 77.388) (writing took 14.496865380555391 seconds)
[2025-02-03 11:57:08,351][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2025-02-03 11:57:08,355][train][INFO] - {"epoch": 28, "train_loss": "26.112", "train_nll_loss": "0.765", "train_total": "711.575", "train_n_correct": "643.07", "train_ppl": "1.7", "train_accuracy": "90.373", "train_wps": "431.3", "train_ups": "0.61", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "22510", "train_lr": "0.000153534", "train_gnorm": "6.108", "train_loss_scale": "512", "train_train_wall": "1199", "train_gb_free": "8", "train_wall": "47914"}
[2025-02-03 11:57:09,428][fairseq.trainer][INFO] - begin training epoch 29
[2025-02-03 11:57:09,428][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 12:00:19,456][train_inner][INFO] - {"epoch": 29, "update": 28.112, "loss": "25.867", "nll_loss": "0.749", "total": "708", "n_correct": "641.225", "ppl": "1.68", "accuracy": "90.569", "wps": "347.7", "ups": "0.49", "wpb": "708", "bsz": "56.9", "num_updates": "22600", "lr": "0.000151479", "gnorm": "6.112", "loss_scale": "512", "train_wall": "270", "gb_free": "8", "wall": "48105"}
[2025-02-03 12:05:20,843][train_inner][INFO] - {"epoch": 29, "update": 28.361, "loss": "25.766", "nll_loss": "0.709", "total": "708.38", "n_correct": "647.37", "ppl": "1.64", "accuracy": "91.387", "wps": "470.1", "ups": "0.66", "wpb": "708.4", "bsz": "56.2", "num_updates": "22800", "lr": "0.000147008", "gnorm": "5.916", "loss_scale": "512", "train_wall": "293", "gb_free": "8", "wall": "48407"}
[2025-02-03 12:10:20,780][train_inner][INFO] - {"epoch": 29, "update": 28.609, "loss": "25.584", "nll_loss": "0.742", "total": "710.31", "n_correct": "645.225", "ppl": "1.67", "accuracy": "90.837", "wps": "473.7", "ups": "0.67", "wpb": "710.3", "bsz": "57.5", "num_updates": "23000", "lr": "0.000142669", "gnorm": "5.877", "loss_scale": "512", "train_wall": "290", "gb_free": "8", "wall": "48707"}
[2025-02-03 12:15:25,730][train_inner][INFO] - {"epoch": 29, "update": 28.858, "loss": "26.153", "nll_loss": "0.732", "total": "707.7", "n_correct": "643.595", "ppl": "1.66", "accuracy": "90.942", "wps": "464.2", "ups": "0.66", "wpb": "707.7", "bsz": "55.8", "num_updates": "23200", "lr": "0.000138459", "gnorm": "6.101", "loss_scale": "512", "train_wall": "280", "gb_free": "8", "wall": "49011"}
[2025-02-03 12:18:11,259][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2025-02-03 12:18:11,262][train][INFO] - {"epoch": 29, "train_loss": "25.778", "train_nll_loss": "0.734", "train_total": "711.575", "train_n_correct": "647.164", "train_ppl": "1.66", "train_accuracy": "90.948", "train_wps": "453", "train_ups": "0.64", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "23314", "train_lr": "0.000136114", "train_gnorm": "5.969", "train_loss_scale": "512", "train_train_wall": "1138", "train_gb_free": "8", "train_wall": "49177"}
[2025-02-03 12:18:13,223][fairseq.trainer][INFO] - begin training epoch 30
[2025-02-03 12:18:13,224][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 12:21:04,804][train_inner][INFO] - {"epoch": 30, "update": 29.107, "loss": "25.734", "nll_loss": "0.739", "total": "716.525", "n_correct": "651.815", "ppl": "1.67", "accuracy": "90.969", "wps": "422.7", "ups": "0.59", "wpb": "716.5", "bsz": "57.6", "num_updates": "23400", "lr": "0.000134372", "gnorm": "5.976", "loss_scale": "512", "train_wall": "292", "gb_free": "8", "wall": "49351"}
[2025-02-03 12:26:20,468][train_inner][INFO] - {"epoch": 30, "update": 29.356, "loss": "25.06", "nll_loss": "0.707", "total": "717.34", "n_correct": "655.99", "ppl": "1.63", "accuracy": "91.448", "wps": "454.5", "ups": "0.63", "wpb": "717.3", "bsz": "58.5", "num_updates": "23600", "lr": "0.000130407", "gnorm": "5.692", "loss_scale": "512", "train_wall": "285", "gb_free": "8", "wall": "49666"}
[2025-02-03 12:31:17,031][train_inner][INFO] - {"epoch": 30, "update": 29.604, "loss": "26.076", "nll_loss": "0.693", "total": "709.435", "n_correct": "650.065", "ppl": "1.62", "accuracy": "91.631", "wps": "478.5", "ups": "0.67", "wpb": "709.4", "bsz": "55.2", "num_updates": "23800", "lr": "0.000126558", "gnorm": "5.942", "loss_scale": "512", "train_wall": "287", "gb_free": "8", "wall": "49963"}
[2025-02-03 12:36:23,754][train_inner][INFO] - {"epoch": 30, "update": 29.853, "loss": "25.618", "nll_loss": "0.728", "total": "710.865", "n_correct": "647.035", "ppl": "1.66", "accuracy": "91.021", "wps": "463.5", "ups": "0.65", "wpb": "710.9", "bsz": "57.2", "num_updates": "24000", "lr": "0.000122823", "gnorm": "5.951", "loss_scale": "1024", "train_wall": "298", "gb_free": "8", "wall": "50270"}
[2025-02-03 12:36:31,290][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
[2025-02-03 12:37:06,295][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
[2025-02-03 12:37:17,576][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2025-02-03 12:38:37,244][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2025-02-03 12:39:22,452][fairseq_cli.train][INFO] - begin validation on "valid" subset
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [64, 1, 5, 7, 7], strides() = [245, 1, 49, 7, 1]
bucket_view.sizes() = [64, 1, 5, 7, 7], strides() = [245, 245, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [64, 1, 5, 7, 7], strides() = [245, 1, 49, 7, 1]
bucket_view.sizes() = [64, 1, 5, 7, 7], strides() = [245, 245, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [64, 1, 5, 7, 7], strides() = [245, 1, 49, 7, 1]
bucket_view.sizes() = [64, 1, 5, 7, 7], strides() = [245, 245, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [64, 1, 5, 7, 7], strides() = [245, 1, 49, 7, 1]
bucket_view.sizes() = [64, 1, 5, 7, 7], strides() = [245, 245, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-02-03 12:40:15,007][valid][INFO] - {"epoch": 30, "valid_loss": "39.643", "valid_nll_loss": "1.97", "valid_total": "678.4", "valid_n_correct": "525.85", "valid_ppl": "3.92", "valid_accuracy": "77.513", "valid_wps": "448.4", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "24114", "valid_best_accuracy": "77.513"}
[2025-02-03 12:40:15,010][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 24114 updates
[2025-02-03 12:40:15,010][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 12:40:24,759][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 12:40:30,234][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 24114 updates, score 77.513) (writing took 15.224385845474899 seconds)
[2025-02-03 12:40:30,235][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2025-02-03 12:40:30,238][train][INFO] - {"epoch": 30, "train_loss": "25.523", "train_nll_loss": "0.71", "train_total": "711.365", "train_n_correct": "649.98", "train_ppl": "1.64", "train_accuracy": "91.371", "train_wps": "425", "train_ups": "0.6", "train_wpb": "711.4", "train_bsz": "57", "train_num_updates": "24114", "train_lr": "0.000120743", "train_gnorm": "7.001", "train_loss_scale": "64", "train_train_wall": "1186", "train_gb_free": "6.1", "train_wall": "50516"}
[2025-02-03 12:40:31,240][fairseq.trainer][INFO] - begin training epoch 31
[2025-02-03 12:40:31,241][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 12:43:48,256][train_inner][INFO] - {"epoch": 31, "update": 30.107, "loss": "24.955", "nll_loss": "0.701", "total": "711.485", "n_correct": "650.455", "ppl": "1.63", "accuracy": "91.422", "wps": "320.1", "ups": "0.45", "wpb": "711.5", "bsz": "58", "num_updates": "24200", "lr": "0.000119198", "gnorm": "12.678", "loss_scale": "64", "train_wall": "314", "gb_free": "6.3", "wall": "50714"}
[2025-02-03 12:49:16,690][train_inner][INFO] - {"epoch": 31, "update": 30.356, "loss": "24.904", "nll_loss": "0.679", "total": "709.43", "n_correct": "652.375", "ppl": "1.6", "accuracy": "91.958", "wps": "432", "ups": "0.61", "wpb": "709.4", "bsz": "57.5", "num_updates": "24400", "lr": "0.00011568", "gnorm": "10.91", "loss_scale": "64", "train_wall": "318", "gb_free": "6.7", "wall": "51042"}
[2025-02-03 12:54:43,053][train_inner][INFO] - {"epoch": 31, "update": 30.604, "loss": "25.252", "nll_loss": "0.689", "total": "711.595", "n_correct": "652.85", "ppl": "1.61", "accuracy": "91.745", "wps": "436.1", "ups": "0.61", "wpb": "711.6", "bsz": "57", "num_updates": "24600", "lr": "0.000112266", "gnorm": "11.105", "loss_scale": "64", "train_wall": "318", "gb_free": "6.9", "wall": "51369"}
[2025-02-03 13:00:02,941][train_inner][INFO] - {"epoch": 31, "update": 30.853, "loss": "26.14", "nll_loss": "0.676", "total": "706.76", "n_correct": "650.185", "ppl": "1.6", "accuracy": "91.995", "wps": "441.9", "ups": "0.63", "wpb": "706.8", "bsz": "54.5", "num_updates": "24800", "lr": "0.000108953", "gnorm": "10.703", "loss_scale": "64", "train_wall": "310", "gb_free": "6.8", "wall": "51689"}
[2025-02-03 13:02:51,525][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2025-02-03 13:02:51,529][train][INFO] - {"epoch": 31, "train_loss": "25.18", "train_nll_loss": "0.681", "train_total": "711.575", "train_n_correct": "653.978", "train_ppl": "1.6", "train_accuracy": "91.906", "train_wps": "426.5", "train_ups": "0.6", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "24918", "train_lr": "0.000107044", "train_gnorm": "10.816", "train_loss_scale": "64", "train_train_wall": "1250", "train_gb_free": "6.2", "train_wall": "51857"}
[2025-02-03 13:02:53,257][fairseq.trainer][INFO] - begin training epoch 32
[2025-02-03 13:02:53,263][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 13:05:55,215][train_inner][INFO] - {"epoch": 32, "update": 31.102, "loss": "24.454", "nll_loss": "0.646", "total": "714.65", "n_correct": "660.53", "ppl": "1.56", "accuracy": "92.427", "wps": "405.7", "ups": "0.57", "wpb": "714.6", "bsz": "58.1", "num_updates": "25000", "lr": "0.000105737", "gnorm": "11.005", "loss_scale": "64", "train_wall": "299", "gb_free": "6.2", "wall": "52041"}
[2025-02-03 13:11:17,765][train_inner][INFO] - {"epoch": 32, "update": 31.351, "loss": "23.476", "nll_loss": "0.526", "total": "706.24", "n_correct": "665.28", "ppl": "1.44", "accuracy": "94.2", "wps": "437.9", "ups": "0.62", "wpb": "706.2", "bsz": "56.7", "num_updates": "25200", "lr": "0.000102617", "gnorm": "11.898", "loss_scale": "64", "train_wall": "314", "gb_free": "6.5", "wall": "52364"}
[2025-02-03 13:16:41,060][train_inner][INFO] - {"epoch": 32, "update": 31.6, "loss": "23.501", "nll_loss": "0.503", "total": "716.06", "n_correct": "677.16", "ppl": "1.42", "accuracy": "94.567", "wps": "443", "ups": "0.62", "wpb": "716.1", "bsz": "56.8", "num_updates": "25400", "lr": "9.9588e-05", "gnorm": "10.934", "loss_scale": "64", "train_wall": "315", "gb_free": "6", "wall": "52687"}
[2025-02-03 13:22:07,857][train_inner][INFO] - {"epoch": 32, "update": 31.848, "loss": "22.763", "nll_loss": "0.496", "total": "715.29", "n_correct": "676.755", "ppl": "1.41", "accuracy": "94.613", "wps": "437.8", "ups": "0.61", "wpb": "715.3", "bsz": "58.3", "num_updates": "25600", "lr": "9.66488e-05", "gnorm": "10.372", "loss_scale": "64", "train_wall": "316", "gb_free": "7.1", "wall": "53014"}
[2025-02-03 13:24:55,697][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 13:25:48,013][valid][INFO] - {"epoch": 32, "valid_loss": "33.471", "valid_nll_loss": "1.421", "valid_total": "678.4", "valid_n_correct": "561.5", "valid_ppl": "2.68", "valid_accuracy": "82.768", "valid_wps": "473.2", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "25722", "valid_best_accuracy": "82.768"}
[2025-02-03 13:25:48,016][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 25722 updates
[2025-02-03 13:25:48,017][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 13:25:57,103][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 13:26:02,669][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 25722 updates, score 82.768) (writing took 14.652272446081042 seconds)
[2025-02-03 13:26:02,675][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2025-02-03 13:26:02,681][train][INFO] - {"epoch": 32, "train_loss": "23.338", "train_nll_loss": "0.51", "train_total": "711.575", "train_n_correct": "671.963", "train_ppl": "1.42", "train_accuracy": "94.433", "train_wps": "411.2", "train_ups": "0.58", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "25722", "train_lr": "9.48987e-05", "train_gnorm": "11.127", "train_loss_scale": "64", "train_train_wall": "1243", "train_gb_free": "6.2", "train_wall": "53248"}
[2025-02-03 13:26:03,704][fairseq.trainer][INFO] - begin training epoch 33
[2025-02-03 13:26:03,704][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 13:29:06,538][train_inner][INFO] - {"epoch": 33, "update": 32.097, "loss": "22.974", "nll_loss": "0.466", "total": "711.58", "n_correct": "677.5", "ppl": "1.38", "accuracy": "95.211", "wps": "339.9", "ups": "0.48", "wpb": "711.6", "bsz": "56.7", "num_updates": "25800", "lr": "9.37964e-05", "gnorm": "10.381", "loss_scale": "64", "train_wall": "284", "gb_free": "6.5", "wall": "53432"}
[2025-02-03 13:34:34,046][train_inner][INFO] - {"epoch": 33, "update": 32.346, "loss": "21.903", "nll_loss": "0.427", "total": "713.28", "n_correct": "683.675", "ppl": "1.34", "accuracy": "95.849", "wps": "435.6", "ups": "0.61", "wpb": "713.3", "bsz": "58.4", "num_updates": "26000", "lr": "9.10282e-05", "gnorm": "9.042", "loss_scale": "64", "train_wall": "319", "gb_free": "6.6", "wall": "53760"}
[2025-02-03 13:39:56,529][train_inner][INFO] - {"epoch": 33, "update": 32.595, "loss": "22.68", "nll_loss": "0.424", "total": "707.03", "n_correct": "678.46", "ppl": "1.34", "accuracy": "95.959", "wps": "438.5", "ups": "0.62", "wpb": "707", "bsz": "55.9", "num_updates": "26200", "lr": "8.83417e-05", "gnorm": "9.496", "loss_scale": "64", "train_wall": "313", "gb_free": "6.4", "wall": "54082"}
[2025-02-03 13:45:18,577][train_inner][INFO] - {"epoch": 33, "update": 32.843, "loss": "22.306", "nll_loss": "0.422", "total": "714.6", "n_correct": "685.425", "ppl": "1.34", "accuracy": "95.917", "wps": "443.8", "ups": "0.62", "wpb": "714.6", "bsz": "57.3", "num_updates": "26400", "lr": "8.57345e-05", "gnorm": "8.886", "loss_scale": "64", "train_wall": "314", "gb_free": "6.2", "wall": "54404"}
[2025-02-03 13:48:14,146][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2025-02-03 13:48:14,150][train][INFO] - {"epoch": 33, "train_loss": "22.362", "train_nll_loss": "0.424", "train_total": "711.575", "train_n_correct": "682.582", "train_ppl": "1.34", "train_accuracy": "95.926", "train_wps": "429.7", "train_ups": "0.6", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "26526", "train_lr": "8.41316e-05", "train_gnorm": "9.178", "train_loss_scale": "64", "train_train_wall": "1237", "train_gb_free": "6", "train_wall": "54580"}
[2025-02-03 13:48:16,103][fairseq.trainer][INFO] - begin training epoch 34
[2025-02-03 13:48:16,104][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 13:51:04,305][train_inner][INFO] - {"epoch": 34, "update": 33.092, "loss": "22.141", "nll_loss": "0.401", "total": "710.73", "n_correct": "684.805", "ppl": "1.32", "accuracy": "96.352", "wps": "411.2", "ups": "0.58", "wpb": "710.7", "bsz": "56.8", "num_updates": "26600", "lr": "8.32042e-05", "gnorm": "8.688", "loss_scale": "64", "train_wall": "311", "gb_free": "6", "wall": "54750"}
[2025-02-03 13:56:28,770][train_inner][INFO] - {"epoch": 34, "update": 33.341, "loss": "21.849", "nll_loss": "0.388", "total": "707.16", "n_correct": "682.755", "ppl": "1.31", "accuracy": "96.549", "wps": "435.9", "ups": "0.62", "wpb": "707.2", "bsz": "56.9", "num_updates": "26800", "lr": "8.07486e-05", "gnorm": "8.58", "loss_scale": "64", "train_wall": "315", "gb_free": "6.1", "wall": "55075"}
[2025-02-03 14:01:46,456][train_inner][INFO] - {"epoch": 34, "update": 33.59, "loss": "22.167", "nll_loss": "0.375", "total": "711.245", "n_correct": "688.43", "ppl": "1.3", "accuracy": "96.792", "wps": "447.8", "ups": "0.63", "wpb": "711.2", "bsz": "56", "num_updates": "27000", "lr": "7.83654e-05", "gnorm": "8.204", "loss_scale": "64", "train_wall": "308", "gb_free": "6.8", "wall": "55392"}
[2025-02-03 14:07:13,008][train_inner][INFO] - {"epoch": 34, "update": 33.838, "loss": "21.668", "nll_loss": "0.388", "total": "717.32", "n_correct": "692.645", "ppl": "1.31", "accuracy": "96.56", "wps": "439.4", "ups": "0.61", "wpb": "717.3", "bsz": "58.2", "num_updates": "27200", "lr": "7.60526e-05", "gnorm": "8.227", "loss_scale": "64", "train_wall": "317", "gb_free": "6.2", "wall": "55719"}
[2025-02-03 14:10:16,377][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 14:11:08,765][valid][INFO] - {"epoch": 34, "valid_loss": "32.825", "valid_nll_loss": "1.375", "valid_total": "678.4", "valid_n_correct": "565.7", "valid_ppl": "2.59", "valid_accuracy": "83.387", "valid_wps": "459.1", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "27330", "valid_best_accuracy": "83.387"}
[2025-02-03 14:11:08,767][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 27330 updates
[2025-02-03 14:11:08,768][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 14:11:18,516][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 14:11:23,792][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 34 @ 27330 updates, score 83.387) (writing took 15.024733697064221 seconds)
[2025-02-03 14:11:23,793][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2025-02-03 14:11:23,798][train][INFO] - {"epoch": 34, "train_loss": "21.882", "train_nll_loss": "0.383", "train_total": "711.575", "train_n_correct": "687.659", "train_ppl": "1.3", "train_accuracy": "96.639", "train_wps": "411.7", "train_ups": "0.58", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "27330", "train_lr": "7.4586e-05", "train_gnorm": "8.357", "train_loss_scale": "64", "train_train_wall": "1259", "train_gb_free": "6", "train_wall": "55970"}
[2025-02-03 14:11:24,904][fairseq.trainer][INFO] - begin training epoch 35
[2025-02-03 14:11:24,905][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 14:14:10,357][train_inner][INFO] - {"epoch": 35, "update": 34.087, "loss": "22.039", "nll_loss": "0.373", "total": "705.295", "n_correct": "682.635", "ppl": "1.29", "accuracy": "96.787", "wps": "338", "ups": "0.48", "wpb": "705.3", "bsz": "55.8", "num_updates": "27400", "lr": "7.38081e-05", "gnorm": "8.531", "loss_scale": "64", "train_wall": "290", "gb_free": "6.5", "wall": "56136"}
[2025-02-03 14:19:28,119][train_inner][INFO] - {"epoch": 35, "update": 34.336, "loss": "21.546", "nll_loss": "0.349", "total": "710.645", "n_correct": "690.89", "ppl": "1.27", "accuracy": "97.22", "wps": "447.3", "ups": "0.63", "wpb": "710.6", "bsz": "56.8", "num_updates": "27600", "lr": "7.16298e-05", "gnorm": "7.589", "loss_scale": "64", "train_wall": "308", "gb_free": "6.4", "wall": "56454"}
[2025-02-03 14:24:51,335][train_inner][INFO] - {"epoch": 35, "update": 34.585, "loss": "21.973", "nll_loss": "0.349", "total": "713.28", "n_correct": "693.84", "ppl": "1.27", "accuracy": "97.275", "wps": "441.4", "ups": "0.62", "wpb": "713.3", "bsz": "55.9", "num_updates": "27800", "lr": "6.95158e-05", "gnorm": "7.845", "loss_scale": "64", "train_wall": "314", "gb_free": "6.1", "wall": "56777"}
[2025-02-03 14:30:13,118][train_inner][INFO] - {"epoch": 35, "update": 34.833, "loss": "21.303", "nll_loss": "0.356", "total": "711.185", "n_correct": "690.46", "ppl": "1.28", "accuracy": "97.086", "wps": "442", "ups": "0.62", "wpb": "711.2", "bsz": "57.6", "num_updates": "28000", "lr": "6.74641e-05", "gnorm": "7.912", "loss_scale": "64", "train_wall": "312", "gb_free": "7.1", "wall": "57099"}
[2025-02-03 14:33:26,878][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2025-02-03 14:33:26,882][train][INFO] - {"epoch": 35, "train_loss": "21.498", "train_nll_loss": "0.351", "train_total": "711.575", "train_n_correct": "691.636", "train_ppl": "1.28", "train_accuracy": "97.198", "train_wps": "432.4", "train_ups": "0.61", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "28134", "train_lr": "6.61235e-05", "train_gnorm": "7.783", "train_loss_scale": "64", "train_train_wall": "1234", "train_gb_free": "7", "train_wall": "57293"}
[2025-02-03 14:33:28,698][fairseq.trainer][INFO] - begin training epoch 36
[2025-02-03 14:33:28,699][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 14:36:03,921][train_inner][INFO] - {"epoch": 36, "update": 35.082, "loss": "20.734", "nll_loss": "0.338", "total": "714.94", "n_correct": "696.48", "ppl": "1.26", "accuracy": "97.418", "wps": "407.6", "ups": "0.57", "wpb": "714.9", "bsz": "58.9", "num_updates": "28200", "lr": "6.54731e-05", "gnorm": "7.386", "loss_scale": "128", "train_wall": "318", "gb_free": "6.1", "wall": "57450"}
[2025-02-03 14:41:24,503][train_inner][INFO] - {"epoch": 36, "update": 35.331, "loss": "21.375", "nll_loss": "0.325", "total": "713.295", "n_correct": "696.73", "ppl": "1.25", "accuracy": "97.678", "wps": "445", "ups": "0.62", "wpb": "713.3", "bsz": "56.6", "num_updates": "28400", "lr": "6.35408e-05", "gnorm": "7.192", "loss_scale": "128", "train_wall": "310", "gb_free": "6.8", "wall": "57770"}
[2025-02-03 14:46:49,595][train_inner][INFO] - {"epoch": 36, "update": 35.58, "loss": "21.087", "nll_loss": "0.323", "total": "713.205", "n_correct": "696.79", "ppl": "1.25", "accuracy": "97.698", "wps": "438.8", "ups": "0.62", "wpb": "713.2", "bsz": "57.4", "num_updates": "28600", "lr": "6.16655e-05", "gnorm": "7.5", "loss_scale": "128", "train_wall": "316", "gb_free": "6.4", "wall": "58095"}
[2025-02-03 14:52:14,478][train_inner][INFO] - {"epoch": 36, "update": 35.828, "loss": "21.382", "nll_loss": "0.327", "total": "714.125", "n_correct": "697.27", "ppl": "1.25", "accuracy": "97.64", "wps": "439.6", "ups": "0.62", "wpb": "714.1", "bsz": "56.7", "num_updates": "28800", "lr": "5.98455e-05", "gnorm": "7.378", "loss_scale": "128", "train_wall": "316", "gb_free": "6.5", "wall": "58420"}
[2025-02-03 14:55:30,040][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 14:56:23,052][valid][INFO] - {"epoch": 36, "valid_loss": "32.213", "valid_nll_loss": "1.33", "valid_total": "678.4", "valid_n_correct": "569.9", "valid_ppl": "2.51", "valid_accuracy": "84.006", "valid_wps": "610.3", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "28938", "valid_best_accuracy": "84.006"}
[2025-02-03 14:56:23,056][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 28938 updates
[2025-02-03 14:56:23,058][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 14:56:32,497][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 14:56:37,665][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 36 @ 28938 updates, score 84.006) (writing took 14.609261714853346 seconds)
[2025-02-03 14:56:37,665][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2025-02-03 14:56:37,669][train][INFO] - {"epoch": 36, "train_loss": "21.165", "train_nll_loss": "0.323", "train_total": "711.575", "train_n_correct": "695.127", "train_ppl": "1.25", "train_accuracy": "97.689", "train_wps": "411.4", "train_ups": "0.58", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "28938", "train_lr": "5.86212e-05", "train_gnorm": "7.272", "train_loss_scale": "128", "train_train_wall": "1262", "train_gb_free": "6", "train_wall": "58683"}
[2025-02-03 14:56:38,767][fairseq.trainer][INFO] - begin training epoch 37
[2025-02-03 14:56:38,768][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 14:59:08,136][train_inner][INFO] - {"epoch": 37, "update": 36.077, "loss": "21.369", "nll_loss": "0.315", "total": "706.52", "n_correct": "691.29", "ppl": "1.24", "accuracy": "97.844", "wps": "341.6", "ups": "0.48", "wpb": "706.5", "bsz": "55.8", "num_updates": "29000", "lr": "5.80793e-05", "gnorm": "7.059", "loss_scale": "128", "train_wall": "287", "gb_free": "6.2", "wall": "58834"}
[2025-02-03 15:04:44,116][train_inner][INFO] - {"epoch": 37, "update": 36.326, "loss": "20.017", "nll_loss": "0.305", "total": "715.925", "n_correct": "701.735", "ppl": "1.23", "accuracy": "98.018", "wps": "426.2", "ups": "0.6", "wpb": "715.9", "bsz": "60", "num_updates": "29200", "lr": "5.63652e-05", "gnorm": "6.393", "loss_scale": "128", "train_wall": "326", "gb_free": "6.6", "wall": "59170"}
[2025-02-03 15:10:05,650][train_inner][INFO] - {"epoch": 37, "update": 36.575, "loss": "20.761", "nll_loss": "0.304", "total": "716.58", "n_correct": "702.38", "ppl": "1.23", "accuracy": "98.018", "wps": "445.8", "ups": "0.62", "wpb": "716.6", "bsz": "57.9", "num_updates": "29400", "lr": "5.47017e-05", "gnorm": "6.61", "loss_scale": "128", "train_wall": "311", "gb_free": "6.8", "wall": "59491"}
[2025-02-03 15:15:24,824][train_inner][INFO] - {"epoch": 37, "update": 36.823, "loss": "21.337", "nll_loss": "0.305", "total": "710.34", "n_correct": "696.14", "ppl": "1.24", "accuracy": "98.001", "wps": "445.1", "ups": "0.63", "wpb": "710.3", "bsz": "55.8", "num_updates": "29600", "lr": "5.30873e-05", "gnorm": "6.897", "loss_scale": "128", "train_wall": "309", "gb_free": "6.4", "wall": "59811"}
[2025-02-03 15:18:43,850][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2025-02-03 15:18:43,853][train][INFO] - {"epoch": 37, "train_loss": "20.939", "train_nll_loss": "0.305", "train_total": "711.575", "train_n_correct": "697.404", "train_ppl": "1.24", "train_accuracy": "98.009", "train_wps": "431.4", "train_ups": "0.61", "train_wpb": "711.6", "train_bsz": "57", "train_num_updates": "29742", "train_lr": "5.19701e-05", "train_gnorm": "6.769", "train_loss_scale": "128", "train_train_wall": "1236", "train_gb_free": "6.2", "train_wall": "60010"}
[2025-02-03 15:18:45,710][fairseq.trainer][INFO] - begin training epoch 38
[2025-02-03 15:18:45,711][fairseq_cli.train][INFO] - Start iterating over samples
[2025-02-03 15:21:09,021][train_inner][INFO] - {"epoch": 38, "update": 37.072, "loss": "21.18", "nll_loss": "0.306", "total": "704.07", "n_correct": "689.86", "ppl": "1.24", "accuracy": "97.982", "wps": "409.1", "ups": "0.58", "wpb": "704.1", "bsz": "55.8", "num_updates": "29800", "lr": "5.15205e-05", "gnorm": "7.045", "loss_scale": "128", "train_wall": "307", "gb_free": "6.1", "wall": "60155"}
[2025-02-03 15:26:38,456][train_inner][INFO] - {"epoch": 38, "update": 37.321, "loss": "20.383", "nll_loss": "0.29", "total": "712.835", "n_correct": "700.365", "ppl": "1.22", "accuracy": "98.251", "wps": "432.8", "ups": "0.61", "wpb": "712.8", "bsz": "58.2", "num_updates": "30000", "lr": "5e-05", "gnorm": "6.386", "loss_scale": "128", "train_wall": "320", "gb_free": "6.2", "wall": "60484"}
[2025-02-03 15:26:38,465][fairseq_cli.train][INFO] - Stopping training due to num_updates: 30000 >= max_update: 30000
[2025-02-03 15:26:38,466][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2025-02-03 15:27:34,487][valid][INFO] - {"epoch": 38, "valid_loss": "32.038", "valid_nll_loss": "1.316", "valid_total": "678.4", "valid_n_correct": "570.8", "valid_ppl": "2.49", "valid_accuracy": "84.139", "valid_wps": "495.8", "valid_wpb": "678.4", "valid_bsz": "54.1", "valid_num_updates": "30000", "valid_best_accuracy": "84.139"}
[2025-02-03 15:27:34,489][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 30000 updates
[2025-02-03 15:27:34,490][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 15:27:42,563][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-02-03 15:27:48,076][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 30000 updates, score 84.139) (writing took 13.586654383689165 seconds)
[2025-02-03 15:27:48,077][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2025-02-03 15:27:48,081][train][INFO] - {"epoch": 38, "train_loss": "20.41", "train_nll_loss": "0.293", "train_total": "713.574", "train_n_correct": "700.787", "train_ppl": "1.22", "train_accuracy": "98.208", "train_wps": "338.3", "train_ups": "0.47", "train_wpb": "713.6", "train_bsz": "58.2", "train_num_updates": "30000", "train_lr": "5e-05", "train_gnorm": "6.494", "train_loss_scale": "128", "train_train_wall": "433", "train_gb_free": "6.2", "train_wall": "60554"}
[2025-02-03 15:27:48,081][fairseq_cli.train][INFO] - done training in 60547.4 seconds
