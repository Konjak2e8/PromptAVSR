2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 1
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 4
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 3
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 2
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 6
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 7
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10565
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 5
2024-12-01 00:17:37 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 0
[2024-12-01 00:17:38,760][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/workspace/av_hubert/avhubert', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10565', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 2, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 45000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'av_hubert_seq2seq', 'w2v_path': '/workspace/AV_HuBERT_pretrained/base_vox_iter5.pt', 'apply_mask': False, 'mask_selection': 'static', 'mask_length': 10, 'mask_other': 0, 'mask_prob': 0.75, 'mask_channel_selection': 'static', 'mask_channel_length': 64, 'mask_channel_other': 0, 'mask_channel_prob': 0.5, 'layerdrop': 0.1, 'dropout': 0.0, 'activation_dropout': 0.1, 'attention_dropout': 0.0, 'feature_grad_mult': 1.0, 'decoder_layers': 6, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.0, 'decoder_activation_dropout': 0.1, 'freeze_finetune_updates': 22500, 'share_decoder_input_output_embed': True, 'decoder_normalize_before': True}, 'task': {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/workspace/lrs2/433h_data', 'label_dir': '/workspace/lrs2/433h_data', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'fine_tuning': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video', 'audio'], 'image_aug': True, 'pad_audio': True, 'random_crop': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': True, 'ignore_prefix_size': 0, 'sentence_avg': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 15000, 'hold_steps': 0, 'decay_steps': 30000, 'phase_ratio': None, 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 45000, 'lr': [0.001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-12-01 00:17:38,767][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/finetune_prompt
[2024-12-01 00:17:38,767][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['wrd'], 'label_dir': '/workspace/lrs2/433h_data', 'label_rate': -1, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 500, 'min_sample_size': None, 'max_trim_sample_size': '${task.max_sample_size}', 'single_target': True, 'random_crop': False, 'pad_audio': True, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['video', 'audio'], 'is_s2s': True, 'tokenizer_bpe_name': 'sentencepiece', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': True}
[2024-12-01 00:17:39,717][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/finetune_prompt
[2024-12-01 00:17:39,717][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['km'], 'label_dir': '/checkpoint/bshi/data/lrs3//video/hubert/stitch-iters/envox-iter4-l12c2000/', 'label_rate': 25, 'sample_rate': 25, 'normalize': True, 'enable_padding': False, 'max_sample_size': 2000, 'min_sample_size': 5, 'max_trim_sample_size': 400, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['audio', 'video'], 'is_s2s': False, 'tokenizer_bpe_name': None, 'tokenizer_bpe_model': None, 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': False}
[2024-12-01 00:17:39,726][avhubert.hubert][INFO] - HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
[2024-12-01 00:17:55,134][fairseq_cli.train][INFO] - AVHubertSeq2Seq(
  (encoder): HubertEncoderWrapper(
    (w2v_model): AVHubertModel(
      (feature_extractor_audio): SubModel(
        (proj): Linear(in_features=104, out_features=768, bias=True)
      )
      (feature_extractor_video): SubModel(
        (resnet): ResEncoder(
          (frontend3D): Sequential(
            (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
            (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): PReLU(num_parameters=64)
            (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
          )
          (trunk): ResNet(
            (layer1): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer2): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer3): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer4): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (avgpool): AdaptiveAvgPool2d(output_size=1)
          )
        )
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
      (post_extract_proj): Linear(in_features=1536, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (modal_prompt_learner): MultiModalPromptLearner(
        (compound_prompt_projections_audio): ModuleList(
          (0-5): 6 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_audio): ModuleList(
          (0-5): 6 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (compound_prompt_projections_video): ModuleList(
          (0-5): 6 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_video): ModuleList(
          (0-5): 6 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (common_prompt_projection_video): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
        (common_prompt_projection_audio): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
      )
      (video_encoder): TransformerEncoder_prompt(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (audio_encoder): TransformerEncoder_prompt(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(1000, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
[2024-12-01 00:17:55,145][fairseq_cli.train][INFO] - task: AVHubertPretrainingTask
[2024-12-01 00:17:55,145][fairseq_cli.train][INFO] - model: AVHubertSeq2Seq
[2024-12-01 00:17:55,145][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2024-12-01 00:17:55,154][fairseq_cli.train][INFO] - num. shared model params: 343,079,624 (num. trained: 343,079,624)
[2024-12-01 00:17:55,158][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-12-01 00:17:55,160][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2024-12-01 00:17:55,348][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 1082, skipped 0 short and 0 long and 0 unaligned, longest-loaded=153, shortest-loaded=14
[2024-12-01 00:17:55,349][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/valid.wrd is sequence label. skipped
[2024-12-01 00:17:55,349][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    <avhubert.utils.CenterCrop object at 0x7fe07cf44b50>
    Normalize(mean=0.421, std=0.165)
)
[2024-12-01 00:17:55,349][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2024-12-01 00:17:55,349][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2024-12-01 00:18:04,436][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv1.bias
[2024-12-01 00:18:04,452][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv2.bias
[2024-12-01 00:18:04,452][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv1.bias
[2024-12-01 00:18:04,452][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv2.bias
[2024-12-01 00:18:04,452][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv1.bias
[2024-12-01 00:18:04,453][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv2.bias
[2024-12-01 00:18:04,453][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.0.bias
[2024-12-01 00:18:04,453][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv1.bias
[2024-12-01 00:18:04,453][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv2.bias
[2024-12-01 00:18:04,454][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv1.bias
[2024-12-01 00:18:04,454][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv2.bias
[2024-12-01 00:18:04,454][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.0.bias
[2024-12-01 00:18:04,454][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv1.bias
[2024-12-01 00:18:04,454][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv2.bias
[2024-12-01 00:18:04,455][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv1.bias
[2024-12-01 00:18:04,455][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv2.bias
[2024-12-01 00:18:04,455][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.0.bias
[2024-12-01 00:18:04,455][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv1.bias
[2024-12-01 00:18:04,460][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv2.bias
[2024-12-01 00:18:14,419][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-12-01 00:18:14,420][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,420][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,420][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,420][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,421][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,421][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,421][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,421][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-01 00:18:14,421][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-12-01 00:18:14,422][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-12-01 00:18:14,422][fairseq_cli.train][INFO] - max tokens per device = 1000 and max sentences per device = None
[2024-12-01 00:18:14,425][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2024-12-01 00:18:14,425][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2024-12-01 00:18:14,426][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-12-01 00:18:14,426][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2024-12-01 00:18:15,044][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 163822, skipped 0 short and 292 long and 0 unaligned, longest-loaded=500, shortest-loaded=0
[2024-12-01 00:18:15,132][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/train.wrd is sequence label. skipped
[2024-12-01 00:18:15,133][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    RandomCrop(size=(88, 88))
    <avhubert.utils.HorizontalFlip object at 0x7fe04ba38ee0>
    Normalize(mean=0.421, std=0.165)
)
[2024-12-01 00:18:15,133][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2024-12-01 00:18:15,133][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
[2024-12-01 00:18:17,775][fairseq.trainer][INFO] - begin training epoch 1
[2024-12-01 00:18:17,776][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
[2024-12-01 00:26:06,031][train_inner][INFO] - {"epoch": 1, "update": 0.068, "loss": "210.582", "nll_loss": "8.49", "total": "1332.83", "n_correct": "77.775", "ppl": "359.46", "accuracy": "5.835", "wps": "642.7", "ups": "0.48", "wpb": "1332.8", "bsz": "55.3", "num_updates": "200", "lr": "2.32e-05", "gnorm": "42.759", "loss_scale": "128", "train_wall": "428", "gb_free": "3.9", "wall": "471"}
[2024-12-01 00:32:57,184][train_inner][INFO] - {"epoch": 1, "update": 0.136, "loss": "203.124", "nll_loss": "8.21", "total": "1345.46", "n_correct": "89.825", "ppl": "296.13", "accuracy": "6.676", "wps": "654.5", "ups": "0.49", "wpb": "1345.5", "bsz": "56.3", "num_updates": "400", "lr": "3.64e-05", "gnorm": "28.691", "loss_scale": "128", "train_wall": "402", "gb_free": "3.9", "wall": "883"}
[2024-12-01 00:39:41,934][train_inner][INFO] - {"epoch": 1, "update": 0.205, "loss": "203.12", "nll_loss": "8.187", "total": "1331.79", "n_correct": "90.19", "ppl": "291.39", "accuracy": "6.772", "wps": "658.1", "ups": "0.49", "wpb": "1331.8", "bsz": "55.6", "num_updates": "600", "lr": "4.96e-05", "gnorm": "25.766", "loss_scale": "128", "train_wall": "396", "gb_free": "3.9", "wall": "1287"}
[2024-12-01 00:46:32,603][train_inner][INFO] - {"epoch": 1, "update": 0.273, "loss": "201.813", "nll_loss": "8.173", "total": "1342.96", "n_correct": "93.085", "ppl": "288.6", "accuracy": "6.931", "wps": "654.1", "ups": "0.49", "wpb": "1343", "bsz": "56.3", "num_updates": "800", "lr": "6.28e-05", "gnorm": "23.889", "loss_scale": "128", "train_wall": "401", "gb_free": "3.9", "wall": "1698"}
[2024-12-01 00:53:21,518][train_inner][INFO] - {"epoch": 1, "update": 0.341, "loss": "194.203", "nll_loss": "8.155", "total": "1334.97", "n_correct": "94.74", "ppl": "285.03", "accuracy": "7.097", "wps": "653", "ups": "0.49", "wpb": "1335", "bsz": "58.1", "num_updates": "1000", "lr": "7.6e-05", "gnorm": "23.099", "loss_scale": "128", "train_wall": "400", "gb_free": "3.9", "wall": "2107"}
[2024-12-01 01:00:05,034][train_inner][INFO] - {"epoch": 1, "update": 0.409, "loss": "201.209", "nll_loss": "8.151", "total": "1323.59", "n_correct": "95.07", "ppl": "284.17", "accuracy": "7.183", "wps": "656", "ups": "0.5", "wpb": "1323.6", "bsz": "55.5", "num_updates": "1200", "lr": "8.92e-05", "gnorm": "23.757", "loss_scale": "128", "train_wall": "395", "gb_free": "3.9", "wall": "2511"}
[2024-12-01 01:06:45,069][train_inner][INFO] - {"epoch": 1, "update": 0.478, "loss": "201.758", "nll_loss": "8.068", "total": "1335.74", "n_correct": "100.595", "ppl": "268.34", "accuracy": "7.531", "wps": "667.8", "ups": "0.5", "wpb": "1335.7", "bsz": "55.4", "num_updates": "1400", "lr": "0.0001024", "gnorm": "26.639", "loss_scale": "128", "train_wall": "390", "gb_free": "3.9", "wall": "2911"}
[2024-12-01 01:13:34,026][train_inner][INFO] - {"epoch": 1, "update": 0.546, "loss": "196.72", "nll_loss": "8.006", "total": "1325", "n_correct": "108.37", "ppl": "257.09", "accuracy": "8.179", "wps": "648", "ups": "0.49", "wpb": "1325", "bsz": "56", "num_updates": "1600", "lr": "0.0001156", "gnorm": "29.018", "loss_scale": "128", "train_wall": "399", "gb_free": "3.9", "wall": "3320"}
[2024-12-01 01:20:18,856][train_inner][INFO] - {"epoch": 1, "update": 0.614, "loss": "197.89", "nll_loss": "7.844", "total": "1332.78", "n_correct": "124.375", "ppl": "229.71", "accuracy": "9.332", "wps": "658.4", "ups": "0.49", "wpb": "1332.8", "bsz": "55.1", "num_updates": "1800", "lr": "0.0001288", "gnorm": "42.076", "loss_scale": "128", "train_wall": "396", "gb_free": "3.9", "wall": "3724"}
[2024-12-01 01:27:03,857][train_inner][INFO] - {"epoch": 1, "update": 0.682, "loss": "189.579", "nll_loss": "7.61", "total": "1329.59", "n_correct": "138.195", "ppl": "195.33", "accuracy": "10.394", "wps": "656.6", "ups": "0.49", "wpb": "1329.6", "bsz": "56", "num_updates": "2000", "lr": "0.000142", "gnorm": "59.077", "loss_scale": "128", "train_wall": "396", "gb_free": "3.9", "wall": "4129"}
[2024-12-01 01:33:49,771][train_inner][INFO] - {"epoch": 1, "update": 0.751, "loss": "187.692", "nll_loss": "7.46", "total": "1327.52", "n_correct": "144.795", "ppl": "176.01", "accuracy": "10.907", "wps": "654.1", "ups": "0.49", "wpb": "1327.5", "bsz": "55.6", "num_updates": "2200", "lr": "0.0001552", "gnorm": "66.138", "loss_scale": "256", "train_wall": "397", "gb_free": "3.9", "wall": "4535"}
[2024-12-01 01:40:37,020][train_inner][INFO] - {"epoch": 1, "update": 0.819, "loss": "185.599", "nll_loss": "7.371", "total": "1332.49", "n_correct": "150.845", "ppl": "165.57", "accuracy": "11.32", "wps": "654.4", "ups": "0.49", "wpb": "1332.5", "bsz": "55.9", "num_updates": "2400", "lr": "0.0001684", "gnorm": "66.836", "loss_scale": "256", "train_wall": "398", "gb_free": "3.9", "wall": "4943"}
[2024-12-01 01:47:24,696][train_inner][INFO] - {"epoch": 1, "update": 0.887, "loss": "183.418", "nll_loss": "7.298", "total": "1331.36", "n_correct": "155.2", "ppl": "157.39", "accuracy": "11.657", "wps": "653.2", "ups": "0.49", "wpb": "1331.4", "bsz": "56.1", "num_updates": "2600", "lr": "0.0001816", "gnorm": "62.633", "loss_scale": "256", "train_wall": "399", "gb_free": "3.9", "wall": "5350"}
[2024-12-01 01:54:06,567][train_inner][INFO] - {"epoch": 1, "update": 0.955, "loss": "183.395", "nll_loss": "7.214", "total": "1324.27", "n_correct": "161.23", "ppl": "148.49", "accuracy": "12.175", "wps": "659.1", "ups": "0.5", "wpb": "1324.3", "bsz": "55.3", "num_updates": "2800", "lr": "0.0001948", "gnorm": "56.387", "loss_scale": "256", "train_wall": "393", "gb_free": "3.9", "wall": "5752"}
[2024-12-01 01:58:00,915][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2024-12-01 01:58:00,920][train][INFO] - {"epoch": 1, "train_loss": "194.963", "train_nll_loss": "7.843", "train_total": "1331.66", "train_n_correct": "118.265", "train_ppl": "229.56", "train_accuracy": "8.881", "train_wps": "658.2", "train_ups": "0.49", "train_wpb": "1331.7", "train_bsz": "55.9", "train_num_updates": "2931", "train_lr": "0.000203446", "train_gnorm": "41.691", "train_loss_scale": "256", "train_train_wall": "5819", "train_gb_free": "3.9", "train_wall": "5986"}
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2024-12-01 01:58:02,569][fairseq.trainer][INFO] - begin training epoch 2
[2024-12-01 01:58:02,569][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2024-12-01 02:01:16,095][train_inner][INFO] - {"epoch": 2, "update": 1.024, "loss": "178.905", "nll_loss": "7.148", "total": "1325.29", "n_correct": "167.23", "ppl": "141.83", "accuracy": "12.618", "wps": "617.1", "ups": "0.47", "wpb": "1325.3", "bsz": "56.3", "num_updates": "3000", "lr": "0.000208", "gnorm": "53.761", "loss_scale": "256", "train_wall": "383", "gb_free": "3.9", "wall": "6182"}
[2024-12-01 02:07:59,752][train_inner][INFO] - {"epoch": 2, "update": 1.092, "loss": "183.239", "nll_loss": "7.089", "total": "1342.13", "n_correct": "172.89", "ppl": "136.12", "accuracy": "12.882", "wps": "665", "ups": "0.5", "wpb": "1342.1", "bsz": "55.3", "num_updates": "3200", "lr": "0.0002212", "gnorm": "50.069", "loss_scale": "256", "train_wall": "394", "gb_free": "3.9", "wall": "6585"}
[2024-12-01 02:14:36,929][train_inner][INFO] - {"epoch": 2, "update": 1.16, "loss": "184.706", "nll_loss": "7.041", "total": "1333.18", "n_correct": "174.99", "ppl": "131.64", "accuracy": "13.126", "wps": "671.3", "ups": "0.5", "wpb": "1333.2", "bsz": "54.2", "num_updates": "3400", "lr": "0.0002344", "gnorm": "50.754", "loss_scale": "256", "train_wall": "388", "gb_free": "3.9", "wall": "6983"}
[2024-12-01 02:21:21,759][train_inner][INFO] - {"epoch": 2, "update": 1.228, "loss": "176.745", "nll_loss": "6.97", "total": "1333.95", "n_correct": "182.215", "ppl": "125.38", "accuracy": "13.66", "wps": "659", "ups": "0.49", "wpb": "1334", "bsz": "56.2", "num_updates": "3600", "lr": "0.0002476", "gnorm": "44.861", "loss_scale": "256", "train_wall": "396", "gb_free": "3.9", "wall": "7387"}
[2024-12-01 02:28:10,319][train_inner][INFO] - {"epoch": 2, "update": 1.296, "loss": "175.589", "nll_loss": "6.908", "total": "1333.48", "n_correct": "186.105", "ppl": "120.08", "accuracy": "13.956", "wps": "652.8", "ups": "0.49", "wpb": "1333.5", "bsz": "56.1", "num_updates": "3800", "lr": "0.0002608", "gnorm": "42.69", "loss_scale": "256", "train_wall": "400", "gb_free": "3.9", "wall": "7796"}
[2024-12-01 02:35:04,351][train_inner][INFO] - {"epoch": 2, "update": 1.365, "loss": "168.644", "nll_loss": "6.857", "total": "1331.33", "n_correct": "190.4", "ppl": "115.89", "accuracy": "14.301", "wps": "643.1", "ups": "0.48", "wpb": "1331.3", "bsz": "58", "num_updates": "4000", "lr": "0.000274", "gnorm": "48.644", "loss_scale": "256", "train_wall": "404", "gb_free": "3.9", "wall": "8210"}
[2024-12-01 02:36:08,353][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2024-12-01 02:41:52,765][train_inner][INFO] - {"epoch": 2, "update": 1.433, "loss": "172.283", "nll_loss": "6.814", "total": "1325.32", "n_correct": "191.885", "ppl": "112.5", "accuracy": "14.478", "wps": "649", "ups": "0.49", "wpb": "1325.3", "bsz": "56.3", "num_updates": "4200", "lr": "0.0002872", "gnorm": "42.823", "loss_scale": "128", "train_wall": "399", "gb_free": "3.9", "wall": "8618"}
[2024-12-01 02:48:38,221][train_inner][INFO] - {"epoch": 2, "update": 1.502, "loss": "175.126", "nll_loss": "6.782", "total": "1325.2", "n_correct": "194.09", "ppl": "110.04", "accuracy": "14.646", "wps": "653.7", "ups": "0.49", "wpb": "1325.2", "bsz": "55.1", "num_updates": "4400", "lr": "0.0003004", "gnorm": "46.808", "loss_scale": "128", "train_wall": "396", "gb_free": "3.9", "wall": "9024"}
[2024-12-01 02:55:26,786][train_inner][INFO] - {"epoch": 2, "update": 1.57, "loss": "169.906", "nll_loss": "6.709", "total": "1337.1", "n_correct": "201.44", "ppl": "104.64", "accuracy": "15.065", "wps": "654.6", "ups": "0.49", "wpb": "1337.1", "bsz": "56.9", "num_updates": "4600", "lr": "0.0003136", "gnorm": "43.603", "loss_scale": "128", "train_wall": "400", "gb_free": "3.9", "wall": "9432"}
[2024-12-01 03:02:06,055][train_inner][INFO] - {"epoch": 2, "update": 1.638, "loss": "177.525", "nll_loss": "6.715", "total": "1326.21", "n_correct": "198.945", "ppl": "105.08", "accuracy": "15.001", "wps": "664.3", "ups": "0.5", "wpb": "1326.2", "bsz": "54", "num_updates": "4800", "lr": "0.0003268", "gnorm": "50.485", "loss_scale": "128", "train_wall": "390", "gb_free": "3.9", "wall": "9832"}
[2024-12-01 03:08:53,212][train_inner][INFO] - {"epoch": 2, "update": 1.706, "loss": "172.295", "nll_loss": "6.633", "total": "1336.82", "n_correct": "207.34", "ppl": "99.24", "accuracy": "15.51", "wps": "656.7", "ups": "0.49", "wpb": "1336.8", "bsz": "55.5", "num_updates": "5000", "lr": "0.00034", "gnorm": "41.871", "loss_scale": "128", "train_wall": "397", "gb_free": "3.9", "wall": "10239"}
[2024-12-01 03:15:38,494][train_inner][INFO] - {"epoch": 2, "update": 1.774, "loss": "169.47", "nll_loss": "6.603", "total": "1326.6", "n_correct": "211.025", "ppl": "97.2", "accuracy": "15.907", "wps": "654.7", "ups": "0.49", "wpb": "1326.6", "bsz": "55.8", "num_updates": "5200", "lr": "0.0003532", "gnorm": "39.969", "loss_scale": "128", "train_wall": "396", "gb_free": "3.9", "wall": "10644"}
[2024-12-01 03:22:21,025][train_inner][INFO] - {"epoch": 2, "update": 1.843, "loss": "173.103", "nll_loss": "6.571", "total": "1336.12", "n_correct": "213.595", "ppl": "95.05", "accuracy": "15.986", "wps": "663.9", "ups": "0.5", "wpb": "1336.1", "bsz": "54.8", "num_updates": "5400", "lr": "0.0003664", "gnorm": "38.468", "loss_scale": "128", "train_wall": "394", "gb_free": "3.9", "wall": "11047"}
[2024-12-01 03:29:11,988][train_inner][INFO] - {"epoch": 2, "update": 1.911, "loss": "165.205", "nll_loss": "6.499", "total": "1332.17", "n_correct": "219.57", "ppl": "90.47", "accuracy": "16.482", "wps": "648.3", "ups": "0.49", "wpb": "1332.2", "bsz": "56.8", "num_updates": "5600", "lr": "0.0003796", "gnorm": "36.004", "loss_scale": "128", "train_wall": "402", "gb_free": "3.9", "wall": "11458"}
[2024-12-01 03:36:07,324][train_inner][INFO] - {"epoch": 2, "update": 1.979, "loss": "163.341", "nll_loss": "6.463", "total": "1324.53", "n_correct": "222.86", "ppl": "88.22", "accuracy": "16.826", "wps": "637.8", "ups": "0.48", "wpb": "1324.5", "bsz": "56.9", "num_updates": "5800", "lr": "0.0003928", "gnorm": "35.795", "loss_scale": "128", "train_wall": "406", "gb_free": "3.9", "wall": "11873"}
[2024-12-01 03:37:40,036][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-12-01 03:38:33,589][valid][INFO] - {"epoch": 2, "valid_loss": "83.612", "valid_nll_loss": "6.056", "valid_total": "1356.8", "valid_n_correct": "289.7", "valid_ppl": "66.55", "valid_accuracy": "21.352", "valid_wps": "590.2", "valid_wpb": "1356.8", "valid_bsz": "108.2", "valid_num_updates": "5861"}
[2024-12-01 03:38:33,591][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 5861 updates
[2024-12-01 03:38:33,592][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2024-12-01 03:38:45,306][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2024-12-01 03:38:50,179][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 5861 updates, score 21.352) (writing took 16.58750183787197 seconds)
[2024-12-01 03:38:50,180][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2024-12-01 03:38:50,185][train][INFO] - {"epoch": 2, "train_loss": "173.279", "train_nll_loss": "6.764", "train_total": "1331.65", "train_n_correct": "197.584", "train_ppl": "108.65", "train_accuracy": "14.837", "train_wps": "645", "train_ups": "0.48", "train_wpb": "1331.7", "train_bsz": "55.9", "train_num_updates": "5861", "train_lr": "0.000396826", "train_gnorm": "43.801", "train_loss_scale": "128", "train_train_wall": "5806", "train_gb_free": "3.9", "train_wall": "12036"}
[2024-12-01 03:38:50,846][fairseq.trainer][INFO] - begin training epoch 3
[2024-12-01 03:38:50,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-12-01 03:44:29,187][train_inner][INFO] - {"epoch": 3, "update": 2.047, "loss": "168.76", "nll_loss": "6.435", "total": "1325.39", "n_correct": "224.205", "ppl": "86.52", "accuracy": "16.916", "wps": "528.2", "ups": "0.4", "wpb": "1325.4", "bsz": "54.9", "num_updates": "6000", "lr": "0.000406", "gnorm": "34.175", "loss_scale": "128", "train_wall": "369", "gb_free": "3.9", "wall": "12375"}
[2024-12-01 03:48:34,352][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2024-12-01 03:51:23,891][train_inner][INFO] - {"epoch": 3, "update": 2.116, "loss": "163.952", "nll_loss": "6.383", "total": "1322.82", "n_correct": "229.68", "ppl": "83.44", "accuracy": "17.363", "wps": "638", "ups": "0.48", "wpb": "1322.8", "bsz": "56", "num_updates": "6200", "lr": "0.0004192", "gnorm": "33.724", "loss_scale": "128", "train_wall": "405", "gb_free": "3.9", "wall": "12789"}
[2024-12-01 03:58:16,745][train_inner][INFO] - {"epoch": 3, "update": 2.184, "loss": "167.993", "nll_loss": "6.363", "total": "1327.31", "n_correct": "233.435", "ppl": "82.34", "accuracy": "17.587", "wps": "643.1", "ups": "0.48", "wpb": "1327.3", "bsz": "54.7", "num_updates": "6400", "lr": "0.0004324", "gnorm": "32.767", "loss_scale": "128", "train_wall": "403", "gb_free": "3.9", "wall": "13202"}
[2024-12-01 04:05:14,806][train_inner][INFO] - {"epoch": 3, "update": 2.252, "loss": "159.92", "nll_loss": "6.296", "total": "1343.86", "n_correct": "243.845", "ppl": "78.56", "accuracy": "18.145", "wps": "642.9", "ups": "0.48", "wpb": "1343.9", "bsz": "57.7", "num_updates": "6600", "lr": "0.0004456", "gnorm": "29.319", "loss_scale": "128", "train_wall": "408", "gb_free": "3.9", "wall": "13620"}
[2024-12-01 04:12:14,364][train_inner][INFO] - {"epoch": 3, "update": 2.321, "loss": "161.196", "nll_loss": "6.272", "total": "1333.85", "n_correct": "245.255", "ppl": "77.3", "accuracy": "18.387", "wps": "635.9", "ups": "0.48", "wpb": "1333.9", "bsz": "56.7", "num_updates": "6800", "lr": "0.0004588", "gnorm": "29.434", "loss_scale": "128", "train_wall": "411", "gb_free": "3.9", "wall": "14040"}
[2024-12-01 04:19:16,678][train_inner][INFO] - {"epoch": 3, "update": 2.389, "loss": "164.112", "nll_loss": "6.244", "total": "1327.59", "n_correct": "249.43", "ppl": "75.8", "accuracy": "18.788", "wps": "628.7", "ups": "0.47", "wpb": "1327.6", "bsz": "55.2", "num_updates": "7000", "lr": "0.000472", "gnorm": "28.583", "loss_scale": "128", "train_wall": "413", "gb_free": "3.9", "wall": "14462"}
[2024-12-01 04:26:16,708][train_inner][INFO] - {"epoch": 3, "update": 2.457, "loss": "159.789", "nll_loss": "6.192", "total": "1325.27", "n_correct": "253.075", "ppl": "73.13", "accuracy": "19.096", "wps": "631", "ups": "0.48", "wpb": "1325.3", "bsz": "56.2", "num_updates": "7200", "lr": "0.0004852", "gnorm": "27.475", "loss_scale": "128", "train_wall": "410", "gb_free": "3.9", "wall": "14882"}
[2024-12-01 04:33:07,842][train_inner][INFO] - {"epoch": 3, "update": 2.525, "loss": "164.175", "nll_loss": "6.155", "total": "1336.94", "n_correct": "259.84", "ppl": "71.26", "accuracy": "19.435", "wps": "650.4", "ups": "0.49", "wpb": "1336.9", "bsz": "55", "num_updates": "7400", "lr": "0.0004984", "gnorm": "24.791", "loss_scale": "128", "train_wall": "403", "gb_free": "3.9", "wall": "15293"}
[2024-12-01 04:39:55,522][train_inner][INFO] - {"epoch": 3, "update": 2.594, "loss": "161.097", "nll_loss": "6.106", "total": "1330.32", "n_correct": "266.295", "ppl": "68.88", "accuracy": "20.017", "wps": "652.7", "ups": "0.49", "wpb": "1330.3", "bsz": "55.4", "num_updates": "7600", "lr": "0.0005116", "gnorm": "24.174", "loss_scale": "128", "train_wall": "399", "gb_free": "3.9", "wall": "15701"}
[2024-12-01 04:46:52,414][train_inner][INFO] - {"epoch": 3, "update": 2.662, "loss": "159.548", "nll_loss": "6.063", "total": "1339.54", "n_correct": "274.285", "ppl": "66.84", "accuracy": "20.476", "wps": "642.6", "ups": "0.48", "wpb": "1339.5", "bsz": "56", "num_updates": "7800", "lr": "0.0005248", "gnorm": "24.037", "loss_scale": "128", "train_wall": "408", "gb_free": "3.9", "wall": "16118"}
[2024-12-01 04:53:57,335][train_inner][INFO] - {"epoch": 3, "update": 2.73, "loss": "150.307", "nll_loss": "6.006", "total": "1335.35", "n_correct": "281.255", "ppl": "64.25", "accuracy": "21.062", "wps": "628.5", "ups": "0.47", "wpb": "1335.4", "bsz": "58.8", "num_updates": "8000", "lr": "0.000538", "gnorm": "21.651", "loss_scale": "128", "train_wall": "415", "gb_free": "3.9", "wall": "16543"}
[2024-12-01 05:00:54,434][train_inner][INFO] - {"epoch": 3, "update": 2.798, "loss": "156.328", "nll_loss": "6.002", "total": "1324.7", "n_correct": "281.705", "ppl": "64.09", "accuracy": "21.265", "wps": "635.2", "ups": "0.48", "wpb": "1324.7", "bsz": "56.1", "num_updates": "8200", "lr": "0.0005512", "gnorm": "21.347", "loss_scale": "256", "train_wall": "408", "gb_free": "3.9", "wall": "16960"}
[2024-12-01 05:07:46,398][train_inner][INFO] - {"epoch": 3, "update": 2.867, "loss": "160.843", "nll_loss": "5.957", "total": "1335.72", "n_correct": "289.875", "ppl": "62.12", "accuracy": "21.702", "wps": "648.5", "ups": "0.49", "wpb": "1335.7", "bsz": "54.6", "num_updates": "8400", "lr": "0.0005644", "gnorm": "20.356", "loss_scale": "256", "train_wall": "404", "gb_free": "3.9", "wall": "17372"}
[2024-12-01 05:14:36,328][train_inner][INFO] - {"epoch": 3, "update": 2.935, "loss": "161.698", "nll_loss": "5.953", "total": "1337.04", "n_correct": "290.94", "ppl": "61.96", "accuracy": "21.76", "wps": "652.4", "ups": "0.49", "wpb": "1337", "bsz": "54.4", "num_updates": "8600", "lr": "0.0005776", "gnorm": "21.97", "loss_scale": "256", "train_wall": "402", "gb_free": "3.9", "wall": "17782"}
[2024-12-01 05:20:41,483][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2024-12-01 05:20:41,487][train][INFO] - {"epoch": 3, "train_loss": "160.708", "train_nll_loss": "6.149", "train_total": "1331.65", "train_n_correct": "261.996", "train_ppl": "70.98", "train_accuracy": "19.675", "train_wps": "638.4", "train_ups": "0.48", "train_wpb": "1331.6", "train_bsz": "55.9", "train_num_updates": "8791", "train_lr": "0.000590206", "train_gnorm": "26.098", "train_loss_scale": "256", "train_train_wall": "5924", "train_gb_free": "3.9", "train_wall": "18147"}
[2024-12-01 05:20:43,401][fairseq.trainer][INFO] - begin training epoch 4
[2024-12-01 05:20:43,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-12-01 05:21:54,270][train_inner][INFO] - {"epoch": 4, "update": 3.003, "loss": "153.078", "nll_loss": "5.895", "total": "1325.02", "n_correct": "296.615", "ppl": "59.5", "accuracy": "22.386", "wps": "605.2", "ups": "0.46", "wpb": "1325", "bsz": "56.5", "num_updates": "8800", "lr": "0.0005908", "gnorm": "18.677", "loss_scale": "256", "train_wall": "391", "gb_free": "3.9", "wall": "18220"}
[2024-12-01 05:28:47,476][train_inner][INFO] - {"epoch": 4, "update": 3.071, "loss": "152.085", "nll_loss": "5.857", "total": "1336.85", "n_correct": "302.015", "ppl": "57.96", "accuracy": "22.592", "wps": "647.1", "ups": "0.48", "wpb": "1336.9", "bsz": "57.1", "num_updates": "9000", "lr": "0.000604", "gnorm": "18.456", "loss_scale": "256", "train_wall": "405", "gb_free": "3.9", "wall": "18633"}
[2024-12-01 05:35:33,309][train_inner][INFO] - {"epoch": 4, "update": 3.14, "loss": "152.591", "nll_loss": "5.821", "total": "1326.12", "n_correct": "306.725", "ppl": "56.55", "accuracy": "23.129", "wps": "653.5", "ups": "0.49", "wpb": "1326.1", "bsz": "56.1", "num_updates": "9200", "lr": "0.0006172", "gnorm": "18.9", "loss_scale": "256", "train_wall": "397", "gb_free": "3.9", "wall": "19039"}
[2024-12-01 05:42:08,018][train_inner][INFO] - {"epoch": 4, "update": 3.208, "loss": "155.446", "nll_loss": "5.803", "total": "1331.29", "n_correct": "308.75", "ppl": "55.85", "accuracy": "23.192", "wps": "674.6", "ups": "0.51", "wpb": "1331.3", "bsz": "55.2", "num_updates": "9400", "lr": "0.0006304", "gnorm": "18.098", "loss_scale": "256", "train_wall": "386", "gb_free": "3.9", "wall": "19434"}
[2024-12-01 05:49:02,938][train_inner][INFO] - {"epoch": 4, "update": 3.276, "loss": "151.76", "nll_loss": "5.775", "total": "1334.83", "n_correct": "315.06", "ppl": "54.77", "accuracy": "23.603", "wps": "643.4", "ups": "0.48", "wpb": "1334.8", "bsz": "56.5", "num_updates": "9600", "lr": "0.0006436", "gnorm": "16.948", "loss_scale": "256", "train_wall": "407", "gb_free": "3.9", "wall": "19849"}
[2024-12-01 05:55:55,948][train_inner][INFO] - {"epoch": 4, "update": 3.344, "loss": "150.663", "nll_loss": "5.764", "total": "1330.49", "n_correct": "315.665", "ppl": "54.33", "accuracy": "23.725", "wps": "644.3", "ups": "0.48", "wpb": "1330.5", "bsz": "56.6", "num_updates": "9800", "lr": "0.0006568", "gnorm": "16.037", "loss_scale": "256", "train_wall": "404", "gb_free": "3.9", "wall": "20262"}
[2024-12-01 06:02:45,696][train_inner][INFO] - {"epoch": 4, "update": 3.412, "loss": "149.855", "nll_loss": "5.719", "total": "1332.22", "n_correct": "322.555", "ppl": "52.68", "accuracy": "24.212", "wps": "650.3", "ups": "0.49", "wpb": "1332.2", "bsz": "56.6", "num_updates": "10000", "lr": "0.00067", "gnorm": "16.243", "loss_scale": "256", "train_wall": "401", "gb_free": "3.9", "wall": "20671"}
[2024-12-01 06:09:44,588][train_inner][INFO] - {"epoch": 4, "update": 3.481, "loss": "155.263", "nll_loss": "5.732", "total": "1331.78", "n_correct": "320.66", "ppl": "53.16", "accuracy": "24.078", "wps": "635.9", "ups": "0.48", "wpb": "1331.8", "bsz": "54.7", "num_updates": "10200", "lr": "0.0006832", "gnorm": "15.954", "loss_scale": "256", "train_wall": "410", "gb_free": "3.9", "wall": "21090"}
[2024-12-01 06:16:42,353][train_inner][INFO] - {"epoch": 4, "update": 3.549, "loss": "149.694", "nll_loss": "5.697", "total": "1324.7", "n_correct": "322.725", "ppl": "51.88", "accuracy": "24.362", "wps": "634.2", "ups": "0.48", "wpb": "1324.7", "bsz": "56.2", "num_updates": "10400", "lr": "0.0006964", "gnorm": "14.397", "loss_scale": "512", "train_wall": "410", "gb_free": "3.9", "wall": "21508"}
[2024-12-01 06:23:37,009][train_inner][INFO] - {"epoch": 4, "update": 3.617, "loss": "154.228", "nll_loss": "5.686", "total": "1334.65", "n_correct": "329.27", "ppl": "51.47", "accuracy": "24.671", "wps": "643.7", "ups": "0.48", "wpb": "1334.7", "bsz": "54.9", "num_updates": "10600", "lr": "0.0007096", "gnorm": "14.979", "loss_scale": "512", "train_wall": "406", "gb_free": "3.9", "wall": "21923"}
[2024-12-01 06:26:58,655][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
[2024-12-01 06:30:37,771][train_inner][INFO] - {"epoch": 4, "update": 3.686, "loss": "150.416", "nll_loss": "5.649", "total": "1325.89", "n_correct": "327.785", "ppl": "50.16", "accuracy": "24.722", "wps": "630.2", "ups": "0.48", "wpb": "1325.9", "bsz": "55.6", "num_updates": "10800", "lr": "0.0007228", "gnorm": "13.574", "loss_scale": "256", "train_wall": "412", "gb_free": "3.9", "wall": "22343"}
[2024-12-01 06:37:34,245][train_inner][INFO] - {"epoch": 4, "update": 3.754, "loss": "150.128", "nll_loss": "5.633", "total": "1337.59", "n_correct": "335.83", "ppl": "49.62", "accuracy": "25.107", "wps": "642.4", "ups": "0.48", "wpb": "1337.6", "bsz": "56.1", "num_updates": "11000", "lr": "0.000736", "gnorm": "13.56", "loss_scale": "256", "train_wall": "408", "gb_free": "3.9", "wall": "22760"}
[2024-12-01 06:44:32,510][train_inner][INFO] - {"epoch": 4, "update": 3.822, "loss": "152.398", "nll_loss": "5.617", "total": "1335.57", "n_correct": "338.38", "ppl": "49.07", "accuracy": "25.336", "wps": "638.6", "ups": "0.48", "wpb": "1335.6", "bsz": "55.1", "num_updates": "11200", "lr": "0.0007492", "gnorm": "13.539", "loss_scale": "256", "train_wall": "410", "gb_free": "3.9", "wall": "23178"}
[2024-12-01 06:51:31,830][train_inner][INFO] - {"epoch": 4, "update": 3.89, "loss": "148.459", "nll_loss": "5.609", "total": "1327.4", "n_correct": "337.365", "ppl": "48.81", "accuracy": "25.415", "wps": "633.1", "ups": "0.48", "wpb": "1327.4", "bsz": "56.1", "num_updates": "11400", "lr": "0.0007624", "gnorm": "12.948", "loss_scale": "256", "train_wall": "412", "gb_free": "3.9", "wall": "23597"}
[2024-12-01 06:58:26,949][train_inner][INFO] - {"epoch": 4, "update": 3.959, "loss": "149.439", "nll_loss": "5.592", "total": "1331.21", "n_correct": "340.59", "ppl": "48.25", "accuracy": "25.585", "wps": "641.4", "ups": "0.48", "wpb": "1331.2", "bsz": "55.8", "num_updates": "11600", "lr": "0.0007756", "gnorm": "12.476", "loss_scale": "256", "train_wall": "406", "gb_free": "3.9", "wall": "24013"}
[2024-12-01 07:02:01,934][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-12-01 07:02:55,930][valid][INFO] - {"epoch": 4, "valid_loss": "74.574", "valid_nll_loss": "5.224", "valid_total": "1356.8", "valid_n_correct": "402.9", "valid_ppl": "37.37", "valid_accuracy": "29.695", "valid_wps": "553.3", "valid_wpb": "1356.8", "valid_bsz": "108.2", "valid_num_updates": "11721", "valid_best_accuracy": "29.695"}
[2024-12-01 07:02:55,933][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 11721 updates
[2024-12-01 07:02:55,934][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2024-12-01 07:03:16,326][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2024-12-01 07:03:34,948][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 11721 updates, score 29.695) (writing took 39.01450947672129 seconds)
[2024-12-01 07:03:34,950][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2024-12-01 07:03:34,978][train][INFO] - {"epoch": 4, "train_loss": "151.593", "train_nll_loss": "5.706", "train_total": "1331.69", "train_n_correct": "323.875", "train_ppl": "52.19", "train_accuracy": "24.321", "train_wps": "632", "train_ups": "0.47", "train_wpb": "1331.7", "train_bsz": "55.9", "train_num_updates": "11721", "train_lr": "0.000783586", "train_gnorm": "15.317", "train_loss_scale": "256", "train_train_wall": "5919", "train_gb_free": "3.9", "train_wall": "24321"}
[2024-12-01 07:03:40,814][fairseq.trainer][INFO] - begin training epoch 5
[2024-12-01 07:03:40,815][fairseq_cli.train][INFO] - Start iterating over samples
[2024-12-01 07:06:57,352][train_inner][INFO] - {"epoch": 5, "update": 4.027, "loss": "153.429", "nll_loss": "5.556", "total": "1341.55", "n_correct": "346.055", "ppl": "47.03", "accuracy": "25.795", "wps": "525.7", "ups": "0.39", "wpb": "1341.5", "bsz": "54.5", "num_updates": "11800", "lr": "0.0007888", "gnorm": "12.423", "loss_scale": "256", "train_wall": "369", "gb_free": "3.9", "wall": "24523"}
[2024-12-01 07:13:42,234][train_inner][INFO] - {"epoch": 5, "update": 4.095, "loss": "144.841", "nll_loss": "5.518", "total": "1331.35", "n_correct": "347.585", "ppl": "45.81", "accuracy": "26.108", "wps": "657.7", "ups": "0.49", "wpb": "1331.4", "bsz": "57", "num_updates": "12000", "lr": "0.000802", "gnorm": "11.813", "loss_scale": "256", "train_wall": "397", "gb_free": "3.9", "wall": "24928"}
[2024-12-01 07:20:17,517][train_inner][INFO] - {"epoch": 5, "update": 4.163, "loss": "152.49", "nll_loss": "5.535", "total": "1328.82", "n_correct": "346.645", "ppl": "46.36", "accuracy": "26.087", "wps": "672.4", "ups": "0.51", "wpb": "1328.8", "bsz": "54.1", "num_updates": "12200", "lr": "0.0008152", "gnorm": "12.131", "loss_scale": "256", "train_wall": "386", "gb_free": "3.9", "wall": "25323"}
[2024-12-01 07:27:06,288][train_inner][INFO] - {"epoch": 5, "update": 4.232, "loss": "147.332", "nll_loss": "5.512", "total": "1335.32", "n_correct": "352.92", "ppl": "45.62", "accuracy": "26.43", "wps": "653.3", "ups": "0.49", "wpb": "1335.3", "bsz": "56.1", "num_updates": "12400", "lr": "0.0008284", "gnorm": "11.41", "loss_scale": "256", "train_wall": "400", "gb_free": "3.9", "wall": "25732"}
[2024-12-01 07:33:48,058][train_inner][INFO] - {"epoch": 5, "update": 4.3, "loss": "147.805", "nll_loss": "5.496", "total": "1324.17", "n_correct": "349.43", "ppl": "45.14", "accuracy": "26.389", "wps": "659.2", "ups": "0.5", "wpb": "1324.2", "bsz": "55.4", "num_updates": "12600", "lr": "0.0008416", "gnorm": "11.455", "loss_scale": "256", "train_wall": "393", "gb_free": "3.9", "wall": "26134"}
[2024-12-01 07:40:30,494][train_inner][INFO] - {"epoch": 5, "update": 4.368, "loss": "150.974", "nll_loss": "5.498", "total": "1329.57", "n_correct": "352.31", "ppl": "45.18", "accuracy": "26.498", "wps": "660.8", "ups": "0.5", "wpb": "1329.6", "bsz": "54.4", "num_updates": "12800", "lr": "0.0008548", "gnorm": "11.091", "loss_scale": "512", "train_wall": "394", "gb_free": "3.9", "wall": "26536"}
[2024-12-01 07:47:14,513][train_inner][INFO] - {"epoch": 5, "update": 4.436, "loss": "145.778", "nll_loss": "5.483", "total": "1336.3", "n_correct": "355.9", "ppl": "44.71", "accuracy": "26.633", "wps": "661.5", "ups": "0.5", "wpb": "1336.3", "bsz": "56.5", "num_updates": "13000", "lr": "0.000868", "gnorm": "10.561", "loss_scale": "512", "train_wall": "395", "gb_free": "3.9", "wall": "26940"}
[2024-12-01 07:54:06,874][train_inner][INFO] - {"epoch": 5, "update": 4.505, "loss": "142.709", "nll_loss": "5.471", "total": "1327.94", "n_correct": "354.955", "ppl": "44.35", "accuracy": "26.73", "wps": "644.1", "ups": "0.49", "wpb": "1327.9", "bsz": "57.3", "num_updates": "13200", "lr": "0.0008812", "gnorm": "10.497", "loss_scale": "512", "train_wall": "404", "gb_free": "3.9", "wall": "27352"}
[2024-12-01 08:00:48,431][train_inner][INFO] - {"epoch": 5, "update": 4.573, "loss": "146.213", "nll_loss": "5.456", "total": "1327.56", "n_correct": "356.445", "ppl": "43.89", "accuracy": "26.85", "wps": "661.2", "ups": "0.5", "wpb": "1327.6", "bsz": "55.8", "num_updates": "13400", "lr": "0.0008944", "gnorm": "10.17", "loss_scale": "512", "train_wall": "394", "gb_free": "3.9", "wall": "27754"}
[2024-12-01 08:07:36,049][train_inner][INFO] - {"epoch": 5, "update": 4.641, "loss": "143.307", "nll_loss": "5.465", "total": "1319.96", "n_correct": "354.71", "ppl": "44.18", "accuracy": "26.873", "wps": "647.7", "ups": "0.49", "wpb": "1320", "bsz": "56.7", "num_updates": "13600", "lr": "0.0009076", "gnorm": "10.063", "loss_scale": "512", "train_wall": "399", "gb_free": "3.9", "wall": "28162"}
[2024-12-01 08:14:22,180][train_inner][INFO] - {"epoch": 5, "update": 4.709, "loss": "146.399", "nll_loss": "5.438", "total": "1330.95", "n_correct": "359.81", "ppl": "43.35", "accuracy": "27.034", "wps": "655.4", "ups": "0.49", "wpb": "1331", "bsz": "55.7", "num_updates": "13800", "lr": "0.0009208", "gnorm": "9.992", "loss_scale": "512", "train_wall": "398", "gb_free": "3.9", "wall": "28568"}
[2024-12-01 08:21:17,325][train_inner][INFO] - {"epoch": 5, "update": 4.778, "loss": "142.061", "nll_loss": "5.431", "total": "1345.74", "n_correct": "365.395", "ppl": "43.14", "accuracy": "27.152", "wps": "648.3", "ups": "0.48", "wpb": "1345.7", "bsz": "58", "num_updates": "14000", "lr": "0.000934", "gnorm": "9.511", "loss_scale": "512", "train_wall": "408", "gb_free": "3.9", "wall": "28983"}
[2024-12-01 08:27:58,596][train_inner][INFO] - {"epoch": 5, "update": 4.846, "loss": "150.563", "nll_loss": "5.441", "total": "1336.77", "n_correct": "361.715", "ppl": "43.45", "accuracy": "27.059", "wps": "666.3", "ups": "0.5", "wpb": "1336.8", "bsz": "54.4", "num_updates": "14200", "lr": "0.0009472", "gnorm": "10.252", "loss_scale": "512", "train_wall": "393", "gb_free": "3.9", "wall": "29384"}
[2024-12-01 08:34:48,440][train_inner][INFO] - {"epoch": 5, "update": 4.914, "loss": "144.469", "nll_loss": "5.42", "total": "1334.23", "n_correct": "364.54", "ppl": "42.81", "accuracy": "27.322", "wps": "651.1", "ups": "0.49", "wpb": "1334.2", "bsz": "56.5", "num_updates": "14400", "lr": "0.0009604", "gnorm": "9.417", "loss_scale": "512", "train_wall": "401", "gb_free": "3.9", "wall": "29794"}
[2024-12-01 08:41:29,337][train_inner][INFO] - {"epoch": 5, "update": 4.982, "loss": "149.415", "nll_loss": "5.429", "total": "1333.44", "n_correct": "361.615", "ppl": "43.09", "accuracy": "27.119", "wps": "665.2", "ups": "0.5", "wpb": "1333.4", "bsz": "54.6", "num_updates": "14600", "lr": "0.0009736", "gnorm": "9.815", "loss_scale": "512", "train_wall": "393", "gb_free": "3.9", "wall": "30195"}
[2024-12-01 08:42:47,056][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2024-12-01 08:42:47,060][train][INFO] - {"epoch": 5, "train_loss": "146.694", "train_nll_loss": "5.471", "train_total": "1331.66", "train_n_correct": "355.916", "train_ppl": "44.36", "train_accuracy": "26.727", "train_wps": "655.8", "train_ups": "0.49", "train_wpb": "1331.7", "train_bsz": "55.9", "train_num_updates": "14652", "train_lr": "0.000977032", "train_gnorm": "10.604", "train_loss_scale": "512", "train_train_wall": "5789", "train_gb_free": "3.9", "train_wall": "30273"}
[2024-12-01 08:42:48,630][fairseq.trainer][INFO] - begin training epoch 6
[2024-12-01 08:42:48,630][fairseq_cli.train][INFO] - Start iterating over samples
[2024-12-01 08:48:42,463][train_inner][INFO] - {"epoch": 6, "update": 5.05, "loss": "144.811", "nll_loss": "5.369", "total": "1329.69", "n_correct": "368.71", "ppl": "41.33", "accuracy": "27.729", "wps": "614", "ups": "0.46", "wpb": "1329.7", "bsz": "55.7", "num_updates": "14800", "lr": "0.0009868", "gnorm": "9.429", "loss_scale": "1024", "train_wall": "395", "gb_free": "3.9", "wall": "30628"}
[2024-12-01 08:54:32,610][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
[2024-12-01 08:55:36,610][train_inner][INFO] - {"epoch": 6, "update": 5.119, "loss": "141.708", "nll_loss": "5.349", "total": "1330.66", "n_correct": "371.25", "ppl": "40.74", "accuracy": "27.9", "wps": "642.6", "ups": "0.48", "wpb": "1330.7", "bsz": "56.8", "num_updates": "15000", "lr": "0.001", "gnorm": "9.343", "loss_scale": "512", "train_wall": "405", "gb_free": "3.9", "wall": "31042"}
[2024-12-01 09:02:21,954][train_inner][INFO] - {"epoch": 6, "update": 5.187, "loss": "144.179", "nll_loss": "5.332", "total": "1336.76", "n_correct": "375.51", "ppl": "40.28", "accuracy": "28.091", "wps": "659.6", "ups": "0.49", "wpb": "1336.8", "bsz": "56", "num_updates": "15200", "lr": "0.000980227", "gnorm": "9.218", "loss_scale": "512", "train_wall": "397", "gb_free": "3.9", "wall": "31448"}
[2024-12-01 09:09:09,143][train_inner][INFO] - {"epoch": 6, "update": 5.256, "loss": "144.292", "nll_loss": "5.354", "total": "1326.41", "n_correct": "368.22", "ppl": "40.91", "accuracy": "27.761", "wps": "651.5", "ups": "0.49", "wpb": "1326.4", "bsz": "55.7", "num_updates": "15400", "lr": "0.000960844", "gnorm": "8.998", "loss_scale": "512", "train_wall": "399", "gb_free": "3.9", "wall": "31855"}
[2024-12-01 09:15:59,765][train_inner][INFO] - {"epoch": 6, "update": 5.324, "loss": "145.119", "nll_loss": "5.329", "total": "1330.33", "n_correct": "373.575", "ppl": "40.19", "accuracy": "28.081", "wps": "648", "ups": "0.49", "wpb": "1330.3", "bsz": "55.3", "num_updates": "15600", "lr": "0.000941845", "gnorm": "8.87", "loss_scale": "512", "train_wall": "402", "gb_free": "3.9", "wall": "32265"}
[2024-12-01 09:22:54,961][train_inner][INFO] - {"epoch": 6, "update": 5.392, "loss": "140.583", "nll_loss": "5.295", "total": "1336.95", "n_correct": "381.935", "ppl": "39.25", "accuracy": "28.568", "wps": "644", "ups": "0.48", "wpb": "1337", "bsz": "57.1", "num_updates": "15800", "lr": "0.000923221", "gnorm": "8.489", "loss_scale": "512", "train_wall": "407", "gb_free": "3.9", "wall": "32681"}
[2024-12-01 09:29:42,894][train_inner][INFO] - {"epoch": 6, "update": 5.46, "loss": "143.997", "nll_loss": "5.313", "total": "1324.59", "n_correct": "376.21", "ppl": "39.76", "accuracy": "28.402", "wps": "649.4", "ups": "0.49", "wpb": "1324.6", "bsz": "55.4", "num_updates": "16000", "lr": "0.000904966", "gnorm": "8.791", "loss_scale": "512", "train_wall": "400", "gb_free": "3.9", "wall": "33088"}
[2024-12-01 09:36:26,812][train_inner][INFO] - {"epoch": 6, "update": 5.528, "loss": "148.261", "nll_loss": "5.303", "total": "1329.72", "n_correct": "379.29", "ppl": "39.48", "accuracy": "28.524", "wps": "658.4", "ups": "0.5", "wpb": "1329.7", "bsz": "53.9", "num_updates": "16200", "lr": "0.000887072", "gnorm": "8.817", "loss_scale": "512", "train_wall": "395", "gb_free": "3.9", "wall": "33492"}
[2024-12-01 09:43:24,403][train_inner][INFO] - {"epoch": 6, "update": 5.597, "loss": "138.99", "nll_loss": "5.259", "total": "1331.88", "n_correct": "385.05", "ppl": "38.29", "accuracy": "28.91", "wps": "637.9", "ups": "0.48", "wpb": "1331.9", "bsz": "57.2", "num_updates": "16400", "lr": "0.000869531", "gnorm": "8.286", "loss_scale": "512", "train_wall": "409", "gb_free": "3.9", "wall": "33910"}
[2024-12-01 09:50:06,163][train_inner][INFO] - {"epoch": 6, "update": 5.665, "loss": "144.754", "nll_loss": "5.277", "total": "1334.59", "n_correct": "382.805", "ppl": "38.77", "accuracy": "28.683", "wps": "664.4", "ups": "0.5", "wpb": "1334.6", "bsz": "55.2", "num_updates": "16600", "lr": "0.000852338", "gnorm": "8.538", "loss_scale": "512", "train_wall": "394", "gb_free": "3.9", "wall": "34312"}
[2024-12-01 09:56:56,341][train_inner][INFO] - {"epoch": 6, "update": 5.733, "loss": "140.067", "nll_loss": "5.25", "total": "1329.11", "n_correct": "387.365", "ppl": "38.06", "accuracy": "29.145", "wps": "648.1", "ups": "0.49", "wpb": "1329.1", "bsz": "56.6", "num_updates": "16800", "lr": "0.000835484", "gnorm": "8.139", "loss_scale": "512", "train_wall": "401", "gb_free": "3.9", "wall": "34722"}
[2024-12-01 10:03:48,947][train_inner][INFO] - {"epoch": 6, "update": 5.801, "loss": "140.213", "nll_loss": "5.252", "total": "1333.48", "n_correct": "387.77", "ppl": "38.12", "accuracy": "29.079", "wps": "646.4", "ups": "0.48", "wpb": "1333.5", "bsz": "56.8", "num_updates": "17000", "lr": "0.000818964", "gnorm": "8.147", "loss_scale": "512", "train_wall": "404", "gb_free": "3.9", "wall": "35135"}
[2024-12-01 10:04:34,515][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
[2024-12-01 10:10:32,078][train_inner][INFO] - {"epoch": 6, "update": 5.87, "loss": "142.421", "nll_loss": "5.238", "total": "1330.1", "n_correct": "387.915", "ppl": "37.74", "accuracy": "29.164", "wps": "659.9", "ups": "0.5", "wpb": "1330.1", "bsz": "55.6", "num_updates": "17200", "lr": "0.00080277", "gnorm": "8.104", "loss_scale": "512", "train_wall": "395", "gb_free": "3.9", "wall": "35538"}
[2024-12-01 10:17:12,433][train_inner][INFO] - {"epoch": 6, "update": 5.938, "loss": "144.172", "nll_loss": "5.224", "total": "1331.4", "n_correct": "391.36", "ppl": "37.39", "accuracy": "29.395", "wps": "665.2", "ups": "0.5", "wpb": "1331.4", "bsz": "54.9", "num_updates": "17400", "lr": "0.000786896", "gnorm": "8.264", "loss_scale": "512", "train_wall": "393", "gb_free": "3.9", "wall": "35938"}
