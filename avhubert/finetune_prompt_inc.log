2024-12-12 13:08:32 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17120
2024-12-12 13:08:32 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17120
2024-12-12 13:08:32 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17120
2024-12-12 13:08:32 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17120
2024-12-12 13:08:32 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 1
2024-12-12 13:08:33 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 3
2024-12-12 13:08:33 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 2
2024-12-12 13:08:33 | INFO | fairseq.distributed.utils | initialized host dell-SYS-4029GP-TRT as rank 0
[2024-12-12 13:08:33,974][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': 'tblog', 'wandb_project': None, 'azureml_logging': False, 'seed': 1337, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/workspace/av_hubert/avhubert', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17120', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 2, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 45000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'av_hubert_seq2seq', 'w2v_path': '/workspace/AV_HuBERT_pretrained/base_vox_iter5.pt', 'apply_mask': False, 'mask_selection': 'static', 'mask_length': 10, 'mask_other': 0, 'mask_prob': 0.75, 'mask_channel_selection': 'static', 'mask_channel_length': 64, 'mask_channel_other': 0, 'mask_channel_prob': 0.5, 'layerdrop': 0.1, 'dropout': 0.0, 'activation_dropout': 0.1, 'attention_dropout': 0.0, 'feature_grad_mult': 1.0, 'decoder_layers': 6, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.0, 'decoder_activation_dropout': 0.1, 'freeze_finetune_updates': 22500, 'share_decoder_input_output_embed': True, 'decoder_normalize_before': True}, 'task': {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/workspace/lrs2/433h_data', 'label_dir': '/workspace/lrs2/433h_data', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'fine_tuning': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video', 'audio'], 'image_aug': True, 'pad_audio': True, 'random_crop': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': True, 'ignore_prefix_size': 0, 'sentence_avg': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 15000, 'hold_steps': 0, 'decay_steps': 30000, 'phase_ratio': None, 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 45000, 'lr': [0.001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-12-12 13:08:33,980][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/finetune_prompt
[2024-12-12 13:08:33,980][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['wrd'], 'label_dir': '/workspace/lrs2/433h_data', 'label_rate': -1, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 500, 'min_sample_size': None, 'max_trim_sample_size': '${task.max_sample_size}', 'single_target': True, 'random_crop': False, 'pad_audio': True, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['video', 'audio'], 'is_s2s': True, 'tokenizer_bpe_name': 'sentencepiece', 'tokenizer_bpe_model': '/workspace/lrs2/spm1000/spm_unigram1000.model', 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': True}
[2024-12-12 13:08:36,986][avhubert.hubert_pretraining][INFO] - current directory is /workspace/av_hubert/finetune_prompt
[2024-12-12 13:08:36,986][avhubert.hubert_pretraining][INFO] - AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/workspace/lrs2/433h_data', 'labels': ['km'], 'label_dir': '/checkpoint/bshi/data/lrs3//video/hubert/stitch-iters/envox-iter4-l12c2000/', 'label_rate': 25, 'sample_rate': 25, 'normalize': True, 'enable_padding': False, 'max_sample_size': 2000, 'min_sample_size': 5, 'max_trim_sample_size': 400, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['audio', 'video'], 'is_s2s': False, 'tokenizer_bpe_name': None, 'tokenizer_bpe_model': None, 'noise_wav': None, 'noise_prob': 0.0, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': False}
[2024-12-12 13:08:36,997][avhubert.hubert][INFO] - HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 1.0, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}
[2024-12-12 13:08:46,436][fairseq_cli.train][INFO] - AVHubertSeq2Seq(
  (encoder): HubertEncoderWrapper(
    (w2v_model): AVHubertModel(
      (feature_extractor_audio): SubModel(
        (proj): Linear(in_features=104, out_features=768, bias=True)
      )
      (feature_extractor_video): SubModel(
        (resnet): ResEncoder(
          (frontend3D): Sequential(
            (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
            (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): PReLU(num_parameters=64)
            (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
          )
          (trunk): ResNet(
            (layer1): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): BasicBlock(
                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=64)
                (relu2): PReLU(num_parameters=64)
                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer2): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=128)
                (relu2): PReLU(num_parameters=128)
                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer3): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=256)
                (relu2): PReLU(num_parameters=256)
                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (layer4): Sequential(
              (0): BasicBlock(
                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (downsample): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (1): BasicBlock(
                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (relu1): PReLU(num_parameters=512)
                (relu2): PReLU(num_parameters=512)
                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (avgpool): AdaptiveAvgPool2d(output_size=1)
          )
        )
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
      (post_extract_proj): Linear(in_features=1536, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (modal_prompt_learner): MultiModalPromptLearner(
        (compound_prompt_projections_audio): ModuleList(
          (0-11): 12 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_audio): ModuleList(
          (0-11): 12 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (compound_prompt_projections_video): ModuleList(
          (0-11): 12 x Sequential(
            (0): Linear(in_features=1536, out_features=96, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=96, out_features=768, bias=True)
          )
        )
        (layernorm_video): ModuleList(
          (0-11): 12 x LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (common_prompt_projection_video): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
        (common_prompt_projection_audio): Sequential(
          (0): Linear(in_features=768, out_features=48, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=48, out_features=768, bias=True)
        )
      )
      (video_encoder): TransformerEncoder_prompt(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (audio_encoder): TransformerEncoder_prompt(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(1000, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
[2024-12-12 13:08:46,443][fairseq_cli.train][INFO] - task: AVHubertPretrainingTask
[2024-12-12 13:08:46,443][fairseq_cli.train][INFO] - model: AVHubertSeq2Seq
[2024-12-12 13:08:46,443][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2024-12-12 13:08:46,448][fairseq_cli.train][INFO] - num. shared model params: 345,845,576 (num. trained: 345,845,576)
[2024-12-12 13:08:46,452][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-12-12 13:08:46,453][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2024-12-12 13:08:46,463][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 1082, skipped 0 short and 0 long and 0 unaligned, longest-loaded=153, shortest-loaded=14
[2024-12-12 13:08:46,464][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/valid.wrd is sequence label. skipped
[2024-12-12 13:08:46,464][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    <avhubert.utils.CenterCrop object at 0x7f9a2bb19bb0>
    Normalize(mean=0.421, std=0.165)
)
[2024-12-12 13:08:46,464][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2024-12-12 13:08:46,464][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv1.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv2.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv1.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv2.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv1.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv2.bias
[2024-12-12 13:08:46,611][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.0.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv1.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv2.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv1.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv2.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.0.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv1.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv2.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv1.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv2.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.0.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv1.bias
[2024-12-12 13:08:46,612][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- encoder.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv2.bias
[2024-12-12 13:08:47,151][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2024-12-12 13:08:47,152][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-12 13:08:47,152][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-12 13:08:47,152][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-12 13:08:47,152][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 11.762 GB ; name = NVIDIA GeForce RTX 3060                 
[2024-12-12 13:08:47,152][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2024-12-12 13:08:47,152][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2024-12-12 13:08:47,152][fairseq_cli.train][INFO] - max tokens per device = 1000 and max sentences per device = None
[2024-12-12 13:08:47,153][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2024-12-12 13:09:13,329][fairseq.trainer][INFO] - Loaded checkpoint checkpoints/checkpoint_last.pt (epoch 125 @ 0 updates)
[2024-12-12 13:09:13,330][fairseq.trainer][INFO] - loading train data for epoch 125
[2024-12-12 13:09:13,330][avhubert.hubert_pretraining][INFO] - Using tokenizer
[2024-12-12 13:09:14,324][avhubert.hubert_dataset][INFO] - max_keep=500, min_keep=None, loaded 163822, skipped 0 short and 292 long and 0 unaligned, longest-loaded=500, shortest-loaded=0
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
[2024-12-12 13:09:14,428][avhubert.hubert_dataset][INFO] - /workspace/lrs2/433h_data/train.wrd is sequence label. skipped
[2024-12-12 13:09:14,428][avhubert.hubert_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    RandomCrop(size=(88, 88))
    <avhubert.utils.HorizontalFlip object at 0x7f99f935a850>
    Normalize(mean=0.421, std=0.165)
)
[2024-12-12 13:09:14,428][avhubert.hubert_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2024-12-12 13:09:14,428][avhubert.hubert_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2024-12-12 13:09:15,155][fairseq.trainer][INFO] - begin training epoch 125
[2024-12-12 13:09:15,155][fairseq_cli.train][INFO] - Start iterating over samples
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
/workspace/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
/root/miniconda3/envs/avhubert/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/workspace/av_hubert/fairseq/fairseq/trainer.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)
/workspace/av_hubert/fairseq/fairseq/distributed/utils.py:767: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(buffer, map_location="cpu")
[2024-12-12 13:10:01,326][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2024-12-12 13:10:36,986][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2024-12-12 13:10:57,319][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-12-12 13:13:07,724][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-12-12 13:15:07,478][train_inner][INFO] - {"epoch": 125, "update": 124.035, "loss": "74.837", "nll_loss": "1.832", "total": "669.16", "n_correct": "491.41", "ppl": "3.56", "accuracy": "73.437", "wps": "438.7", "ups": "0.66", "wpb": "669.2", "bsz": "27.2", "num_updates": "200", "lr": "2.32e-05", "gnorm": "147.286", "loss_scale": "8", "train_wall": "324", "gb_free": "3.8", "wall": "380"}
[2024-12-12 13:20:06,035][train_inner][INFO] - {"epoch": 125, "update": 124.069, "loss": "71.742", "nll_loss": "1.794", "total": "668.14", "n_correct": "493.43", "ppl": "3.47", "accuracy": "73.851", "wps": "447.6", "ups": "0.67", "wpb": "668.1", "bsz": "28", "num_updates": "400", "lr": "3.64e-05", "gnorm": "136.737", "loss_scale": "8", "train_wall": "292", "gb_free": "3.8", "wall": "679"}
[2024-12-12 13:24:56,176][train_inner][INFO] - {"epoch": 125, "update": 124.103, "loss": "74.02", "nll_loss": "1.83", "total": "671.54", "n_correct": "493.245", "ppl": "3.56", "accuracy": "73.45", "wps": "462.9", "ups": "0.69", "wpb": "671.5", "bsz": "27.6", "num_updates": "600", "lr": "4.96e-05", "gnorm": "162.838", "loss_scale": "8", "train_wall": "285", "gb_free": "3.8", "wall": "969"}
[2024-12-12 13:27:10,004][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-12-12 13:29:50,518][train_inner][INFO] - {"epoch": 125, "update": 124.137, "loss": "72.293", "nll_loss": "1.75", "total": "665.38", "n_correct": "496.075", "ppl": "3.36", "accuracy": "74.555", "wps": "452.1", "ups": "0.68", "wpb": "665.4", "bsz": "27.3", "num_updates": "800", "lr": "6.28e-05", "gnorm": "171.487", "loss_scale": "4", "train_wall": "289", "gb_free": "3.8", "wall": "1263"}
[2024-12-12 13:34:48,241][train_inner][INFO] - {"epoch": 125, "update": 124.171, "loss": "68.251", "nll_loss": "1.754", "total": "670.605", "n_correct": "500.21", "ppl": "3.37", "accuracy": "74.591", "wps": "450.5", "ups": "0.67", "wpb": "670.6", "bsz": "29.2", "num_updates": "1000", "lr": "7.6e-05", "gnorm": "148.133", "loss_scale": "4", "train_wall": "292", "gb_free": "3.8", "wall": "1561"}
[2024-12-12 13:39:51,666][train_inner][INFO] - {"epoch": 125, "update": 124.206, "loss": "70.662", "nll_loss": "1.756", "total": "674.655", "n_correct": "502.525", "ppl": "3.38", "accuracy": "74.486", "wps": "444.7", "ups": "0.66", "wpb": "674.7", "bsz": "28.4", "num_updates": "1200", "lr": "8.92e-05", "gnorm": "203.083", "loss_scale": "4", "train_wall": "297", "gb_free": "3.8", "wall": "1865"}
[2024-12-12 13:44:57,696][train_inner][INFO] - {"epoch": 125, "update": 124.24, "loss": "70.935", "nll_loss": "1.739", "total": "671.33", "n_correct": "501.085", "ppl": "3.34", "accuracy": "74.641", "wps": "438.8", "ups": "0.65", "wpb": "671.3", "bsz": "28", "num_updates": "1400", "lr": "0.0001024", "gnorm": "191.114", "loss_scale": "4", "train_wall": "300", "gb_free": "3.8", "wall": "2171"}
[2024-12-12 13:49:51,634][train_inner][INFO] - {"epoch": 125, "update": 124.274, "loss": "71.082", "nll_loss": "1.763", "total": "667.465", "n_correct": "497.605", "ppl": "3.39", "accuracy": "74.551", "wps": "454.2", "ups": "0.68", "wpb": "667.5", "bsz": "28", "num_updates": "1600", "lr": "0.0001156", "gnorm": "211.014", "loss_scale": "4", "train_wall": "288", "gb_free": "3.8", "wall": "2464"}
[2024-12-12 13:54:51,911][train_inner][INFO] - {"epoch": 125, "update": 124.308, "loss": "69.919", "nll_loss": "1.769", "total": "664.575", "n_correct": "494.14", "ppl": "3.41", "accuracy": "74.354", "wps": "442.7", "ups": "0.67", "wpb": "664.6", "bsz": "28.4", "num_updates": "1800", "lr": "0.0001288", "gnorm": "191.647", "loss_scale": "4", "train_wall": "294", "gb_free": "3.8", "wall": "2765"}
[2024-12-12 14:00:43,269][train_inner][INFO] - {"epoch": 125, "update": 124.342, "loss": "70.419", "nll_loss": "1.763", "total": "665.22", "n_correct": "495.415", "ppl": "3.39", "accuracy": "74.474", "wps": "378.7", "ups": "0.57", "wpb": "665.2", "bsz": "28.1", "num_updates": "2000", "lr": "0.000142", "gnorm": "176.45", "loss_scale": "4", "train_wall": "315", "gb_free": "3.8", "wall": "3116"}
[2024-12-12 14:00:49,780][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-12-12 14:04:15,321][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-12-12 14:06:44,951][train_inner][INFO] - {"epoch": 125, "update": 124.376, "loss": "71.456", "nll_loss": "1.761", "total": "668.15", "n_correct": "497.68", "ppl": "3.39", "accuracy": "74.486", "wps": "369.5", "ups": "0.55", "wpb": "668.1", "bsz": "27.8", "num_updates": "2200", "lr": "0.0001552", "gnorm": "200.354", "loss_scale": "1", "train_wall": "356", "gb_free": "3.8", "wall": "3478"}
[2024-12-12 14:11:50,036][train_inner][INFO] - {"epoch": 125, "update": 124.411, "loss": "74.259", "nll_loss": "1.764", "total": "666.705", "n_correct": "495.67", "ppl": "3.4", "accuracy": "74.346", "wps": "437.1", "ups": "0.66", "wpb": "666.7", "bsz": "26.7", "num_updates": "2400", "lr": "0.0001684", "gnorm": "248.558", "loss_scale": "1", "train_wall": "295", "gb_free": "3.8", "wall": "3783"}
[2024-12-12 14:17:40,935][train_inner][INFO] - {"epoch": 125, "update": 124.445, "loss": "68.007", "nll_loss": "1.693", "total": "664.775", "n_correct": "501.675", "ppl": "3.23", "accuracy": "75.465", "wps": "378.9", "ups": "0.57", "wpb": "664.8", "bsz": "28.5", "num_updates": "2600", "lr": "0.0001816", "gnorm": "232.679", "loss_scale": "1", "train_wall": "345", "gb_free": "3.8", "wall": "4134"}
[2024-12-12 14:22:32,630][train_inner][INFO] - {"epoch": 125, "update": 124.479, "loss": "71.273", "nll_loss": "1.697", "total": "660.945", "n_correct": "497.305", "ppl": "3.24", "accuracy": "75.242", "wps": "453.2", "ups": "0.69", "wpb": "660.9", "bsz": "27.1", "num_updates": "2800", "lr": "0.0001948", "gnorm": "170.155", "loss_scale": "1", "train_wall": "286", "gb_free": "3.8", "wall": "4425"}
[2024-12-12 14:27:21,098][train_inner][INFO] - {"epoch": 125, "update": 124.513, "loss": "69.645", "nll_loss": "1.748", "total": "668.145", "n_correct": "498.695", "ppl": "3.36", "accuracy": "74.639", "wps": "463.2", "ups": "0.69", "wpb": "668.1", "bsz": "28.4", "num_updates": "3000", "lr": "0.000208", "gnorm": "229.716", "loss_scale": "1", "train_wall": "282", "gb_free": "3.8", "wall": "4714"}
[2024-12-12 14:32:27,764][train_inner][INFO] - {"epoch": 125, "update": 124.547, "loss": "72.968", "nll_loss": "1.732", "total": "668.89", "n_correct": "501.345", "ppl": "3.32", "accuracy": "74.952", "wps": "436.3", "ups": "0.65", "wpb": "668.9", "bsz": "27", "num_updates": "3200", "lr": "0.0002212", "gnorm": "173.08", "loss_scale": "1", "train_wall": "301", "gb_free": "3.8", "wall": "5021"}
[2024-12-12 14:37:24,304][train_inner][INFO] - {"epoch": 125, "update": 124.581, "loss": "70.279", "nll_loss": "1.757", "total": "663.65", "n_correct": "494.765", "ppl": "3.38", "accuracy": "74.552", "wps": "447.6", "ups": "0.67", "wpb": "663.6", "bsz": "28.1", "num_updates": "3400", "lr": "0.0002344", "gnorm": "155.785", "loss_scale": "1", "train_wall": "291", "gb_free": "3.8", "wall": "5317"}
[2024-12-12 14:41:57,005][train_inner][INFO] - {"epoch": 125, "update": 124.615, "loss": "73.505", "nll_loss": "1.77", "total": "662.645", "n_correct": "492.61", "ppl": "3.41", "accuracy": "74.34", "wps": "486", "ups": "0.73", "wpb": "662.6", "bsz": "26.9", "num_updates": "3600", "lr": "0.0002476", "gnorm": "138.293", "loss_scale": "1", "train_wall": "266", "gb_free": "3.8", "wall": "5590"}
[2024-12-12 14:46:50,513][train_inner][INFO] - {"epoch": 125, "update": 124.649, "loss": "68.445", "nll_loss": "1.787", "total": "665.59", "n_correct": "493.595", "ppl": "3.45", "accuracy": "74.159", "wps": "453.6", "ups": "0.68", "wpb": "665.6", "bsz": "29.1", "num_updates": "3800", "lr": "0.0002608", "gnorm": "151.815", "loss_scale": "1", "train_wall": "287", "gb_free": "3.8", "wall": "5883"}
[2024-12-12 14:51:53,127][train_inner][INFO] - {"epoch": 125, "update": 124.684, "loss": "69.586", "nll_loss": "1.74", "total": "664.765", "n_correct": "495.72", "ppl": "3.34", "accuracy": "74.571", "wps": "439.4", "ups": "0.66", "wpb": "664.8", "bsz": "28.2", "num_updates": "4000", "lr": "0.000274", "gnorm": "271.881", "loss_scale": "1", "train_wall": "296", "gb_free": "3.8", "wall": "6186"}
[2024-12-12 14:56:52,244][train_inner][INFO] - {"epoch": 125, "update": 124.718, "loss": "68.601", "nll_loss": "1.753", "total": "665.47", "n_correct": "495.745", "ppl": "3.37", "accuracy": "74.495", "wps": "445", "ups": "0.67", "wpb": "665.5", "bsz": "28.8", "num_updates": "4200", "lr": "0.0002872", "gnorm": "370.724", "loss_scale": "1", "train_wall": "293", "gb_free": "3.8", "wall": "6485"}
[2024-12-12 15:01:53,575][train_inner][INFO] - {"epoch": 125, "update": 124.752, "loss": "66.542", "nll_loss": "1.735", "total": "659.26", "n_correct": "493.445", "ppl": "3.33", "accuracy": "74.848", "wps": "437.6", "ups": "0.66", "wpb": "659.3", "bsz": "29.2", "num_updates": "4400", "lr": "0.0003004", "gnorm": "254.458", "loss_scale": "1", "train_wall": "295", "gb_free": "3.8", "wall": "6786"}
[2024-12-12 15:06:39,645][train_inner][INFO] - {"epoch": 125, "update": 124.786, "loss": "68.822", "nll_loss": "1.768", "total": "658.99", "n_correct": "490.925", "ppl": "3.4", "accuracy": "74.497", "wps": "460.7", "ups": "0.7", "wpb": "659", "bsz": "28.5", "num_updates": "4600", "lr": "0.0003136", "gnorm": "225.539", "loss_scale": "1", "train_wall": "279", "gb_free": "3.8", "wall": "7072"}
[2024-12-12 15:11:23,055][train_inner][INFO] - {"epoch": 125, "update": 124.82, "loss": "70.006", "nll_loss": "1.741", "total": "662.56", "n_correct": "496.69", "ppl": "3.34", "accuracy": "74.965", "wps": "467.6", "ups": "0.71", "wpb": "662.6", "bsz": "28", "num_updates": "4800", "lr": "0.0003268", "gnorm": "124.432", "loss_scale": "1", "train_wall": "278", "gb_free": "3.8", "wall": "7356"}
[2024-12-12 15:16:08,590][train_inner][INFO] - {"epoch": 125, "update": 124.854, "loss": "70.987", "nll_loss": "1.771", "total": "665.73", "n_correct": "496.03", "ppl": "3.41", "accuracy": "74.509", "wps": "466.3", "ups": "0.7", "wpb": "665.7", "bsz": "28", "num_updates": "5000", "lr": "0.00034", "gnorm": "378.737", "loss_scale": "1", "train_wall": "279", "gb_free": "3.8", "wall": "7641"}
[2024-12-12 15:20:39,364][train_inner][INFO] - {"epoch": 125, "update": 124.888, "loss": "72.844", "nll_loss": "1.758", "total": "659.425", "n_correct": "491.66", "ppl": "3.38", "accuracy": "74.559", "wps": "487.1", "ups": "0.74", "wpb": "659.4", "bsz": "26.9", "num_updates": "5200", "lr": "0.0003532", "gnorm": "245.988", "loss_scale": "1", "train_wall": "264", "gb_free": "3.8", "wall": "7912"}
[2024-12-12 15:25:26,004][train_inner][INFO] - {"epoch": 125, "update": 124.922, "loss": "69.459", "nll_loss": "1.718", "total": "657.19", "n_correct": "493.72", "ppl": "3.29", "accuracy": "75.126", "wps": "458.6", "ups": "0.7", "wpb": "657.2", "bsz": "27.8", "num_updates": "5400", "lr": "0.0003664", "gnorm": "258.284", "loss_scale": "1", "train_wall": "280", "gb_free": "3.8", "wall": "8199"}
[2024-12-12 15:30:09,942][train_inner][INFO] - {"epoch": 125, "update": 124.956, "loss": "71.725", "nll_loss": "1.733", "total": "670.175", "n_correct": "501.85", "ppl": "3.33", "accuracy": "74.883", "wps": "472.1", "ups": "0.7", "wpb": "670.2", "bsz": "27.6", "num_updates": "5600", "lr": "0.0003796", "gnorm": "278.73", "loss_scale": "1", "train_wall": "278", "gb_free": "3.9", "wall": "8483"}
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/workspace/av_hubert/fairseq/fairseq/tasks/fairseq_task.py:502: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
